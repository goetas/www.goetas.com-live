<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Asmir Mustafic - Software Architect and IT Consultant]]></title>
    <link href="https://www.goetas.com/atom.xml" rel="self"/>
    <link href="https://www.goetas.com/"/>
    <updated>2023-07-06T11:25:34+02:00</updated>
    <id>https://www.goetas.com/</id>        <author><name><![CDATA[Asmir Mustafic]]></name><email><![CDATA[info@goetas.com]]></email>        </author>    <generator uri="http://sculpin.io/">Sculpin</generator>        <entry>
            <title>Multi-namespace migrations with doctrine/migrations 3.0</title>
            <link href="https://www.goetas.com/blog/multi-namespace-migrations-with-doctrinemigrations-30"/>
            <updated>2020-04-05T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/multi-namespace-migrations-with-doctrinemigrations-30</id>                <summary>doctrine/migrations 3.0-alpha1 has been made available last week. The main user-facing feature is the ability to define migrations into multiple folders/namespaces and to specify dependencies between migrations.
</summary>                <content type="html"><![CDATA[<p>Last week <a href="/blog/released-doctrinemigrations-30-alpha">I've announced the first alpha release</a> 
for <a href="https://github.com/doctrine/migrations"><code>doctrine/migrations</code> 3.0</a>.
This new major release is the result of almost 6 months of work. 
Brings a completely refactored/rewritten internal structure and some interesting new features.</p>

<p>This post will focus on the <strong>ability to collect migrations from multiple folders/namespaces and
to specify dependencies between them</strong> (this feature has been introduced with the 3.0 major release and 
was not available before).</p>

<h2 id="multi-namespace-migrations">Multi-namespace migrations</h2>

<p>Let's suppose we have a multi-installation application(such as example Wordpress, Magento, Drupal or similar).
All this applications have some kind of optional Modules/Plugins/Extensions.</p>

<p>Modules/Plugins/Extensions by definition are optional, and if a module defines migrations, 
then even the database structure will be different from installation to installation.
When modules have dependencies to other modules, then the execution order 
depends on those dependencies and not only on the chronological order.</p>

<p>Consider the following application diagram:</p>

<pre><code class="text">            +--------+
            |  Core  |
            +---+----+
                |
         +------+------+
         |             |
         v             v
    +--------+    +--------+
    |Module A|    |Module B|
    +--------+    +---+----+
                      |
                      v
                  +--------+
                  |Module C|
                  +--------+
</code></pre>

<p>We can see that there is a <code>Core</code> application, then the modules <code>Module A</code>, <code>Module B</code>, and <code>Module C</code>.
We can see also that <code>Module C</code> has a dependency on <code>Module B</code>.</p>

<p>If the <code>Module C</code> defines some migrations, they most likely will depend on structures defined 
in the <code>Module B</code>, thus they should be executed after all the migrations in <code>ModuleB</code> have been executed. 
On the other hand, since <code>Module A</code> does not depend on <code>Module B</code>, their execution order relative to each other is not important.</p>

<p>In previous versions of <code>doctrine/migrations</code> this case was not handled, let's see how this use case can be solved by 
the upcoming 3.0 release.</p>

<h2 id="use-case">Use case</h2>

<p>Here we are going to see an example of how multi-namespace migrations can be implemented using <code>doctrine/migrations</code> <code>3.0</code>.</p>

<h3 id="directory-layout">Directory layout</h3>

<p>Our example application has the following directory layout:</p>

<pre><code class="text">├── config/
|   └── cli-config.php
├── src/
|   ├── Core/
|   |  └── Migrations/
|   |     └── Version182289181.php
|   ├── ModuleA/
|   |  └── Migrations/
|   |     └── Version182289181.php
|   ├── ModuleB/    
|   |  └── Migrations/
|   |      └── Version098766776.php
|   └── ModuleC/
|      └── Migrations/
|         └── Version987689789.php
├── vendor/
└── composer.json
</code></pre>

<ul>
<li><code>composer.json</code> and <code>vendor</code> are the standard composer file and vendor directory, containing the list of packages
we depend on and the source code. 
By default executable files are available in the <code>vendor/bin</code> directory, and 
<code>doctrine/migrations</code> offers the <code>vendor/bin/doctrine-migrations</code> executable command to manage database migrations.</li>
<li><code>src</code> contains our source code, in this case, the application is divided into 4 parts, the <code>Core</code> and 3 modules.
(This is just an example directory layout, in your application you can use any other layout you prefer.)</li>
<li><code>config/cli-config.php</code> is loaded by <code>doctrine/migrations</code> to configure itself.
For other ways to integrate <code>doctrine/migrations</code>, please take a look at the <a href="https://www.doctrine-project.org/projects/doctrine-migrations/en/latest/reference/custom-integration.html">official documentation</a>.</li>
</ul>

<p><em>Note that this is just an example, your application can have a different layout that might depend on how
modules are loaded, framework in use and many other factors. The important thing is that the configurations defined
in the <code>config/cli-config.php</code> file are able to discover the migration classes.</em></p>

<h3 id="configurations">Configurations</h3>

<p>This is the content of our <code>cli-config.php</code>:</p>

<pre><code class="php">use Doctrine\DBAL\DriverManager;
use Doctrine\Migrations\Configuration\Connection\ExistingConnection;
use Doctrine\Migrations\Configuration\Migration\ConfigurationArray;
use Doctrine\Migrations\DependencyFactory;
use Doctrine\Migrations\Version\Comparator;
use App\Core\ProjectVersionComparator;

// setup database connection
$conn = DriverManager::getConnection(['driver' =&gt; 'pdo_sqlite', 'memory' =&gt; true]);

// define configurations
$config = new ConfigurationArray([
    'migrations_paths'      =&gt; [
        'App\Core\Migrations' =&gt; 'src/Core/Migrations',
        'App\ModuleA\Migrations' =&gt; 'src/ModuleA/Migrations',
        'App\ModuleB\Migrations' =&gt; 'src/ModuleB/Migrations',
        'App\ModuleC\Migrations' =&gt; 'src/ModuleC/Migrations',
     ],
]);

$di = DependencyFactory::fromConnection($config, new ExistingConnection($conn));

// define custom migration sorting
$di-&gt;setService(Comparator::class, new ProjectVersionComparator());

return $di;
</code></pre>

<p>Analyzing line-by-line this file, we can see that:</p>

<ul>
<li><code>$conn</code> is the connection to your database using the <a href="https://github.com/doctrine/dbal">DBAL</a> library.</li>
<li><code>$config</code> defines the doctrine migration configurations. The only mandatory parameter is  <code>migrations_paths</code> 
that is a key/value array that tells to <code>doctrine/migrations</code> where to find the migration files for each module
(keys are namespace prefixes and values are directory locations. 
Many other configuration options are explained in the <a href="https://www.doctrine-project.org/projects/doctrine-migrations/en/latest/reference/custom-configuration.html#custom-configuration">official documentation</a>.</li>
<li><code>$di</code> is the dependency factory that will be used by doctrine to retrieve connection, configurations 
and other dependencies.</li>
<li>as last thing, calling <code>$di-&gt;setService(Comparator::class, ...)</code> we define a custom 
version comparator (<code>App\Core\ProjectVersionComparator</code>) 
that will have the responsibility of deciding the execution order for our migrations.</li>
</ul>

<h3 id="version-comparator">Version comparator</h3>

<p>The version comparator decides the execution order and must implement the  <code>Doctrine\Migrations\Version\Comparator</code> interface.
The class implementing the interface has the responsibility to figure out the dependencies between migrations
and sort the migrations accordingly.</p>

<p>Let's take a look at the custom version comparator:</p>

<pre><code class="php">namespace App\Core;

use Doctrine\Migrations\Version\Comparator;
use Doctrine\Migrations\Version\AlphabeticalComparator;
use MJS\TopSort\Implementations\ArraySort;

class ProjectVersionComparator implements Comparator
{
    private $dependencies;
    private $defaultSorter;

    public function __construct()
    {
        $this-&gt;defaultSorter = new AlphabeticalComparator();
        $this-&gt;dependencies = $this-&gt;buildDependencies();
    }

    private function buildDependencies(): array
    {        
        $sorter = new ArraySort();

        $sorter-&gt;add('App\Core');
        $sorter-&gt;add('App\ModuleA', ['App\Core']);
        $sorter-&gt;add('App\ModuleB', ['App\Core']);
        $sorter-&gt;add('App\ModuleC', ['App\ModuleB']);

        return array_flip($sorter-&gt;sort());
    }

    private function getNamespacePrefix(Version $version): string
    {
        if (preg_match('~^App\[^\]+~', (string)$version, $mch)) {
            return $mch[0];
        }       

        throw new Exception('Can not find the namespace prefix for the provide migration version.');
    }   

    public function compare(Version $a, Version $b): int
    {
        $prefixA = $this-&gt;getNamespacePrefix($a);
        $prefixB = $this-&gt;getNamespacePrefix($b);

        return $this-&gt;dependencies[$prefixA] &lt;=&gt; $this-&gt;dependencies[$prefixB] 
            ?: $this-&gt;defaultSorter-&gt;compare($a, $b);
    }
}
</code></pre>

<p>Analyzing method-by-method this class, we can see that:</p>

<h4 id="__construct">__construct()</h4>

<p>The constructor initializes two variables that will be used by the main <code>compare()</code> method.</p>

<ul>
<li><code>$this-&gt;defaultSorter</code>: is an instance of <code>new AlphabeticalComparator</code> and initializes
the default alphabetical doctrine migration sorter (will be used later as fallback sorting).</li>
<li><code>$this-&gt;dependencies</code>: is initialized by <code>$this-&gt;buildDependencies()</code> and is an array containing the sorted dependencies between modules</li>
</ul>

<h4 id="builddependencies">buildDependencies()</h4>

<p>This method is the core of our dependency resolution strategy. 
Uses the <a href="https://github.com/marcj/topsort.php">marcj/topsort.php</a> library 
to perform <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sorting</a> and provide a sorted array 
of our dependency graph. Later that array will be used to sort migrations.</p>

<p>For each namespace, we define its dependent namespace. In our case we have:</p>

<ul>
<li><code>App\Core</code> has no dependencies</li>
<li><code>App\ModuleA</code> depends on <code>App\Core</code></li>
<li><code>App\ModuleB</code> depends on <code>App\Core</code></li>
<li><code>App\ModuleC</code> depends on <code>App\ModuleB</code> (the dependency on <code>App\Core</code> is optional here because <code>App\ModuleB</code> depends already on it)</li>
</ul>

<p>In the end, <code>$this-&gt;dependencies</code> will be an array having as keys the application namespaces and as value the execution order.
To be more precise, will look like this:</p>

<pre><code class="php">[
 'App\Core' =&gt; 0,
 'App\ModuleA' =&gt; 1, 
 'App\ModuleB' =&gt; 2,
 'App\ModuleC' =&gt; 3,
]
</code></pre>

<p><em>Note that the hard-coded dependencies in the function <code>buildDependencies()</code> is just an example 
on how to detect relations between packages.
In an ideal situation, modules could be independent composer packages, then a better way
to resolve migration dependencies would be using the package relations defined in the <code>composer.json</code> file.</em></p>

<h4 id="getnamespaceprefix">getNamespacePrefix()</h4>

<p>This is just a utility method that is able to extract the namespace prefix from a <code>doctrine/migrations</code> version name.
As an example, if the migration version is <code>App\ModuleA\Migrations\Version6569787886</code>, it will return <code>App\ModuleA</code>.
It is used to have an easy lookup in the <code>$this-&gt;dependencies</code> array and locate the execution order of the migration
based on the package to which it belongs.</p>

<h4 id="compare">compare()</h4>

<p>This method does the real job of deciding which migration comes first and which comes later.</p>

<p>Let's see line-by-line what is happening:</p>

<ul>
<li>using <code>getNamespacePrefix()</code> we get the namespace prefixes for the two migration versions we are comparing</li>
<li><p>using the space ship operator (<code>&lt;=&gt;</code>) we check the order of execution of the two namespace prefixes 
against the <code>$this-&gt;dependencies</code> array (initialized in the constructor using the <code>buildDependencies()</code> method)</p>

<ul>
<li>if the two migrations belong to two packages that depend on each other, 
the <code>&lt;=&gt;</code>  will return <code>1</code> or <code>-1</code> depending on which comes first. </li>
<li>if the two migrations belong to the same package, the spaceship operator will return <code>0</code>, so we fallback to 
the default doctrine alphabetical sorting.</li>
</ul></li>
</ul>

<h3 id="running-migrations-and-other-commands">Running migrations and other commands</h3>

<p>Most of the other commands have the same output and input argument.
A complete list of available commands can be found on the <a href="https://www.doctrine-project.org/projects/doctrine-migrations/en/latest/index.html">official documentation</a>.</p>

<h3 id="empty-migrations">Empty migrations</h3>

<pre><code class="bash">bin/doctrine-migrations generate --namespace 'App\ModuleA\Migrations'
</code></pre>

<p>Will generate an empty migration in the <code>src/Core/Migrations</code> directory and having as namespace <code>App\Core\Migrations</code>.
By omitting the  <code>--namespace</code> argument, migrations will be generated in the first defined <code>migrations_paths</code> element.</p>

<p>An optional parameter <code>--filter-expression</code> will allow you to include in the diff only changes for a particular 
subset of your schema.</p>

<h3 id="diff-migrations">Diff migrations</h3>

<pre><code class="bash">bin/doctrine-migrations diff --namespace 'App\ModuleA\Migrations'
</code></pre>

<p>Will generate a migration in the <code>src/Core/Migrations</code> directory and having as namespace <code>App\Core\Migrations</code>.
By omitting the  <code>--namespace</code> argument, migrations will be generated in the first defined <code>migrations_paths</code> element 
and all the entities will be considered. (This command is available only if you are using <code>doctine/orm</code>)</p>

<h2 id="symfony-integration">Symfony integration</h2>

<p>If you are using <a href="https://symfony.com/">Symfony</a>, <code>doctrine/migrations</code> 3.0 comes with 
<code>doctrine/doctrine-migrations-bundle</code> 3.0.</p>

<p>Because of how the integration is done we do not need the <code>config/cli-config.php</code> file to define connections, dependency
factories or configurations. 
We also do not need the doctrine migrations executable file anymore since Symfony has already its 
own fully integrated <code>bin/console</code> command.</p>

<p>Symfony's bundle system allows us also to organize our core application and modules in bundles.</p>

<p>Most of the configurations are defined inside <code>doctrine_migrations.yaml</code>.</p>

<pre><code class="yaml">doctrine_migrations:
    migrations_paths:
        'App\Core\Migrations': '%kernel.project_dir%/Migrations'
        'MyCompany\ModuleA\Migrations': '%kernel.root_dir%/vendor/my-company/mod-a/src/Migrations'
        'MyCompany\ModuleB\Migrations': '%kernel.root_dir%/vendor/my-company/mod-b/src/Migrations'
        'MyCompany\ModuleC\Migrations': '%kernel.root_dir%/vendor/my-company/mod-c/src/Migrations'
    services: 
        'Doctrine\Migrations\Version\Comparator': 'App\Core\ProjectVersionComparator'
</code></pre>

<p>This configuration implies that:</p>

<ul>
<li>your <code>Core</code> module is in the <code>src</code> directory and the other
modules are in the <code>vendor</code> directory installed as composer packages</li>
<li><code>App\Core\ProjectVersionComparator</code> is a service based on the <code>App\Core\ProjectVersionComparator</code> class we saw before 
(<a href="https://symfony.com/doc/current/service_container.html">here more info on how to define Symfony services</a>)</li>
</ul>

<p>The <code>doctrine/doctrine-migrations-bundle</code> will auto-configure itself to use the default DBAL connection 
or the default ORM entity manager if available. 
There are many other configuration parameters <a href="https://www.doctrine-project.org/projects/doctrine-migrations/en/latest/reference/configuration.html#configuration">explained in the official documentation</a>.</p>

<p><em>Depending on which Symfony version you are using, this file could be located inside <code>config/packages</code>
or its content has to be placed inside <code>config/config.yml</code>. 
If you are using Symfony Flex, a skeleton of this file will be auto-generated.</em></p>

<h3 id="extra%3A-custom-migration-factories-aka-decorating-services">Extra: Custom migration factories (aka Decorating services)</h3>

<p>Sometimes migrations need external services to get some data before running migrations. Use cases are fetching 
default data and inserting them into newly created tables, resolving IP addresses to countries if you are adding the 
"country" column to an existing user table, and many other situations.</p>

<p>This can be done by defining a custom factory on the <code>doctrine_migrations.yaml</code> file:</p>

<pre><code class="yaml">doctrine_migrations:
    # ...
    services: 
        'Doctrine\Migrations\Version\MigrationFactory': 'App\Core\ProjectVersionFactory'

services:
    App\Core\ProjectVersionFactory.inner:
      class: Doctrine\Migrations\Version\DbalMigrationFactory
      factory: ['@doctrine.migrations.dependency_factory', 'getMigrationFactory']

    App\Core\ProjectVersionFactory:
      arguments: ['@App\Core\ProjectVersionFactory.inner', '@service_container']
</code></pre>

<p>The <code>App\Core\ProjectVersionFactory</code> can look like this:</p>

<pre><code class="php">namespace App\Core;

use Doctrine\Migrations\AbstractMigration;
use Doctrine\Migrations\Version\MigrationFactory;
use Symfony\Component\DependencyInjection\ContainerAwareInterface;
use Symfony\Component\DependencyInjection\ContainerInterface;

class ProjectVersionFactory implements MigrationFactory
{
    private $container;
    private $migrationFactory;

    public function __construct(MigrationFactory $migrationFactory, ContainerInterface $container)
    {
        $this-&gt;container = $container;
        $this-&gt;migrationFactory = $migrationFactory;
    }

    public function createVersion(string $migrationClassName): AbstractMigration
    {        
        $instance = $this-&gt;migrationFactory-&gt;createVersion($migrationClassName);

        if ($instance instanceof ContainerAwareInterface) {
            $instance-&gt;setContainer($this-&gt;container);
        } 

        return $instance;
    }
}
</code></pre>

<p>Custom migration factories can be defined also in the project without Symfony by declaring them in the <code>cli-config.php</code> file.</p>

<pre><code class="php">// ...

$oldMigrationFactory = $di-&gt;getMigrationFactory();
$di-&gt;setService(MigrationFactory::class, new ProjectVersionFactory($oldMigrationFactory, $container));

return $di;
</code></pre>

<h3 id="extra%3A-auto-registered-migrations">Extra: Auto registered migrations</h3>

<p>This is a feature useful for bundle authors/maintainers and allows a bundle to "append"
some migrations to the list of migrations to be executed by the core application.
It is done using the Symfony's <a href="https://symfony.com/doc/current/bundles/prepend_extension.html">prepend extension</a> interface.</p>

<pre><code class="php">namespace MyCompany\ModuleA\DependencyInjection;

use Symfony\Component\DependencyInjection\ContainerBuilder;
use Symfony\Component\DependencyInjection\Extension\PrependExtensionInterface;
use Symfony\Component\HttpKernel\DependencyInjection\Extension;

class MyCompanyModuleAExtension extends Extension implements PrependExtensionInterface
{
    // ...
    public function prepend(ContainerBuilder $container)
    {
        $container-&gt;prependExtensionConfig('doctrine_migrations', [
            'migrations_paths' =&gt; [
                'MyCompany\ModuleA\Migrations' =&gt; __DIR__. '/../Migrations',
            ]
        ]);
    }
}
</code></pre>

<p>Using the Symfony dependency injection and the prepend extension feature,  <code>ModuleA</code> is able to auto-register
its migrations into the <code>Core</code> application.</p>

<p>Of course, the sorting algorithm provided by the <code>ProjectVersionComparator</code> class must be able to sort accordingly 
the migrations provided by <code>ModuleA</code>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>That's it. With these configurations, <code>doctrine/migrations</code> is able to run migrations from multiple namespaces
and the execution order will depend on the dependencies between the different modules.</p>

<h3 id="what-is-next">What is next</h3>

<p><code>doctrine/migrations</code> <code>3.0</code> is still in alpha, 
<strong>to be able to deliver a good stable release it is important that you test the pre-release and share your feedback!</strong></p>

<p>To try the alpha version, you can run:</p>

<pre><code class="bash">composer require 'doctrine/migrations:^3.0@alpha'
</code></pre>

<p>You can have a look also to the <a href="https://github.com/doctrine/migrations/blob/master/UPGRADE.md">upgrading</a> document.</p>

<p>If you are using Symfony:</p>

<pre><code class="bash">composer require 'doctrine/doctrine-migrations-bundle:^3.0@alpha' 'doctrine/migrations:^3.0@alpha'
</code></pre>

<p>You can have a look also to the <a href="https://github.com/doctrine/DoctrineMigrationsBundle/blob/master/UPGRADE.md">upgrading</a> document 
for the symfony bundle.</p>]]></content>
        </entry>        <entry>
            <title>Released doctrine/migrations 3.0-alpha</title>
            <link href="https://www.goetas.com/blog/released-doctrinemigrations-30-alpha"/>
            <updated>2020-03-29T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/released-doctrinemigrations-30-alpha</id>                <summary>doctrine/migrations 3.0-alpha has been made available today.  This new major release is the result of almost 6 months of work, brings a completely refactored/rewritten internal structure  and many new features.
</summary>                <content type="html"><![CDATA[<p>I'm proud to announce that the first alpha release
 for <a href="https://github.com/doctrine/migrations"><code>doctrine/migrations</code> 3.0</a> 
 has been <a href="https://github.com/doctrine/migrations/tree/3.0.0-alpha1">published</a> today. 
This new major release is the result of almost 6 months of work and also 
one of the reasons for me <a href="https://twitter.com/goetas_asmir/status/1204668554165018627?s=20">joining</a>
 the <a href="https://www.doctrine-project.org/">Doctrine</a> team.</p>

<h3 id="why-a-new-major-release%3F">Why a new major release?</h3>

<p>The <code>doctrine/migrations</code> <code>v1.x</code> codebase is 10 years old, and in the past years a lot of features have been added on top of its
initial architecture.<br />
<code>doctrine/migrations</code> <code>2.0</code> was released a bit more than a year ago. This major release did a bit of cleanup, 
but the general structure remained the same.
In this schema you can see the dependencies between classes in the latest <code>2.3.x</code> branch:</p>

<div class="text-center">
<object width="70%" data="/img/posts/doctrine-migrations-3.0/complex-cycle-v2.svg" type="image/svg+xml"></object>
</div>

<p>The red lines are circular dependencies (and we already know that in software development circular dependencies are not 
a good thing).</p>

<p>In <code>doctrine/migrations</code> <code>3.x</code>, most of the internal classes have been re-written and dependency injection has been 
widely adopted.<br />
In this schema you can see the dependencies between classes in the latest <code>master</code> branch (release v3.0):</p>

<div class="text-center">
<object width="70%" data="/img/posts/doctrine-migrations-3.0/complex-cycle-v3.svg" type="image/svg+xml"></object>
</div>

<p>As you can see the circular dependencies are gone. This has been possible thanks to extensive use of dependency injection
and applying SOLID principles.
To reduce future backward incompatibilities, many classes have been marked as <code>final</code> or as <code>@internal</code> while 
keeping the functionalities intact. Extensibility is still possible by using dependency injection and providing 
classes implementing dedicated interfaces.</p>

<p><em>These schemas have been generated using <a href="https://github.com/mamuz/PhpDependencyAnalysis">PhpDependencyAnalysis</a> 
with <a href="https://gist.github.com/goetas/e6343746a6ccd6ebb191cbbd675898e0">this configuration</a>.</em></p>

<h3 id="new-features-and-improvements">New features and improvements</h3>

<p>Beside the code quality improvements, there is a a long list of improvements (see below), but 
<strong>the main user-facing feature is the ability to collect migrations from multiple folders/namespaces and
to specify dependencies between migrations.</strong> 
On this topic, in the next days I will publish one more post with additional details 
about it and a guide on how it works.</p>

<p><em>Update, 5th April 2020: The article has been published <a href="/blog/multi-namespace-migrations-with-doctrinemigrations-30">here</a>.</em></p>

<p>Here a (probably not complete) list of improvements implemented in the upcoming <code>3.0</code> release:</p>

<ul>
<li>ability to collect migrations from multiple folders/namespaces and to specify dependencies between migrations</li>
<li><code>doctrine/migrations</code> will write to your database only when running migrations 
(previously the metadata table was created on the very first command run even if it was a read-only command)</li>
<li>Output verbosity can be controlled using the <code>-v</code>, <code>-vv</code> or <code>-vvv</code> command parameters</li>
<li>Use of dependency injection allows you to decorate/replace most services</li>
<li>Removed usage of console helpers to provide the connection or entity manager in favor of dependency injection</li>
<li>Introduced <code>migrations:list</code> command to list the available/executed migrations</li>
<li>Introduced <code>migrations:sync-metadata-storage</code> command to explicitly update the metadata schema in case a newer version 
introduces changes to the metadata table</li>
<li>Multiple migrations can be passed to the <code>migrations:execute</code> command</li>
<li>More organized output of the <code>migrations:status</code> command</li>
<li>Configurations and Dependency Factory are read-only during the migration process</li>
<li>The <code>down()</code> migration is optional now</li>
<li>Multi-namespace migrations</li>
<li>Custom migrations metadata storage</li>
<li>Added warning when using the <code>migrations:diff</code> if there are not executed migrations</li>
</ul>

<h3 id="backward-compatibility">Backward compatibility</h3>

<p>In <code>doctrine/migrations</code> <code>3.0</code> a lot of things changed, but for end-users most of the things will look the same.
Your migration files do not need any update.</p>

<p>You will have to change  your configuration files, as the configuration format has changed.
The <a href="https://www.doctrine-project.org/projects/doctrine-migrations/en/latest/reference/configuration.html#configuration">official documentation</a> contains more information about these changes.
This documentation should be particularly helpful if you did also some custom integration with third party frameworks 
or libraries.</p>

<p>If you wrote custom event listeners, please take a look at them as the method signatures for event listeners have been updated.</p>

<h3 id="symfony-integration">Symfony Integration</h3>

<p>If you are using <a href="https://github.com/doctrine/DoctrineMigrationsBundle">DoctrineMigrationsBundle</a> then things are even 
easier: the 2.3.0 release introduced some deprecation notices and if you have already solved them 
your configuration is already compatible. If you want you can have a look to the latest configuration format 
available on the <a href="https://www.doctrine-project.org/projects/doctrine-migrations-bundle/en/3.0/index.html#configuration">official documentation</a>.
You can look more in detail to which changes are needed in the <a href="https://github.com/doctrine/DoctrineMigrationsBundle/blob/master/UPGRADE.md">upgrading</a> document.</p>

<h3 id="what-is-next">What is next</h3>

<p>In the upcoming weeks, we will be preparing the first beta release and starting the process to reach a stable release.
<strong>To be able to deliver a good stable release it is important that you test the pre-release and share your feedback!</strong></p>

<p>To try the alpha version, you can run:</p>

<pre><code class="bash">composer require 'doctrine/migrations:^3.0@alpha'
</code></pre>

<p>If you are using Symfony:</p>

<pre><code class="bash">composer require 'doctrine/doctrine-migrations-bundle:^3.0@alpha' 'doctrine/migrations:^3.0@alpha'
</code></pre>

<p>You can be also more brave trying the development versions by specifying <code>@dev</code> instead of <code>@alpha</code> 
when requiring the composer dependencies above.</p>

<p>You can also have a look at the <a href="https://github.com/doctrine/migrations/releases/tag/3.0.0-alpha1">release notes</a> 
and the <a href="https://github.com/doctrine/migrations/blob/3.0.0-alpha1/UPGRADE.md">upgrading</a> document.</p>

<p>Similarly you can also have a look at the <a href="https://github.com/doctrine/DoctrineMigrationsBundle/releases/tag/3.0.0-alpha.1">release notes</a> 
and the <a href="https://github.com/doctrine/DoctrineMigrationsBundle/blob/3.0.0-alpha.1/UPGRADE.md">upgrading</a> document for the Symfony bundle.</p>

<p>In the alpha release, breaking changes are still possible. 
In the beta, release breaking changes are possible but will happen
only if we will find very unexpected behaviors. 
When the alpha and beta phase will be completed, a stable version will be made available.</p>]]></content>
        </entry>        <entry>
            <title>Make the difference when contributing to open-source</title>
            <link href="https://www.goetas.com/blog/make-the-difference-when-contributing-to-open-source"/>
            <updated>2019-10-22T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/make-the-difference-when-contributing-to-open-source</id>                <summary>Collaboration in open-source it is always a hard thing.  It is a delicate balance of freedom, ideas, rules, time, money and motivation.  If you decide to contribute, make your contributions count. Do not waste precious resources.
</summary>                <content type="html"><![CDATA[<p>Collaboration in open-source it is always a hard thing. 
It is a delicate balance of freedom, ideas, rules, time, money and motivation and many other factors.</p>

<p>This post focuses on the PHP community, as it is the one I'm in contact with. 
I’m pretty sure it applies to many other communities.</p>

<h2 id="make-the-difference">Make the difference</h2>

<p>The PHP community has matured a lot in the last decade, we moved from a “development in the wild” period (2000-2008?)
 to a more organized community with standards, best practices, frameworks, meetups, conferences, and much more.</p>

<p>Some big projects emerged, with thousands of contributors. 
If you look more carefully at those projects you will notice that even if there are a lot of contributors, 
there are always some individuals that emerge from the crowd.</p>

<ul>
<li>Symfony has ~2000 contributors, but most of its code it is written by 10-15 people </li>
<li>Doctrine ORM has ~600 contributors, but most of its code it is written by 6-10 people </li>
<li>Laravel has ~2000 contributors, but most of its code it is written by 3-5 people </li>
<li>Guzzle has ~300 contributors, but most of its code it is written by 1-3 people </li>
<li>Curl (a very popular C library) has ~600 contributors, but most of its code it is written by 1-3 people </li>
</ul>

<p>What does that mean? <strong>Most of the code we use nowadays is the result of tears and sweat of a few individuals.</strong></p>

<p>If we also look at how some PHP projects evolved, 
hundreds of people have contributed to those projects, but there has always been a limited group of people
 who decided to take the responsibility and push the project forward by changing architecture, 
 adding groundbreaking features or working hard to deliver new development standards. 
 Each major change has been done again by a few individuals.</p>

<ul>
<li>Doctrine 2.0 when released was written mainly by 3 developers</li>
<li>Doctrine Second Level Cache has brought in 14k likes of code, written by a single developer</li>
<li>The performance improvements in PHP 7.0 are the result of the work of two developers</li>
<li>Each new Symfony Component is the result of the initiative of a single developer</li>
</ul>

<p>Of course all this work gets reviewed by many other contributors, but in the end, 
<strong>somebody has to sit behind a chair and write the code</strong>.</p>

<p>When talking about open source, the focus is very often on the “community” and the long tail of contributions, 
but those contributions are possible only thanks to the huge work by the few individuals who maintain and 
keep running the projects we use our daily work. It takes a lot of courage to step into the open-source world.</p>

<p>In the last few years, as part of the PHP evolution, we got strict coding standards, static analysis, immutability, 
and many other things. 
I have the feeling that those new concepts shifted the attention from important changes, 
to a bunch of small and tedious cosmetic changes. 
Formatting, sorting of array keys, static analysis, naming conventions, so-called Yoda-conditions, docblocks and much more 
became the focus for many contributions. 
There have been <a href="https://github.com/composer/composer/pull/7803#issuecomment-441607476">cases</a> as 
where maintainers started to reject such kind of pull requests. 
Unfortunately, also many code review sessions tend to focus on code style, immutability, strict 
adherence to some way of writing code.</p>

<p>Instead of challenging and reviewing the architecture behind some changes or the foundations on which it is built, 
 attention and effort drifted away to those other aspects.</p>

<p>Initiatives such as Hacktoberfest, 24PullRequests and EU-fossa recently have promoted open source contributions. 
Unfortunately, all those initiatives focus on small contributions, often fixing small mistakes, typos or sometimes
 on the border of being <a href="https://github.com/doctrine/mongodb-odm/pull/2072">ridiculous</a>.
Some open source maintainers <a href="https://twitter.com/frankdejonge/status/1179774214443851776">have tried to put attention on those behaviors</a> 
but with not that much result.</p>

<p>Most of these initiatives don’t manage to motivate people to do bigger contributions. Contributions that span more than a day of work are rare.</p>

<p>Since important changes take time (more than a day), this brings us to the next point: the people behind open source and how open source is built.</p>

<h2 id="what-is-open-source">What is open source</h2>

<div class="text-center">

<p><img src="/img/posts/Strip-Vision-Open-source-650-finalenglish.jpg" alt="application and components" /></p>

<p><small><a href="http://www.commitstrip.com/en/2014/05/07/the-truth-behind-open-source-apps/">http://www.commitstrip.com/en/2014/05/07/the-truth-behind-open-source-apps/</a></small></p>

</div>

<p>This picture, in my opinion, explains the reality of open source development very well.</p>

<p>Most of open-source projects are often the result of the hard work of some individuals. Working in their private time 
(not always from a dark room as in the picture). 
In some cases, people even get paid to do open source, but in most cases it is through some other project that is 
not directly related to open source code itself. 
In very rare cases and for extremely popular projects (as programming languages) it is possible to see people working 
full time on open-source.</p>

<p><strong>Doing a major refactoring of a library, releasing a new component for a framework, 
fixing a not trivial bug can take easily more than 100h of work</strong>. 
Yes, I said 100, it is 2-3 weeks of work full time. It is giving away more than half of your monthly salary.
This means that the time people invest in open source is extremely valuable.</p>

<p>Each refactoring or bigger change that happens in open source, is when someone decides to not earn those 2-3 weeks of salary. 
<strong>This precious time, gifted us by people who run open-source projects (both maintainers and contributors) 
should not be taken for granted nor wasted in non-important things.</strong></p>

<p>Yes, some companies have 1-3 days/month where employees are allowed to contribute to open-source during work time. 
A nontrivial contribution that requires 100h of work will be completed in ~7 months...</p>

<p>I’m not saying here that companies/people should contribute more to open source. 
This has been already said multiple times. 
Companies and individuals are already contributing a lot. I’m saying that those <strong>contributions should be braver</strong>.</p>

<p>Contributors should stop “hacking” into the code the mere bugfix/feature they need. A more broad approach should be taken. 
Evaluate refactorings, encourage code reuse, offering help in triaging issues, writing documentation. 
Understanding the library and seeing how it can be improved it is the right thing to do.</p>

<p>Of course, this approach requires way more time, but contributing in this way makes the project better. 
By reflex makes the community and the ecosystem stronger, and by reflex again goes back to companies who contributed. 
Those companies can benefit from a healthier community, able to give them better support in their daily issues.</p>

<p>Most of the open-source projects we have nowadays are the result of someone who decided to not “hack” a 
solution for a problem, but decided to step back, and implement a proper more decoupled solution, one 
that can be shared with others so they can benefit from it.</p>

<p>Since we have limited time, and good contributions take time… how to do it?</p>

<h2 id="how-to-contribute">How to contribute</h2>

<p>Any contribution is good... almost.</p>

<p>Contributing to an open-source project is not mandatory nor is it a challenge to get a free t-shirt. 
Each minute invested in open-source by a contributor requires a minute to be invested by a maintainer. 
So make your contribution valuable.</p>

<p>In most cases, this means talking to the maintainers, checking what their goals, availability, 
and the direction they want to take the project.</p>

<p><strong>Try. Be brave. Challenge the current status quo. Be ready to be challenged. 
Be ready to give up. Be ready to be put aside.</strong></p>

<p>Don’t create a 10k lines of code contribution that maintainers want but are not able to take in. 
At the same time don’t create a one-line contribution that doesn’t add any value to the project.</p>

<p>Balance your time properly. 
Carefully choose levels of strictness, architecture, project guidelines adherence, best practices. 
Remember that a delivered contribution that improves the project by 20% is better than a contribution 
that would improve the project by 80%, but can’t be merged because it’s too complex.
Choose whatever approach you feel comfortable and try it!</p>

<p>Sometimes you will get good suggestions from other people that will help you to improve your work. 
Sometimes you will get good suggestions, 
but those suggestions might put you on the spot to not be able to deliver your work. 
It can be because you don’t have yet some expertise or maybe just because you do not have the time to do so. 
Choose carefully what will help you to deliver and what not.</p>

<p>Measure your time, your motivation, your ability to make an impact, 
community motivation and community interest in your contribution.</p>

<p>Take a deep breath, and just jump! You will not regret it!</p>]]></content>
        </entry>        <entry>
            <title>How to use external services with the Symfony Validator</title>
            <link href="https://www.goetas.com/blog/how-to-use-external-services-with-the-symfony-validator"/>
            <updated>2019-10-07T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-to-use-external-services-with-the-symfony-validator</id>                <summary>In this article we are going to see a particular case of validation using the Symfony Validator expression language  and accessing third party services.
</summary>                <content type="html"><![CDATA[<p>Validation is one of the most common tasks when building many types of software applications.</p>

<p>Talking more specifically about the <a href="https://symfony.com">Symfony Framework</a>, 
its <a href="https://symfony.com/doc/current/validation.html">Validator</a> component offers
a very powerful set of APIs to validate objects, arrays, forms and much more.</p>

<p>We are going to see a particular case of validation that requires third party services.
This question emerged during <a href="/talks/symfony-live-berlin-2019-great-apis-with-symfony-and-open-source-software">my talk</a> 
at <a href="https://berlin2019.live.symfony.com">Symfony Live</a> in Berlin last week.</p>

<h2 id="the-problem">The problem</h2>

<p>Let's consider the following class:</p>

<pre><code class="php">&lt;?php
use Symfony\Component\Validator\Constraints as Assert;

class User 
{
    /**
     * @var string
     * @Assert\Length(min="2", max="32")
     */
    public $nickname;
}
</code></pre>

<p>Using the <code>@Assert\Length</code> annotation we want to ensure that the nickname length is between 2 and 32 characters.
What about having a list of not allowed nicknames? That can be done using the <code>@Assert\Expression</code> assertion, 
that allows us to use the Symfony Expression Language in our validation rules.</p>

<pre><code class="php">&lt;?php
use Symfony\Component\Validator\Constraints as Assert;

class User 
{
    /**
     * @var string
     *
     * @Assert\Expression(
     *     "this.nickname not in ['admin', 'superuser']",
     *     message="This nickname is not allowed!"
     * )
     */
    public $nickname;
}
</code></pre>

<p>In this way we do not allow "admin" and "superuser" as nicknames.
 <strong>What if we want this list of not allowed nicknames to be provided by another "service" available in our application?</strong> 
With the default symfony validator setup this is not possible.</p>

<p>The expression language used by the validator component has a very limited set of features and can not refer 
to other parts of the application except of the object on which the validation is performed 
(through the <code>object</code> variable).</p>

<h2 id="the-solution">The solution</h2>

<p>Luckily it is pretty easy to re-configure the expression language provider and much more powerful.</p>

<p>We need to change the <code>validator.expression</code> expression language with a different instance having a wider
access to system services.</p>

<p>This can be done using a compiler pass and hooking into the Symfony Dependency Injection process.
We create  <code>src/DependencyInjection/CompilerPass/ValidatorExpressionLanguagePass.php</code> with the following content:</p>

<pre><code class="php">&lt;?php

namespace App\DependencyInjection\CompilerPass;

use Symfony\Component\DependencyInjection\Compiler\CompilerPassInterface;
use Symfony\Component\DependencyInjection\ContainerBuilder;
use Symfony\Component\DependencyInjection\Reference;

class ValidatorExpressionLanguagePass implements CompilerPassInterface
{
    public function process(ContainerBuilder $container)
    {
        $container-&gt;getDefinition('validator.expression')-&gt;setArguments([
            null, 
            new Reference('security.expression_language')
        ]);
    }
}
</code></pre>

<p>We change the default validation expression language to <code>security.expression_language</code> 
(provided with the symfony security component), so we can use 
 the provided <code>service</code> and <code>parameter</code> functions to interact easily with the Symfony DI Container.</p>

<p>Next step is to register the <code>ValidatorExpressionLanguagePass</code> compiler pass in the <code>Kernel.php</code>.</p>

<pre><code class="php">&lt;?php

namespace App;

//..
use App\DependencyInjection\CompilerPass\ValidatorExpressionLanguagePass;

class Kernel extends BaseKernel
{
    //..
    protected function configureContainer(ContainerBuilder $container, LoaderInterface $loader)
    {
        // ...
        $container-&gt;addCompilerPass(new ValidatorExpressionLanguagePass());
        // ...
    }
}
</code></pre>

<p>With this change, now is possible to use other public services inside the expression language.</p>

<pre><code class="php">&lt;?php
use Symfony\Component\Validator\Constraints as Assert;

class User 
{
    /**
     * @var string
     *
     * @Assert\Expression(
     *     "this.nickname not in service('app.repo.forbidden_nicknames').getForbiddenNicknames()",
     *     message="This nickname is not allowed!"
     * )
     */
    public $nickname;
}
</code></pre>

<p><strong>Et voilà!</strong></p>

<p>In this case the list of not allowed nicknames is provided by a hypothetical <code>app.repo.forbidden_nicknames</code> service, 
that can be any other service part of your application.</p>

<h3 id="p.s.">P.S.</h3>

<p>We can use any other expression language instance instead of <code>security.expression_language</code>.</p>

<p>To make an example, the <code>jms/serializer-bundle</code> provides the <code>jms_serializer.expression_language</code> 
that has built in functions such as <code>service</code>, <code>is_granted</code> and <code>parameter</code>. You can choose it 
if you want to use the <code>is_granted</code> function that is not provided by the security expression language instance.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Hope you enjoyed this article, if you have some feedback, do not contact me or to send me a tweet!</p>]]></content>
        </entry>        <entry>
            <title>Changing credentials on Docker Swarm Services</title>
            <link href="https://www.goetas.com/blog/changing-credentials-on-docker-swarm-services"/>
            <updated>2019-07-01T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/changing-credentials-on-docker-swarm-services</id>                <summary>This post will give you and overview on how docker performs the authentication into image registries,
 and how changes to the credentials alters the workflow when multiple nodes are involved.
     
</summary>                <content type="html"><![CDATA[<p>This post will give you and overview on how docker performs the authentication into image registries,
 and how changes to the credentials alters the workflow when multiple nodes are involved.</p>

<p>The first part of this post gives and overview of what docker is, where docker images are stored and what <code>docker login</code>
does.
If you already know some basics on how Docker and Docker Swarm works, you can skip directly 
<a href="#downloading-private-images">here</a>.</p>

<h3 id="docker-basics">Docker basics</h3>

<p>Docker has a client-server architecture and most of the <code>docker</code> commands are communicating with the docker daemon via some REST-ish API.</p>

<p>Running <code>docker -H tcp://1.2.4.5:2375 run --rm nginx</code> will contact the docker daemon on the <code>1.2.4.5</code> host and
will instruct it to download and run the <code>nginx</code> image.</p>

<h3 id="docker-swarm">Docker swarm</h3>

<p>Docker Swarm is a container orchestrator that allows you to run docker containers across multiple nodes. 
It allows you also to abstract from the nodes that are running the Swarm, you decide the desired state
and Docker Swarm will distribute the load across the nodes forming the cluster.</p>

<h3 id="docker-image-registry">Docker image registry</h3>

<p>Docker images are stored into a "registry" and images can be uploaded and downloaded from/to it by the docker daemon.
Most of the registries require authentication.</p>

<h3 id="docker-login">Docker login</h3>

<p>To authenticate a docker client into a docker image registry you can run:</p>

<pre><code class="bash">docker login
</code></pre>

<p>This command will try to authenticate us into <a href="https://hub.docker.com/">hub.docker.com</a> (the commercial docker registry), 
 asking us username and password. 
you can also login into other registries (offered by other companies or hosting our own registries) 
by specifying the registry host name as third parameter.</p>

<p>As example  <code>docker login registry.gitlab.com</code> will try authenticate us into the Gitlab registry.</p>

<p>By default credentials are saved into <code>$HOME/.docker/config.json</code> on the computer running the <code>docker login</code> command
(there are also <a href="https://docs.docker.com/engine/reference/commandline/login/#credentials-store">other</a> storages).</p>

<p>To notice that <code>docker login</code> is a "client" command, and does not communicate with the docker daemon.</p>

<h2 id="downloading-private-images">Downloading private images</h2>

<p>What happens when you create/update a Docker Swarm service using the <code>--with-registry-auth</code> option 
(or deploy a Docker Stack with the same option)?</p>

<p>The docker client will:</p>

<ol>
<li>Collect a list of docker images to download</li>
<li>Read the authentication informations from the <a href="https://docs.docker.com/engine/reference/commandline/login/#credentials-store">credentials-store</a> for the images to download (for instance reading the <code>$HOME/.docker/config.json</code> from the computer where the client is installed) </li>
<li>Create (or update) the service in the Swarm <a href="https://docs.docker.com/engine/swarm/raft/">RAFT</a> state.
The state information <strong>include also the authentication information on how to download the image</strong>.</li>
</ol>

<p>In this way when the container will be scheduled on some node, 
the docker daemon will read the RAFT state and forward the stored credentials to that node, allowing it do download autonomously the docker image.</p>

<h3 id="changing-credentials">Changing credentials</h3>

<p>If you update your credentials (as example change your password), 
the credentials stored in the Swarm RAFT state are not correct anymore and new nodes will not be able to download images.</p>

<p>As a solution, you should re-authenticate into the image registry using the <code>docker login</code> command 
and update the credentials stored into the Swarm RAFT state by running:</p>

<pre><code class="bash">docker login

docker service update SERVICE_NAME --with-registry-auth
</code></pre>

<p>This command will leave the service as it is, it will just update the credentials.</p>

<p>The same strategy can be used for the Docker Stack, 
but is more risky as it could trigger unwanted container updates since Docker in some cases might try to download a newer 
image (if you update a docker image tag) or re-run services that are configured as <code>restart: never</code>.</p>

<h3 id="other-things-to-consider">Other things to consider</h3>

<p><strong>Which services should I update?</strong></p>

<p>If you have a lot of services (maybe using different registries) it might be difficult to keep track of which credentials 
 should be updated.</p>

<p>Services can be labeled (using  the <code>--label</code> or <code>--label-add</code> options), and later you can update only the services having a specific label.</p>

<pre><code class="bash">docker login -u "me" -p "xxxxxx" registry.gitlab.com

for i in $(docker service ls --filter=label=gitlab-registry); do docker service update "$i" --with-registry-auth -d; done
</code></pre>

<p>This  script will update the credentials for all the services having the <code>gitlab-registry</code> label.</p>

<p><em>Note: It is not safe to provide the password as commandline argument, 
find <a href="https://docs.docker.com/engine/reference/commandline/login/#provide-a-password-using-stdin">here</a> some alternatives.</em></p>

<p><strong>Expiring credentials</strong></p>

<p>Some docker registries (AWS or Azure) have credentials that expire automatically after a specified amount of time.
Also in this case the credentials stored in the Swarm RAFT state will be not correct anymore and new nodes
will not be able to download images.</p>

<p>As a solution, it is possible to setup a cronjob to perform periodically the update.</p>

<h3 id="there-is-a-trap%21">There is a trap!</h3>

<p>If you have a <a href="https://hub.docker.com/">DockerHub</a> account (the commercial docker registry), and you did login into it using
the <code>docker login</code> command, and you created/updated a service or stack using the <code>--with-registry-auth</code> option, 
 <strong>those credentials WILL be used to retrieve the public images</strong> (that could have been downloaded anonymously otherwise).
This means that if you change credentials on DockerHub, your nodes will not be able to download public images.</p>

<p>To mitigate this issue make sure to not use the  <code>--with-registry-auth</code> option 
when creating or updating a service that uses public images.
Unfortunately if you are deploying a Docker Stack that contains mixed services (with private and public images),
the <code>--with-registry-auth</code> is global and can not be specified per single service. 
In alternative you can use the same labeling strategy described before and update also the credentials for public services.</p>]]></content>
        </entry>        <entry>
            <title>Traps on the way of Blue-Green deployments</title>
            <link href="https://www.goetas.com/blog/traps-on-the-way-of-blue-green-deployments"/>
            <updated>2019-06-14T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/traps-on-the-way-of-blue-green-deployments</id>                <summary>Docker (as well as Kubernetes) gives you a way to update your applications with no-downtime through a strategy called Blue-Green deployment. Here we can see a common trap (and solution) when building PHP-based applications and  doing blue-green deployments.
     
</summary>                <content type="html"><![CDATA[<p>Docker (as well as Kubernetes) offers you a way to update your applications with no-downtime through a common strategy called
Blue-Green deployment.</p>

<p>Blue-Green deployments work in this way:</p>

<ol>
<li>Your currently deployed application ("Green") is serving the incoming traffic.</li>
<li>A new version of your application is deployed ("Blue") and tested, but is not yet receiving any traffic.</li>
<li>When "Blue" is ready, we can start sending the incoming traffic to "Blue" too.</li>
<li>At this point we have two copies of our application running in parallel (the "Green" and the "Blue").</li>
<li>Now we have to stop sending incoming traffic to the "Green" application, "Blue" is handling all the incoming traffic.</li>
<li>Since "Green" is not receiving any traffic anymore, it can be safely removed.</li>
<li>The "Blue" will be marked as "Green", allowing in the future a deploy of a newer version using the same strategy.</li>
</ol>

<h2 id="docker-swarm-blue-green-deployment">Docker Swarm blue-green deployment</h2>

<p>If you are using Docker Swarm, this can be a stack file that implements the blue-green deployment strategy.</p>

<pre><code class="yaml"># app.yml

version: '3.4'
services:
  app:
    image: acme/todo-list:${VERSION}
    deploy:
      update_config:
        order: start-first
</code></pre>

<p><code>acme/todo-list</code> is a simple todo-list web application. The <code>update_config.order: start-first</code> instructs docker swarm to 
use the blue-green deploy strategy.</p>

<p>Do deploy the <code>v1</code>  of our todo-list in a Docker-Swarm cluster we can run:</p>

<pre><code class="bash">VERSION=v1 docker stack deploy todolist_app -c app.yml
</code></pre>

<p>If we want to update the app to <code>v2</code> we can run:</p>

<pre><code class="bash">VERSION=v2 docker stack deploy todolist_app -c app.yml
</code></pre>

<p>The update will follow the blue-green deploy strategy as described before. 
Docker will keep <code>v1</code> running and will deploy <code>v2</code>. 
When <code>v2</code> is ready, it will redirect all the traffic to <code>v2</code> and will remove <code>v1</code>. Neat!</p>

<p>But if we look at the logs of our load balancer we can see something as:</p>

<pre><code>1.2.3.4 - - [13/Jun/2019:06:00:10 +0000] "GET /toto/list HTTP/1.1" 502 150 ....
</code></pre>

<p>The status code is <code>502</code>, "Bad Gateway". 
This HTTP status code that means that the load balancer has received an invalid response from the application server (or no response).
Why is that?</p>

<p>To remove <code>v1</code>, after stopping to send incoming traffic, Docker sends the <code>SIG_TERM</code> signal to the app and waits<br />
up to 10 seconds for the app to gracefully terminate itself. 
If <code>v1</code> is still running after 10 seconds, Docker brutally kills the <code>v1</code> app.
This will terminate any connection the app had (and pending requests will receive 502 error).</p>

<h3 id="graceful-stop">Graceful stop</h3>

<p>We can change the amount of seconds Docker will wait before removing the container by tuning the <code>stop_grace_period</code> parameter:</p>

<pre><code class="yaml"># app.yml

version: '3.4'
services:
  php:
    image: acme/todo-list:${VERSION}
    stop_grace_period: 120s
    deploy:
      update_config:
        order: start-first
</code></pre>

<p>With this configuration, after sending the <code>SIG_TERM</code> signal, docker will wait up to two minutes before killing the app.
Depending on the specific logic and response times of your application, you can tell to docker how long to 
 wait for your application to terminate before forcing it.</p>

<h3 id="blue-green-deployments-and-php-fpm">Blue-green deployments and PHP-FPM</h3>

<p>If you are using PHP-FPM, the previous configurations might not be enough.</p>

<p>Unfortunately (?) PHP-FPM is configured by default to terminate immediately after receiving the <code>SIG_TERM</code> signal.
Even if docker is ready to wait for 10 seconds (or any other value you might have configured with <code>stop_grace_period</code>) 
PHP will terminate itself (and all the requests being served) without waiting.</p>

<p>This will lead again to <code>502</code> errors.</p>

<p>To solve this we have to instruct also PHP to give enough time itself to complete serving the pending requests, 
tuning <code>process_control_timeout</code> parameter (check <a href="https://www.php.net/manual/en/install.fpm.configuration.php">here</a> for a full list of PHP-FPM configurations).</p>

<p>By setting  <code>process_control_timeout = 5</code>, 
PHP-FPM will wait up to 5 seconds before exiting and killing all the processes that were serving requests.</p>

<p>We can add this parameter in the <code>Dockerfile</code> when building our PHP image.</p>

<pre><code class="dockerfile"># Dockerfile
FROM php:fpm

# ... 

RUN { \
    echo '[global]'; \
    echo 'process_control_timeout = 5'; \
    } | tee /usr/local/etc/php-fpm.conf

# ... 
</code></pre>

<p>In the same way how docker was waiting for a container to finish its job, now PHP will do the same and wait up to 5 seconds
for its child processes to finish serving the requests.</p>

<p>In this way we configured how long docker should wait before terminating the container, and also how long PHP 
will wait to complete the requests.</p>

<p>If PHP is able to stop running in less than 5 seconds, it will do it (for instance when all the pending requests are served quickly). 
The same applies for docker. 
In this way, these timeouts are applied only for the worse case.</p>]]></content>
        </entry>        <entry>
            <title>Why and how some people run open source projects</title>
            <link href="https://www.goetas.com/blog/why-and-how-some-people-run-open-source-projects"/>
            <updated>2019-02-06T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/why-and-how-some-people-run-open-source-projects</id>                <summary>In the past weeks I asked some people to answer few questions on how and why they maintain/develop open source projects. The results are here.
     
</summary>                <content type="html"><![CDATA[<p>In the past weeks I asked some people to answer few questions on how and why they maintain/develop open source
projects.</p>

<p>I decided to do this survey moved mainly by the desire to explore new ways of funding open source software.
It has to be said that the current way of financing open source software in my opinion is wrong at its foundation,
but despite of that somehow it still works... (more about this at the end of the article).</p>

<p>My survey results probably are biased by the dataset that produced the results. 
I have asked 30 people to fill the questionnaire and actually 17 of them answered (was hoping a better response rate). 
Those people were carefully selected, all of them are maintainers/authors of some of the most popular 
PHP projects. This group of people most probably can give an idea on the PHP community, but I'm pretty sure
that the Java, C++ or Javascript communities have some differences if compared to the people that maintain PHP projects.</p>

<h2 id="the-survey-results">The survey results</h2>

<p>In PHP, open source software seems to be built mainly by hobbyists that invest up to 20 h/week to keep the PHP ecosystem running.
Some of them maintain alone up to 10 different projects.
Almost 3/4 of the responders are employed, but a good sign is that almost 50% of the maintainers said that their contributions are 
done both in their personal time and at work.<br />
On the other side, a very small amount of people said that has been paid in the past to work open source.</p>

<p>One of the most interesting results are the mixed answers received when asking if they are interested in earning money 
for what they do. It seems that many people are not strongly interested in being paid for their work, 
 as it looks that gives them a general satisfaction that goes beyond money.</p>

<p>Detailed results can be found <a href="https://docs.google.com/spreadsheets/d/147d1WK07JTUM31PZCxxwJ0w-9CxGw_Mfb3byzpFZNag/edit?usp=sharing">here</a>.</p>

<h2 id="a-personal-point-of-view">A personal point of view</h2>

<p>I'm not going to talk about the philosophical reasons of open source software since in profoundly believe in them,
 but will talk more about the financial part of it.<br />
It is more than 10 years that I regularly try to give back to the community, maintaining my self 
<a href="/opensource/">some open source projects</a>, so my interest anyway is not mainly financial.</p>

<p>Currently the open source is mainly built by people in their personal time. Of course there are exceptions.
For those that get somehow paid, from what I can see, this are the ways how they manage to do it:</p>

<p>If contributions are done in work time:</p>

<ul>
<li>Some companies believe in open source and some of they employees work full time or part time on open source 
(This companies are very few in the world and their contributions are often done on projects that have world-wide impact).</li>
<li>Some companies believe in open source their employees have 0.5, 1 or 2 days a month when they are allowed to contribute in work time.</li>
<li>Similar to the other case, if the developer and company-owner are the same person, they allow them-self to invest some 
of their time in contributing.</li>
<li>Some contributions are done without the company management knowing about it.</li>
</ul>

<p>If contributions are done in personal time (this is the majority of the cases):</p>

<ul>
<li>most of them are done for free</li>
<li>some lucky projects manage to become business ideas</li>
<li>some projects have bounties-programs to fix bugs or implement features (sometimes few dollars for weeks of work) </li>
<li>some people have a Patreon, OpenCollective (or similar) profile where developers can receive donations from people for their
work.</li>
</ul>

<h2 id="the-donations-strategy">The donations strategy</h2>

<p>One of the way I less like to finance open source projects are donations. 
I feel that donations are some kind of charity given to contributors.
This diminishes the value of their work in my opinion.
I personally feel a professional developer, and I would like to be paid for the professional part of my work, 
not to be given some tips as in a restaurant.</p>

<p>The more sad part of donations is that very often they are made by other developers, spending they private money 
to say "thank you" to project maintainers. 
The real beneficiary of those projects, companies, are the one that donate the less.</p>

<h2 id="some-conclusions">Some conclusions</h2>

<p>I'm not here to propose a solution as I feel that I do not have one. 
Would like to see in the future more companies encouraging their employees to use their work time to contribute.
And yes, 0.5 days a month is not enough! It sounded as a joke to me the first time I heard it.</p>

<p>On the other side, even if companies allow their employees/developers to contribute, I feel that those developers 
needs to be educated on the value that they are giving (and in some cases asking) to the community. 
To understand the costs of forking projects, making random changes and then having to maintain those changes on their own, 
instead of trying to make those changes compatible with the original project and sending them back.
I feel that those developers needs to be educated on the value of contributing back to open source. 
Only when that concept will be properly understood, they will be able to 
educate their employers on the value of those contributions and why each company should do it.</p>

<hr>

<p>Header image <a href="https://www.flickr.com/photos/nengard/5755231592/in/photolist-9Lz4QU-4ePgT-6dJkCr-py8bjm-2e58LK6-5BPdKf-7DjV3i-7v3Dx-5BZWPJ-9oxKX4-8vA62c-8vD7yq-2aRkiXW-9AbgQj-ptrgGX-H8A8Rf-8Hxyjh-6tY9k-2cFhNZ8-bCzfj6-H8A6nC-PRKrpi-2cY8xqh-H8A5w9-9XQVw7-2dZuY1b-5CiuYw-cse3q5-6dNu9Y-9oLE1K-6xKpyE-H8A5gu-fbCh8j-ouZtDk-H8A6EG-2aRkiUu-2cegfiz-d46Wid-6dNuMS-dbQNMW-6qrAdE-2cegiNv-8HupwT-2cFhApD-2aRkiR3-5URjyn-iSunw-2cFhPQB-pcefSY-2dxpRwW">nengard</a>.</p>]]></content>
        </entry>        <entry>
            <title>What&#039;s new in jms/serializer v2.0</title>
            <link href="https://www.goetas.com/blog/whats-new-in-jmsserializer-v20"/>
            <updated>2018-09-12T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/whats-new-in-jmsserializer-v20</id>                <summary>After more than a two years of work, jms/serializer v2.0 is going to see the light. Here a preview of the changes, new features and improvements that will be released soon.
</summary>                <content type="html"><![CDATA[<p>After more than a two years of work (the <a href="https://github.com/goetas/serializer/pull/3/commits">first commit</a> 
was on July 20 2016), <a href="https://github.com/schmittjoh/serializer">jms/serializer</a> <strong>v2.0</strong> is going to see the light.</p>

<p><code>jms/serializer</code> is a popular library to serialize/deserialize objects from/to XML and JSON. 
The installs counter on <a href="https://packagist.org/packages/jms/serializer/stats">packagist</a> has already passed 20 million downloads.</p>

<p>The version 1.x works well but over the years has accumulated inconsistent behaviours 
and even if is one of the fastest php serializers, I've realized that there is still room for improvements.
The version v2.0 is not a big-bang release but is a improved version of what was available in the 
previous 1.x series.</p>

<p><strong>Upgrading from 1.x to 2.0 should be really simple and in some cases fully transparent.</strong>
The metadata format is untouched and the user-api is almost identical.
If you have implemented some more advanced features on top of the serializer by overriding core classes, 
things might be more complicated but is mainly about adding PHP 7.2 type hints 
(7.2 is the minimum PHP version for the 2.0 version).
Here is the initial <a href="https://github.com/schmittjoh/serializer/blob/master/UPGRADING.md">UPGRADING</a> guide.</p>

<p>Side note. Yes <code>jms/serializer</code> finally has a logo!</p>

<div class="text-center">

<p><img src="/img/posts/jms/logo.png" alt="jms/serializer logo" /></p>

</div>

<h2 id="license">License</h2>

<p><code>jms/serializer</code> 2.0 is licensed under the permissive <a href="https://opensource.org/licenses/MIT">MIT</a> license
 (and also the 1.x versions starting from 1.12.0).
This solves some adoption issues of the serializer in other projects that have license incompatibilities with the 
previous serializer license, the Apache-2.0 license.</p>

<h2 id="improvements">Improvements</h2>

<p>Here a quick list of improvements that are immediately noticeable when using the version 2.0.</p>

<p><strong>1) Performance</strong></p>

<p>The new serializer is approximately <strong>35% faster</strong> than the v1.x and consumes 3% less memory.</p>

<p>Achieving this was possible by moving to  more information to compiled metadata, 
by reducing the number of <code>context</code> accesses, by using simpler access strategies, by using a faster event dispatcher
and few other tricks. 
In most of the cases was about reducing the number of repetitive operations by moving them in a "place" where they 
need to be invoked only one time per serialization call (instead of computing the operation for each object present in the 
object graph).</p>

<p>I did some benchmark by using a the <a href="https://github.com/egeloen/ivory-serializer-benchmark">ivory-serializer-benchmark</a>.
The JMS serializer was configured to use the APCu cache for metadata and annotations (by default files are used), 
the Symfony serializer was configured as in <a href="https://github.com/egeloen/ivory-serializer-benchmark/pull/8">this</a> PR 
and the Ivory serializer was configured by the author of the original benchmark.</p>

<p>The JMS serializer is the only serializer with complete Doctrine support out of the box. To make a more fair comparison, 
there are two versions of the JMS benchmark, the standard one (with all the features enabled) and the minimal with Doctrine 
support disabled.</p>

<p><a href="https://github.com/goetas/ivory-serializer-benchmark/tree/failing-hoa">Here</a> is available the source code used for the 
benchmark and <a href="https://travis-ci.org/goetas/ivory-serializer-benchmark/builds/393520712">here</a> are some results from 
a recent TravisCi build (n.</p>

<p><a href="https://docs.google.com/spreadsheets/d/1_ruZTW1_9AeBfxiT4mjpYRW6KCucMFJflLW4Pl1Sxtw">Here</a> a more detailed set 
of benchmarks run on my laptop (Intel® Core™ i7-8550U 16GB RAM). 
In all the benchmarks I've excluded the Symfony Object Normalizer benchmark, its performance are so bad that this graphs 
will become unreadable if I had to fit also that in the graph area.
The following graphs summarize the benchmark results (lower is better).</p>

<div class="text-center benchmark-graphs">

<p><img src="/img/posts/jms/100-100.png" alt="complex object graph" />
<img src="/img/posts/jms/10-10.png" alt="mid-complex object graph" />
<img src="/img/posts/jms/1-100.png" alt="mid-complex object graph" />
<img src="/img/posts/jms/10-1.png" alt="simple object graph" />
<img src="/img/posts/jms/1-10.png" alt="very simple object graph" />
<img src="/img/posts/jms/1-1.png" alt="trivial object graph" /></p>

</div>

<p>Let's explain the meaning of this benchmark, horizontal and vertical complexity are characteristic of the 
benchmarked object graph.</p>

<ul>
<li>Big values of <strong>horizontal</strong> complexity indicates a very "deep" object graph, with many objects having many child objects 
(<code>horizontal=100</code> indicates an object with a chain of 200 nested objects)</li>
<li>Big values of <strong>vertical</strong> complexity indicate objects that have many children, but those children have no more child objects
(<code>vertical=100</code> indicates an object having 200 direct child objects)</li>
<li>Big values of <strong>vertical</strong> and <strong>horizontal</strong> complexity indicates a very complex object graph with the combination of the previous two cases</li>
<li>Small values of <strong>vertical</strong> and/or <strong>horizontal</strong> complexity indicates very simple object graphs with one, two of few nested objects</li>
</ul>

<p><em>Benchmark Conclusions</em>: what can be concluded from this benchmark is that the version 2.0 got ~30% better on not trivial 
object graphs, but it got worse on very simple cases when compared to the 1.x.<br>
A personal opinion on the results, is that on trivial cases we are talking about a difference of 
300 micro-seconds when serializing a graph of less than 5-10 objects. Benchmarking such small workload makes no sense.
The difference between v1 and v2 mostly is caused by a more complex internal structure as graph factories and visitor factories
introduced in v2. Another conclusion is that this is a benchmark on big graphs with no database access, 
if your application needs to serialize 5-50 objects then the performance impact is not that visible. Choose the serializer
that has the features and DX you want/need.</p>

<p><strong>2) Simpler context object</strong></p>

<p>The v2.0 does not depend on the <code>phpcollection</code> library anymore, this means that "context attributes" now are a plain PHP
array. This make the context API simple and intuitive.</p>

<p>v1.x:</p>

<pre><code class="php">&lt;?php
$context = SerializationContext::create();
$context-&gt;attributes-&gt;set("foo", "bar"); // set a value
$holder = $context-&gt;attributes-&gt;get("foo"); 
if ($holder instanceof Some) {
    $value = $holder-&gt;get(); // get a value
} else {
    // value was not in the context attributes
}
</code></pre>

<p>v2.x:</p>

<pre><code class="php">&lt;?php
$context = SerializationContext::create();
$context-&gt;setAttribute("foo", "bar"); // set a value 
if ($context-&gt;hasAttribute("foo")) {
    $value = $context-&gt;getAttribute("foo"); // get a value
} else {
    // value was not in the context attributes
}
</code></pre>

<p><strong>3) No more obsolete exclusion strategies</strong></p>

<p>Another unexpected behaviour in the context object were "obsolete exclusion strategies".</p>

<p>v1.x:</p>

<pre><code class="php">&lt;?php
$context = SerializationContext::create();
$context-&gt;setGroups(["foo"]);
$context-&gt;setGroups(["bar"]);

// now the context has "foo" and "bar" as groups
</code></pre>

<p>v2.x:</p>

<pre><code class="php">&lt;?php
$context = SerializationContext::create();
$context-&gt;setGroups(["foo"]);
$context-&gt;setGroups(["bar"]);

// now the context has only "bar"
</code></pre>

<p>The new behaviour is more consistent and in line with what is considered standard.</p>

<p><strong>4) Recursive calls</strong></p>

<p>The visitors are fully stateless now, so it is possible to use it in recursive contextes.</p>

<p><strong>5) Metadata errors vs Runtime errors</strong></p>

<p>In 2.0, all the "metadata-related" errors extend the <code>JMS\Serializer\Exception\InvalidMetadataException</code> exception.
Thanks to this is possible to distinguish configuration errors from errors caused by invalid object state.</p>

<h2 id="fixed-inconsistencies">Fixed inconsistencies</h2>

<p>The number of improvements is pretty long, a full list can be found in the 
<a href="https://github.com/schmittjoh/serializer/blob/master/CHANGELOG.md">CHANGELOG.md</a>.</p>

<p>Here some of the most important.</p>

<p><strong>1) Stateless JSON visitor</strong></p>

<p>The old JSON visitor had a concept of a "root" node and the user has to take care of it. The user had to make sure 
the root was always present and to not overwrite it when already set.
This made custom handles not intuitive and this created many unexpected behaviours.</p>

<p>Let's see how was implemented a custom JSON handler for an hypothetical <code>User</code> object.</p>

<p>v1.x:</p>

<pre><code class="php">&lt;?php
use JMS\Serializer\GraphNavigatorInterface;
use JMS\Serializer\JsonSerializationVisitor;
use App\Entity\User;

final class CustomUserHandler implements SubscribingHandlerInterface
{
    public static function getSubscribingMethods()
    {
        return [
            [
               'direction' =&gt; GraphNavigatorInterface::DIRECTION_SERIALIZATION,
               'type' =&gt; User::class,
               'format' =&gt; 'json',
               'method' =&gt; 'serializeUserToJson',
            ]
        ];
    }

    public function serializeUserToJson(JsonSerializationVisitor $visitor, User $user)
    {
        // custom user serialization
        $data = [
            'username' =&gt; $user-&gt;getUsername()
        ];

        // extra root check
        if ($visitor-&gt;getRoot() === null) {
            $visitor-&gt;setRoot($data);
        }
        return $data;
    }
}
</code></pre>

<p>v2.x:</p>

<pre><code class="php">&lt;?php
use JMS\Serializer\GraphNavigatorInterface;
use JMS\Serializer\JsonSerializationVisitor;
use App\Entity\User;

final class CustomUserHandler implements SubscribingHandlerInterface
{
    public static function getSubscribingMethods()
    {
        return [
            [
               'direction' =&gt; GraphNavigatorInterface::DIRECTION_SERIALIZATION,
               'type' =&gt; User::class,
               'format' =&gt; 'json',
               'method' =&gt; 'serializeUserToJson',
            ]
        ];
    }

    public function serializeUserToJson(JsonSerializationVisitor $visitor, User $user)
    {
        // custom user serialization
        return [
            'username' =&gt; $user-&gt;getUsername()
        ];
    }
}
</code></pre>

<p><strong>2) <code>@MaxDepth</code> on object collections</strong></p>

<p>When using the "max depth" exclusion strategy, traversing arrays and collections (ArrayCollection from Doctrine as example)
had different behaviours. 
To achieve a desired "max-depth" of 1, for arrays was necessary declare it as <code>@MaxDepth(1)</code>, but for collections
was necessary declare it as <code>@MaxDepth(2)</code>. This was a very old bug that become effectively a feature in v1.x, in 2.0 this
has been fixed and now in both cases <code>@MaxDepth(1)</code> works as expected.</p>

<p><strong>3) Consistent Exclusion Strategies</strong></p>

<p>In v1.x the exclusion strategies were returning <code>null</code> in some cases, sometimes empty objects as <code>{}</code>  and other
 were effectively excluding the property.
Now the behaviour is more consistent and handles better collections and circular references 
(see <a href="https://github.com/schmittjoh/serializer/pull/895">#895</a>).</p>

<h2 id="other-changes">Other changes</h2>

<p>Many other changes allowed to reduce complexity and improve speed, here some of them:</p>

<ul>
<li><p>A relaxed version of the <a href="https://github.com/doctrine/coding-standard">doctrine/coding-standard</a> has been adopted as official 
coding standard. 
Now the codebase is checked by PHPCS on each build for coding violations allowing to avoid bugs to land in released code</p></li>
<li><p>The <code>VisitorInterface</code> has been replaced by two more specialized interfaces: <code>SerializationVisitorInterface</code> and 
<code>DeserializationVisitorInterface</code>.</p></li>
<li>The type parsing now uses <a href="https://hoa-project.net/En/Literature/Hack/Compiler.html">HOA Compiler</a>, an AST based lexer and parser.</li>
<li>Updated <code>jms/metadata</code> v2.0 that does not rely heavily on reflection as the v1.x allowing to gain some performance 
and saving some memory. </li>
<li>To improve performance, the graph-navigators and visitors are stateless and instantiated on each <code>serialize</code>/<code>deserialize</code>
call.</li>
<li>Removed support for "obsolete" libraries as Propel, PHPCollection and the minimum supported Symfony version is 3.0</li>
<li>Removed YAML serialization support (to not be confused with YAML metadata support that is still part of the serializer core)</li>
<li>Removed PHP metadata definitions support</li>
</ul>

<h2 id="what%27s-next%3F">What's next?</h2>

<p>Today the beta release has been tagged, in two-four weeks the RC release will be tagged.
Before entering in the RC stage it will be still possible to break things by introducing big changes (if necessary), 
but in the RC stage will not be possible to add features anymore till 2.0 will not be completed 
(eventual pull requests that are not bug-fixes will land in v2.1). 
Four weeks after the last RC, a stable version will be tagged.</p>

<p>In case of big changes in the beta period or bugs discovered in the RC phase, a second beta or RC release could happen, 
shifting the release process for one or two iterations.</p>

<p>In parallel, for Symfony users, will be released the new 
<a href="https://github.com/schmittjoh/JMSSerializerBundle">JMSSerializer bundle</a> (v3.0) that will integrate the new serializer 
into Symfony. The JMSSerializer bundle will follow the same release schedule as the serializer library.</p>

<p>There are many popular libraries that use the serializer, 
as <a href="https://github.com/willdurand/Hateoas">willdurand/Hateoas</a>, 
<a href="https://github.com/nelmio/NelmioApiDocBundle">nelmio/NelmioApiDocBundle</a> or
<a href="https://github.com/FriendsOfSymfony/FOSRestBundle">FriendsOfSymfony/FOSRestBundle</a> and also they need to be upgraded.
Hope to see some feedback from the PHP community.</p>

<h2 id="do-you-want-to-help%3F">Do you want to help?</h2>

<p>That's great! As you can see there is a lot of work that needs to be done.
Feel free to contact me via <a href="https://github.com/goetas">Github</a> or <a href="&#109;&#x61;&#105;&#x6c;&#116;&#x6f;&#58;&#x67;&#111;&#x65;&#116;&#x61;&#115;&#x40;&#103;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#109;">email</a>.
You can test it with your project, write documentation, tests or contribute to the project! Do not hesitate.</p>

<p>You can also support me on <a href="https://www.patreon.com/goetas">https://www.patreon.com/goetas</a>.</p>

<p>Feedback via email or comments are always welcome! :)</p>]]></content>
        </entry>        <entry>
            <title>How I approached software development and why I prefer PostgreSQL to MySQL</title>
            <link href="https://www.goetas.com/blog/how-i-approached-software-development-and-why-i-prefer-postgresql-to-mysql"/>
            <updated>2018-03-04T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/how-i-approached-software-development-and-why-i-prefer-postgresql-to-mysql</id>                <summary>This post is basically a PostgreSQL vs MySQL comparison.  Working with MySQL has always been a pain for me, on the other side the experience with PostgreSQL was always  just great! That is why I prefer PostgreSQL.
</summary>                <content type="html"><![CDATA[<p><em>This post is more a rant against MySQL (InnoDB) rather than a technical tutorial (as I was used to write in the previous months).
Everything started with <a href="https://twitter.com/goetas_asmir/status/966743735194222592">this</a> tweet. 
I'm not going to talk that much about exotic features in a multi server configuration in my comparison,
I will try to focus on inconsistencies that might affect relatively simple every-day-applications.</em></p>

<p>If you are interested just in the MySQL wideness I've found, just jump to the next paragraph. 
On the other side if you want to hear a nostalgic story to get a bit more of a context continue reading.</p>

<p>When I approached web development was the 2001 (I was 18). 
In reality I even did not know really well what "software development" means or where I will end up.
I learned to code with PHP and to use MySQL, just to build my personal website.</p>

<p>Later I got hired to work in a local web agency. They were using PostgreSQL across the majority of their products.
At that time, for me there were no differences between the two databases, 
the only difference I was noticing was that I had to use
<code>pg_connect</code> instead of <code>mysql_connect</code> and the admin panel was "phppgadmin" instead of "phpmyadmin".</p>

<p>Fast forward, to nowadays. I've learned a lot and there is still a lot to learn. 
I fall in love with PostgreSQL and for its consistency, but really often I have to work on projects that use MySQL and 
the experience is not that nice.</p>

<p>I've tried to analyze the "<strong>why people start projects by using mysql instead of postgres?</strong>"
My conclusion is just one. Back in the time when I started development (~2001) PostgreSQL was not available on Windows
(Mac and Linux based SO were not yet that good). MySQL was easy to install and run.
<strong>So generations of developers learned to use MySQL, not as "a" database, but as "the" database</strong>. 
In 2005 postgres 8.0 was running natively on windows, but only 2-3 years later become good enough.</p>

<p>In the last two decades I've built and/or maintained a variety of applications, working intensively with both databases.
Each time I had to deal with Postgres was always "ordinary work", I was able to focus on the application logic I was 
developing. On the other hand, most of the time I had to work with MySQL 
I had always to pay attention to quirks and edge-cases.</p>

<h2 id="mysql...">MySQL...</h2>

<p>MySQL is a great database that contributed to the success of many other projects (included PHP). With a promise of being 
SQL-compliant and ACID-compliant.
But as many of the software out there, what is MySQL today is the product of evolution. 
Unfortunately <em>MySQL developers did not manage to keep a good balance between stability, consistency and new features</em>.</p>

<p>Many of the parameter configurations available in MySQL 5.7 (the most popular installed version) are about "disabling" 
some of the weird features that MySQL offers.
The even more weird thing that most of the time, the default setting prefers the less-standard value, 
creating more damage than benefit when they were introduced in first.</p>

<p>Examples:</p>

<ul>
<li><a href="https://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_sql-mode">sql-mode</a> is a mixed mess of 
options aimed to disable/enable some of the weird features were added in previous versions.

<ul>
<li><code>ALLOW_INVALID_DATES</code>: (enabled by default) MySQL allows you to save a date as "1900-00-00". 
Why? Which days is "1900-00-00" </li>
<li><code>NO_ZERO_DATE</code>: similarly as in the previous case, in the past (enabled by default)
MySQL allows you to save an invalid date as "0000-00-00".</li>
<li><code>explicit_defaults_for_timestamp</code>: to make things more confusing, in MySQL 8.0 there is this new parameter that is 
somewhere in between of the previous two.</li>
<li><code>ONLY_FULL_GROUP_BY</code>: <a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by">this</a> 
option is still used nowadays to be able to retrieve some extra columns when doing a "group by" query; 
The rows retrieved are not predictable and in more of the case depend on how data are internally stored in MySQL.</li>
</ul></li>
<li><code>ENUM</code> is one of the basic data types in MySQL, but it has many trade offs:

<ul>
<li>A column with <code>ENUM('Mercury', 'Venus', 'Earth')</code> allows you to insert records as <code>NULL</code>, <code>"Mercury"</code>, <code>"Venus"</code>, 
<code>"Earth"</code> and <code>""</code> (empty string)... Why <code>""</code>... ? </li>
</ul></li>
<li><p>(this is one of my favorites) Adding a column in a table might (and will in highly concurrent environments) 
create duplicate rows and the failure of the column addition.
This weird behaviour is explained <a href="https://stackoverflow.com/questions/22580529/mysql-duplicate-entry-error-when-trying-to-add-new-column/27067862">here</a>, 
the MySQL documentation explains the technical reason of this <a href="https://dev.mysql.com/doc/refman/5.6/en/innodb-create-index-limitations.html">here</a>.<br></p>

<blockquote>
  <p><em>When running an online DDL operation, the thread that runs the ALTER TABLE statement applies an “online log” of
  DML operations that were run concurrently on the same table from other connection threads. 
  When the DML operations are applied, it is possible to encounter a duplicate key entry error 
  (<code>ERROR 1062 (23000): Duplicate entry</code>), even if the duplicate entry is only temporary and would be reverted by a 
  later entry in the “online log”.</em> <br></p>
</blockquote>

<p>Documenting the bug allows MySQL to not fix it! Nice!</p></li>
<li>MySQL will lock the tables for most of the schema operation as adding columns, removing columns and so on, 
leading to the inability to write data to the table and in some cases also to read data from it.
This is a real pain point if you have tables with a lot of data.</li>
<li>When making schema changes (adding/removing tables, columns, indexes and so on) MySQL will automatically commit all 
the changes... making transactions useless...</li>
<li>MySQL allows only one trigger per type/table... making that trigger generally more complex and hard to maintain.</li>
</ul>

<p>Another fun detail, this year MySQL started to roll-out a new major release, MySQL 8.0. Wait... where is the 6.0 and 7.0?</p>

<h3 id="good-examples">Good examples</h3>

<p>Out there, there are some examples of great ways of using MySQL.</p>

<ul>
<li><p><a href="https://eng.uber.com/schemaless-part-one/">Designing Schemaless, Uber Engineering’s Scalable Datastore Using MySQL</a>: 
In this article Uber explains how they use MySQL to store their trip data.<br>
They are using MySQL basically as key-value storage (with some nice extras) to store a huge amount of data 
in an eventually-consistent system. It is not SQL (many operations are just not possible), 
it is just an application-level managed storage. Scales well and works well for the Uber use case. <br>
To be honest, I'm not that smart to understand why want they did could not be implemented 
with any other database (postgres included), since all the storage logic is implemented by the application.</p></li>
<li><p><a href="https://code.facebook.com/posts/190251048047090/myrocks-a-space-and-write-optimized-mysql-database/">MyRocks: A space- and write-optimized MySQL database</a>
In this article, the facebook team explains how they have optimized MySQL to make a write-efficient database.<br>
MyRocks looks to be just a different database built on top of the MySQL storage-engine API. 
It has weak points, limitations and advantages but is far from being MySQL with the InnoDB storage.</p></li>
</ul>

<p>This are just two example of some great software build on top of MySQL. But this cases (and many other) have one point in 
common. They are not really using MySQL, they are using a subset of it and building a brand new application top of it.</p>

<h3 id="mysql-master-master-replication">MySQL master-master replication</h3>

<p>MySQL can be configured to perform master-to-master replication. This is one of the main selling point of MySQL.
Most people I've met, do not know what they are buying.</p>

<p>Master-Master replication is tricky, has many limitations and you lose many of the traditional ACID-compliant 
relational databases features.
Auto-increments are tricky to manage because of possible write conflicts; 
Unique-keys can not be easily supported and can lead to conflict situations having on different masters duplicate rows;
<a href="https://www.quora.com/When-is-a-MySQL-master-master-synchronisation-worth-it">This</a> is an answer to the mysql 
master-master question i really like.</p>

<h2 id="on-the-other-hand...-postgresql%21">On the other hand... PostgreSQL!</h2>

<p>PostgreSQL, initially developed by the Berkeley University (California) started with a much more formal approach 
to databases than MySQL.
The development process of PostgreSQL is relatively strict, but on the other side PostgreSQL has a very low bug-rate 
and the features added are always well thought and consistent.</p>

<p>Personally do not remember cases of weird features introduced just for "marketing".</p>

<p>The list of features that PostgreSQL offers is endless. Will try just to point out some of the most common
 (and that I miss when I have to work with MySQL).</p>

<ul>
<li><strong>SQL-standards</strong>: Reading the <a href="https://www.postgresql.org/docs/9.6/static/features.html">documentation</a> postgres claims to 
follow at least 160 out of 179 SQL:2011 mandatory features required for full Core conformance. On the other hand 
MySQL explicitly <a href="https://dev.mysql.com/doc/refman/5.7/en/compatibility.html">says</a> that they are ready to sacrifice
standards for speed and reliability (instead of pursuing a way to achieve both goals as postgres does most of the time).</li>
<li><strong>Functional Indexes</strong>: In Postgres you can use functions (build in or written by you via stored procedures) and build
indexes over them. This is just a killer feature most of the time! You can putt all the complex logic in a custom function, 
index it and your searches will be super fast!</li>
<li><strong>Window-Functions</strong>: If you have to deal with reporting, this will save your life! Otherwise just do not bother about it.
Window functions perform a calculations across a set of table rows that are somehow related to the current row.
More info about it <a href="https://www.postgresql.org/docs/10/static/tutorial-window.html">here</a>.</li>
<li><strong>GIN and GIST indexes</strong>: This are special kind of indexes that store on the leafs more than one row-reference. 
They are a killer feature if you have to store (and search) arrays, geo-data, full-text and similar complex data-sets.
Moore info about GIN/GIST indexes is available <a href="https://www.cybertec-postgresql.com/en/gin-just-an-index-type/">here</a>.</li>
<li><strong>EXPLAIN</strong>: The postgres EXPLAIN query feature is just great! You should try it to see the difference.
The postgres EXPLAIN query contains detailed index usage, scan and join strategies, timing, buffers and memory usage
and much much more.   </li>
<li><strong>Row level constraints</strong>: PostgreSQL in addition to foreign keys and unique indexes offers 
<a href="https://www.postgresql.org/docs/9.4/static/ddl-constraints.html">CHECK constraints</a>. 
Essentially is a check executed on each update/insert of a row and ensures the consistency of the data. As example<br />
we can create a check as <code>CHECK (price &gt; 0 AND discount &lt;=100 OR special IS NOT NULL)</code> 
to check that the "price" column is always greater than zero and the discount is less than 100 only if "special" 
is not null. The expression inside CHECK can be any valid postgres expression, including checking geo coordinates checks, 
special indexes and so on.</li>
<li><strong>Range types</strong>: Postgres indexable data types to represent datetime and numeric ranges.      </li>
<li><strong>The merge-join strategy</strong>: is a special join strategy that is super fast when both tables have the same sorting.</li>
<li><strong>Locking</strong>: Postgres has many features to avoid locking as example the 
<a href="https://www.postgresql.org/docs/10/static/sql-createindex.html">CREATE INDEX CONCURRENTLY</a>
that allows you to add indexes without locking.</li>
<li><strong>Adding/removing columns is atomic</strong>: adding columns (with default NULL) and removing them is super-fast as it alters
only the table metadata (no need to copy the table onto a new one). 
MySQL in most of the cases has to re-write the table (that might be very expensive if you have millions of rows).
When using postgres at-scale in a highly concurrent environment this makes the difference.</li>
<li><strong>Full text</strong>: MySQL has full text support, but compared to postgres-full-text looks like a toy. PostgreSQL allows you to 
configure the language of the text, fine-tune the stemming rules for each language, weighted ranking and so on. </li>
<li><strong>Spatial data</strong>: if you have to work with spatial data (geo data or geometric types/shapes) postgres is just excellent.
By default postgres has specific data types to store and query information about "shapes" as squares, triangles, circles 
spheres cubes and so on. For more advanced use-cases, 
PostGIS is an extension-set that makes geo-data handling a pleasure (used by OpenStreetMap as example).
MySQL introduced a similar feature-set in 5.7 but is much simpler and limited, with postgres you have more operation 
types built in specialized indexes for read-optimized or write-optimized operations on shapes and much more. </li>
</ul>

<p>The list of PostgreSQL features I like (and miss when working with MySQL) is endless. 
What makes postgres so attractive
to me is the extension mechanism it has that allows to add extra features to postgres really really easily.
Postgres has already <a href="https://www.postgresql.org/docs/10/static/contrib.html">dozens</a> of extensions ready to be used 
and that will make your life easier. Some extensions I use often are 
bloom (super efficient index types for probabilistic index matching), 
cube (multi dimensional indexable data type for 3D or more dimensions searches), 
earthdistance (calculate easily geo distances), hstore (simple, efficient and ACID key-value storage).</p>

<h3 id="postgresql-replication">PostgreSQL replication</h3>

<p>PostgreSQL offers master-slave replication. Works similarly to MySQL. Obviously has some differences but the big picture is similar. 
Offers classical master-slave replication mode with also hot-standby master mode (essential a "waiting" master ready to 
take over in case of the main master failure).</p>

<p>Out of the box PostgreSQL does not offer master-master replication. This could sound a big disadvantage. <br>
In my experience, with PostgreSQL takes much more time (and load) to reach a state where more than one master is necessary
for performance reasons. 
(For high availability, the hot-standby master-mode is already part of the postgres core.)</p>

<p>In reality there are third-party solutions for master-master replications as
 <a href="https://www.2ndquadrant.com/en/resources/bdr/">Postgres-BDR </a> that do an excellent job with multi-master solutions!</p>

<h2 id="conclusion">Conclusion</h2>

<p>If there is a rule I've learned in life is that things are never black or white, most of the time are gray.</p>

<p>So probably there are use cases where MySQL does a much better job than PostgreSQL, I just have not met them yet.
Would be glad to hear experiences from both sides as in my opinion there is always something to learn.</p>

<p>Would be glad to hear experiences where in your opinion MySQL did really a great job.</p>

<p>Hope you enjoyed this article and the others. 
If you have any type of feedback, do not hesitate to leave a comment!</p>

<p>Edit, 2018-03-18: I've updated this document based on the feedback received in the past weeks. 
Thanks to everybody.</p>]]></content>
        </entry>        <entry>
            <title>Modular Application Architecture - Static assets</title>
            <link href="https://www.goetas.com/blog/modular-application-architecture-static-assets"/>
            <updated>2018-01-17T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/modular-application-architecture-static-assets</id>                <summary>When developing software, sometimes we need to allow our application to have plugins or modules  developed by third parties.
Static files as images, fonts or in general &quot;files&quot; are often part of a plugin. In this article we will se how to  deal to them and how to make them available to the core application. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the sixth post from a series of posts that will describe strategies to build 
modular and extensible applications. In this post we will see how to deal with static files (images, fonts...)
and how to make them available to the core application.</strong></p>

<p>In the previous posts we discussed how to build a plugin architecture, hot to register, configure and run plugins.
Plugins are composed of many files, each file has its own propose. The source code (or a compiled version of it in 
case you are dealing with complied languages) is the plugin-itself, but what about "static files" as images, css, 
or any other file that we want to make available to the application core or to its user?</p>

<p>We need a way to publish those "assets" (the static files...) to the application and later the application can make them 
available to the user.</p>

<p>As it should be clear, the "application" is the coordinator and should define how a plugin should publish the assets.</p>

<p>I've seen applications that do not define rules on how to publish assets, and sadly often the result is that the plugin
authors start using different strategies leading to a chaotic plugin system where no one knows which is the best way to 
expose static files.</p>

<h2 id="publishing-assets">Publishing assets</h2>

<p>There are many strategies on how to publish static assets, each of them with advantages and pitfalls.
Some of them require coordination from the main application, some of them require coordination from web server or 
some of them require manual steps to be performed by system administrators and so on...</p>

<p>Lets suppose for our application a directory structure as follows and the <code>application/public</code> folder to be
directly accessible by the webserver:</p>

<pre><code>application/
├── src/ # application code
├── plugins/ 
├── public/ # directory accessible to the web-server
│   ├── index.php
│   ├── app.js
│   └── style.css
</code></pre>

<p>Obviously this is just an example, the <code>plugins</code> can be located anywhere we want.</p>

<h3 id="1.-install-plugins-in-public-folder">1. Install plugins in public folder</h3>

<p>The simples strategy to expose public assets from plugins, is just to put the whole plugins folder into the public directory.</p>

<pre><code>application/
├── src/ # application code
└── public/ # directory accessible to the web-server
    ├── plugins/
    │   ├── plugin-1/
    │   │   ├── src/
    │   │   ├── public/
    │   │   └────── kitten.jpg
    │   └── plugin-n/
    ├── index.php
    ├── app.js
    └── style.css
</code></pre>

<p>Advantages:</p>

<ul>
<li>simple</li>
<li>does not require special server configurations</li>
<li>does not require "application" configurations to expose public assets</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>Very Insecure! (your source code is exposed) </li>
<li>no easy way to control permissions</li>
</ul>

<p>This strategy as example is used by Wordpress.
**This solutions is very insecure and not really suggested, ** it is here just for informational proposes.</p>

<h3 id="2.-copy-public-assets">2. Copy public assets</h3>

<p>To solve the security issues from the first example, we can copy just the static files to the public folder instead
of placing it there the whole plugin.</p>

<p>The application defines one or more folders in the plugin directory structure to be copied to the main public folder.
As example, if yor plugin has a folder named <code>public</code>, its content will be copied to the 
<code>application/public/[plugin-name]</code> folder and made available to users.</p>

<p>If the plugin system has an installation step, the copy can be created automatically when "installing" it, otherwise
has to be done manually or in a "deploy" step.</p>

<pre><code>application/
├── src/ # application code
├── plugins/ # plugins code
│   ├── plugin-1/
│   │   ├── src/
│   │   ├── public/
│   │   └────── kitten.jpg
│   └── plugin-n/ 
└── public/ # directory accessible to the web-server
    ├── plugins/
    │   ├── plugin-1
    │   ├────── kitten.jpg
    │   └── plugin-n*
    ├── index.php
    ├── app.js
    └── style.css
</code></pre>

<p>Advantages:</p>

<ul>
<li>simple</li>
<li>does not require special server configurations</li>
</ul>

<p>Disadvantages:
- the application should have a way to copy public assets from a plugin (an install step), otherwise should be done manually
- requires a "deploy" or an "install" step in which assets are copied to the public folder
- necessary to re-sync the public folder when updating the plugin files
- not convenient while developing plugins
- no easy way to control permissions</p>

<h3 id="3.-symlinking">3. Symlinking</h3>

<p>To solve some of the disadvantages of the "copy" strategy, instead of copying the public folder,<br />
we can use a "symlink" to link its content to the main public folder.</p>

<p>As example, if yor plugin has a folder named <code>public</code>, its content will be symlinked to the 
<code>application/public/[plugin-name]</code> folder and made available to users.</p>

<p>If the plugin system has an installation step, the link can be created automatically when "installing" it, otherwise
has to be done manually or in a "deploy" step.</p>

<pre><code>application/
├── src/ # application code
├── plugins/
│   ├── plugin-1/
│   │   ├── src/
│   │   ├── public/
│   │   └────── kitten.jpg
│   └── plugin-n/ 
└── public/ # directory accessible to the web-server
    ├── plugins/
    │   ├── plugin-1* # points to application/plugins/public
    │   └── plugin-n*
    ├── index.php
    ├── app.js
    └── style.css
</code></pre>

<p>Advantages:</p>

<ul>
<li>simple</li>
<li>does not require special server configurations</li>
<li>changes in the plugin files are automatically available</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>the application should have a way to expose public assets from a plugin (an install step), otherwise should be done manually</li>
<li>no easy way to control permissions</li>
<li>still requires a "deploy" or an "install" step in which assets are symlinked to the public folder</li>
<li>(some systems do not support symlinks (really few...))</li>
</ul>

<h3 id="4.-server-configuration">4. Server configuration</h3>

<p>Some applications that are shipped together with a web server, can modify directly the server configurations and expose 
the files through it; in alternative the web server can be configured manually to expose specific folders/files.</p>

<p>In general, we can resume advantages and disadvantages of exposing plugin assets via server configuration as:</p>

<p>Advantages:</p>

<ul>
<li>high level of control</li>
<li>changes in the plugin files are automatically available</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>requires server configuration (manual or via more complex automation)</li>
<li>depends on the server that you are using</li>
<li>no easy way to control permissions </li>
</ul>

<p>Thanks to the containerization technologies, 
modern applications tend to ship together with the application also part of the infrastructure as webserver, db and so on.
This mean that part of the plugin files we can have also infrastructure configurations as web server configs.
This makes the whole plugin much more complex and often is not worth.</p>

<p>We can go more in detail and see which strategies to expose plugin assets via server configuration we can use.
Different strategies have different trade-offs and are able to solve some of the disadvantages said before.</p>

<p>Lets consider some examples using <a href="https://nginx.org/">nginx</a> and suppose to have a directory structure as:</p>

<pre><code>application/
├── src/ # application code
├── plugins/ # pligins code
│   ├── plugin-1/
│   │   ├── src/
│   │   ├── public/
│   │   └────── kitten.jpg
│   └── plugin-n/ 
└── public/ # directory accessible to the web-server
    ├── index.php
    ├── app.js
    └── style.css
</code></pre>

<p><strong>Explicit</strong></p>

<p>We can explicitly define a list of directories and their exact location on the filesystem and make them exposed by the 
web server. This approach requires an human intervention as the mapping are manually defined (or complex automation).</p>

<pre><code class="nginx">server {
    ...

    location /plugins/plugin-1 {
       root /application/plugins/plugin-1/public
    }
    ...
    location /plugins/plugin-n {
       root /application/plugins/plugin-n/public
    }    
}
</code></pre>

<p>In addition to the general advantages/disadvantages of the configuration based approach we, for this specific strategy 
we have:</p>

<p>Advantages:</p>

<ul>
<li>high level of control</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>requires server configuration, mostly manual or via complex automation</li>
<li>each new plugin require editing the configs</li>
</ul>

<p><strong>Pattern matching</strong></p>

<p>Instead of mapping manually the paths we ca use some pattern matching. If yor plugin has a folder named <code>public</code>, 
its content can be mapped to the <code>/plugins/[plugin-name]</code> location and made available to users.</p>

<pre><code class="nginx">server {
    ...

    location ~ ^/plugins/(?&lt;pluginName&gt;.+) {
       root /application/plugins/$pluginName/public
    } 
}
</code></pre>

<p>In addition to the general advantages/disadvantages of the configuration based approach we, for this specific strategy 
we have:</p>

<p>Advantages:</p>

<ul>
<li>can be automated and the manual intervention can be avoided</li>
<li>does not require a new configuration when adding a ned plugin</li>
<li>no complex automation or human intervention are needed</li>
</ul>

<h3 id="5.-serving-static-files-through-the-application">5. Serving static files through the application</h3>

<p>Most of the strategies said before expose in one way or another the files directly to the webserver.
The approach tend to be very resource-efficient but the drawbacks of the approach are
that the application is not in control the file anymore.</p>

<p>As example to control access permissions to static files part of a plugin we have to serve them 
through the application.
Doing so, each time a file is requested, the application will be invoked and will act as intermediary. 
The application can do processing (decide if we are allowed to see the file as example), 
and eventually serve the file to the user.</p>

<p>This approach is extremely flexible but the biggest drawback are the performances as 
each request will spawn our application too.</p>

<p><strong>Via application only</strong></p>

<p>The application can offer some features and replace the role of a webserver, serving static files by reading them 
instead of the webserver.</p>

<pre><code class="php">&lt;?php
// suppose this piece if code is executed when requesting /plugins/plugin-1/my_protected_file.txt

// here we can do here any type of security check and store it into $hasPermissions

if ($hasPermissions) {
    header("Content-Type: text/plain");
    $fp = fopen("/application/plugins/plugin-1/public/my_protected_file.txt", "r");
    fpassthru($fp);
    fclose($fp);
} else {
    header('HTTP/1.0 401 Unauthorized');
    // or header("HTTP/1.0 404 Not Found");
}
</code></pre>

<p>The main disadvantage of this approach is performance wise. 
Serving a static file through a scripting/programming language is an overkill, especially if the file is big.</p>

<p>The application should re-implement all the "file" handling capabilities as partial downloads, mime type inspections, 
download resume, caching... that are part of most webserver core functionalities.</p>

<p>To sum up:</p>

<p>Advantages:</p>

<ul>
<li>high level of control</li>
<li>files can be stored anywhere (when the application know how to access them, it is enough!)</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>low performance</li>
<li>requires re-implementation of some HTTP basics at application level</li>
</ul>

<p><strong>Via application but assisted by the webserver</strong></p>

<p>To solve the performance issues of the previous strategy, but keeping still the ability to control via application who
can download the files, 
we can use the <code>X-Sendfile</code> feature offered by <a href="https://tn123.org/mod_xsendfile/">Apache</a> 
 or the analog <code>X-Accel</code> feature offered by <a href="https://www.nginx.com/resources/wiki/start/topics/examples/xsendfile/">Nginx</a>.</p>

<p>The previous example will become:</p>

<pre><code class="php">&lt;?php
// suppose this piece if code is executed when requesting /plugins/plugin-1/my_protected_file.txt

// here we can do here any type of security check and store it into $hasPermissions

if ($hasPermissions) {
    header("Content-Type: text/plain");
    header("X-Sendfile: /application/plugins/plugin-1/public/my_protected_file.txt");
    // or header("X-Accel-Redirect: /application/plugins/plugin-1/public/my_protected_file.txt"); for nginx 
} else {
    header('HTTP/1.0 401 Unauthorized');
    // or header("HTTP/1.0 404 Not Found");
}
</code></pre>

<p>This approach combines the ability of giving full control to the application, but delegating to the web server the 
responsibility of how to read an send the file.</p>

<p>The application decides if the file can be downloaded, and then sends the <code>X-Sendfile</code> containing the path of the file.
The webserver will serve the file at the specified location (applying the common HTTP capabilities and optimizations).</p>

<p>To sum up:</p>

<p>Advantages:</p>

<ul>
<li>high level of control</li>
<li>lower performance impact</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>files must be accessible by the web-server </li>
<li>creates some dependency on the webserver</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>In this article we saw some strategies on how to serve static files part of a plugin, 
each discussed strategy has specific trade-offs and can be useful in some cases.</p>

<p>This is the last article for the series of "Modular Application Architecture" on how to build plugin-based architectures.</p>

<p>Hope you enjoyed this article and the others. 
If you have some feedback, do not hesitate to leave a comment!</p>]]></content>
        </entry>        <entry>
            <title>Modular Application Architecture - Considerations</title>
            <link href="https://www.goetas.com/blog/modular-application-architecture-considerations"/>
            <updated>2018-01-03T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/modular-application-architecture-considerations</id>                <summary>When developing software, sometimes we need to allow our application to have plug-ins or modules  developed by third parties. 
In this post we will take a general overview on how some popular design patterns  can allow us to create plugin based applications and some considerations to keep in mind when implementing them.
</summary>                <content type="html"><![CDATA[<p><strong>This is the fifth post from a series of posts that will describe strategies to build 
modular and extensible applications. In this post we will take a general overview on how some popular design patterns
and things to keep in mind when creating plugin based applications.</strong></p>

<p>In the previous posts we saw 
the "Event Manager" (in reality a variation of the <a href="https://en.wikipedia.org/wiki/Mediator_pattern" title="" target="_blank">Mediator</a> pattern), 
the "Pipeline" (in reality a variation of <a href="https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern" title="" target="_blank">Chain of Responsibility</a> pattern)
and the use of inheritance to build plugin systems.
Many of the plugin systems are just customizations of popular <a href="https://en.wikipedia.org/wiki/Software_design_pattern" title="" target="_blank">design patterns</a>.</p>

<p>Anthony Ferrara (alias ircmaxell), 
in <a href="https://blog.ircmaxell.com/2012/03/handling-plugins-in-php.html" title="" target="_blank">this post</a> blogged 
about the use of software patterns to implement plugin-based architectures. 
It is a great article and I suggest everybody to read it.
As it is clear from the article, each of this software patterns has a specific use case 
and the choice of which one to use  depends on which the of integration we want allow for the future plugins.</p>

<h2 id="recap">Recap</h2>

<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Observer_pattern" title="" target="_blank">Observer</a> and <a href="https://en.wikipedia.org/wiki/Mediator_pattern" title="" target="_blank">Mediator</a> pattern
allows you to have communication between independent objects.<br>
Is one of the most popular patterns when building plugin-based systems that allows to <strong>add</strong> or <strong>change</strong> functionalities. 
The application has still full control on what plugins are able to do.</li>
<li><a href="https://en.wikipedia.org/wiki/Strategy_pattern" title="" target="_blank">Strategy</a>, <a href="https://en.wikipedia.org/wiki/Decorator_pattern" title="" target="_blank">Decorator</a>
and <a href="https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern" title="" target="_blank">Chain of Responsibility</a> patterns
are convenient when one of the plugin should be allowed to <strong>change</strong> or the behaviour of one functionality
or to <strong>implement</strong> missing functionalities.</li>
<li><a href="https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)" title="" target="_blank">Inheritance</a> combined with 
the <a href="https://en.wikipedia.org/wiki/Template_method_pattern" title="" target="_blank">Template method</a> pattern is useful when 
we want to grant to a plugin <strong>full control</strong> over the application. 
The plugin replaces the application code with its code. </li>
</ul>

<h2 id="use-cases">Use cases</h2>

<p>All of this patterns are extremely powerful and have they specific use cases. Most of them can be also implemented 
in a not-object-oriented way. As example:</p>

<ul>
<li>The Wordpress theme system can be seen as a "Inheritance+Template method" based system</li>
<li>Drupal and Wordpress use the Mediator pattern to implement most of their plugin systems (most of it was procedural code)</li>
</ul>

<h2 id="tips">Tips</h2>

<p>1) One one fundamental point to make a 
plugin system effective, is the way how the application allows to 
<a href="/blog/modular-application-architecture-intro/#2.-registration">register</a> and 
<a href="/blog/modular-application-architecture-intro/#3.-configuration">configure</a> plugins. <strong>Most of the power of a
good plugin system comes from a good registration and configuration mechanism.</strong></p>

<p>The more a plugin is able to "hook" into the core of the application the more functionalities will be able to provide.
On the other hand, the way that the plugin uses to "hook" into the core should not be "hacky", should be simple and 
should be the same used by the application core to offer its core functionalities.</p>

<p>A good example in my opinion can be the Symfony's <a href="http://symfony.com/doc/current/bundles.html" title="" target="_blank">bundle</a> system. 
It uses <a href="https://symfony.com/doc/current/bundles/extension.html" title="" target="_blank">extension points</a>
and <a href="http://symfony.com/doc/current/service_container/tags.html" title="" target="_blank">service tagging</a>
to allow to plugins (bundles) to register their functionalities in the same way used by symfony itself.</p>

<p>2) Another tip is to <strong>avoid imposing constraints when not necessary</strong>.<br>Can be tempting to offer at application level
some "standardized" control panel or storage to configure the plugins... please don't. 
Sooner or later there will be a case that will be not supported by the "standardized" format you decided.
Most likely you will end up in <a href="https://xkcd.com/927/" title="" target="_blank">this</a> scenario
and you will have to edit the "standardized" format you decided. 
The application core should to only the minimum necessary.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article we saw some use  cases for common software design patterns and some other 
considerations to keep in mind  when building plugin based systems.<br> 
A part that is missing from this series of blog posts (and will be covered in the next post) is 
<strong>how to deal with static assets</strong> (as images, css, files).</p>

<p>Hope you enjoyed this article and if you have some feedback, do not hesitate to leave a comment.</p>]]></content>
        </entry>        <entry>
            <title>Modular Application Architecture - Inheritance</title>
            <link href="https://www.goetas.com/blog/modular-application-architecture-inheritance"/>
            <updated>2017-12-13T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/modular-application-architecture-inheritance</id>                <summary>When developing software, sometimes we need to allow our application to have plug-ins or modules  developed by third parties. 
In this post we will see how &quot;inheritance&quot; can  help us to build a plugin system for our application. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the fourth post from a series of posts that will describe strategies to build 
modular and extensible applications. In this post we will start looking how to use "inheritance" 
to create a plugin based application.</strong></p>

<p><a href="https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)">Inheritance</a> is a characteristic offered
by many class/object oriented languages and allows you to "inherit" some of the functionalities from a "parent" object 
or class. The object that inherits those functionalities can eventually alter some of them but for the rest behave 
as the inherited class.</p>

<p>Being able to change the behaviour of the inherited class but "look" as the original 
comes useful when building plugin systems that allows to change existing functionalities of the core.
<br>
Combined with a good plugin registration system (configurable or based on discovery) is possible to change almost every 
aspect of an application without touching the application core code.</p>

<div class="text-center">

<p><img src="/img/posts/plugin-based-architecture/inheritance.png" alt="application and components" /></p>

</div>

<p>In the picture here above there is a "base page" that has some blocks defined (header, side, content and footer) and
there is a "new page" that inherits earthing from the parent page and "overwrites" the content page.</p>

<p>Possible use cases for this strategy are "themes" for the layout of a website. 
By <a href="https://en.wikipedia.org/wiki/Method_overriding">overriding</a> the right portion of the original class is possible to 
change, add, remove functionalities and/or look-and-feel.</p>

<p>Most of the frameworks from the past 10 years were built on the idea of "extending" some parts of them 
and replacing/implementing some other implementations. Many developers quickly started to 
<a href="https://en.wikipedia.org/wiki/Method_overriding">override</a> any part of the framework (just because the programming 
language was allowing them to do it). But the ability of being able to change everything comes with a price.<br />
Being able to alter fully the behaviour of the application often was resulting that the "core" application could be broken 
really easily by some "bad" plugin.</p>

<h2 id="basic-example">Basic example</h2>

<p>Implementing an "inheritance" based plugin architecture requires the following "things" to be in place:</p>

<ol>
<li>A "base" class that offers some functionalities.</li>
<li>a "configurable" way to get an instance of that class (
using just its name, a factory class or using dependency injection).</li>
<li>A way to "run" that class.  </li>
<li>A way to change the class to run.</li>
</ol>

<p>Let's make an example by looking at a simple example where plugins can change the homepage of a website:</p>

<p>This could be our "base" class, the default homepage.</p>

<pre><code class="php">&lt;?php
class HomePage
{
    public function display()
    {
        return 'This is my homepage!';    
    }  
}
</code></pre>

<p>This could be our "application core".</p>

<pre><code class="php">&lt;?php
// this is the class able to "run" our base class
class Website
{
    private static $pageName = 'HomePage';

    public static function init($pageName)
    {
        self::$pageName = $pageName;
    }

    public static function run()
    {        
        $pageToRun = self::$pageName;
        $page = new $pageToRun();
        echo $page-&gt;display();
    }
}
// run
Website::run();
</code></pre>

<p><code>Website::run</code> is responsible to "run" our application while the method  <code>Website::init</code> is the method that allows
to register the "page plugin".</p>

<p>Now let's suppose we want to write a modified version of the homepage but adding the nome of the owner of the page.
We can do something as:</p>

<pre><code class="php">&lt;?php

// this is our "base" class
class NewHomePage extends HomePage
{
    public function display()
    {
        $original = parent::display();

        $new = 'My name is tom. '. $original;

        return $new;
    }  
}

// change the "configuration"
Website::init('NewHomePage');
</code></pre>

<p>As you can see from this very simple example, the <code>NewHomePage</code> class has full control over the <code>display</code> method. 
Can decide to alter its result or even to not invoke the <em>parent</em> method at all.
<br>
The parent class (<code>HomePage</code>) can be arbitrary complex and have tens (or hundreds) of methods, is up to the developer to
decide which one to override.</p>

<p>Here the "plugin registration" is done by configuration, but can also be done by "discovery". The method <code>Website::init</code>
could lookup for the class name directly from the database, from a predefined list of directories or any other place.</p>

<p><strong>As it will be discussed more in detail at the end of this article, inheritance is a relatively dangerous feature and 
can quickly go out of control. Having long hierarchies and fat-classes with many many methods used by few child classes, 
 is a perfect recipe for failure.<br />
Inheritance has its use cases too, but the hierarchy should be short and almost all the "parent" methods should be used 
by child classes.</strong></p>

<h2 id="a-more-concrete-example.">A more concrete example.</h2>

<p>The previous example can be too much abstract and that pattern can be used to develop software in general.
A better example could be done using <a href="https://twig.symfony.com/">Twig</a> to implement a configurable/theme-able layout system 
for web pages.
Twig offers an inheritance system very similar except that the "parents" can be decided at runtime.</p>

<p>Let's suppose the following Twig template page (named <code>layout.html.twig</code>) that implements a common website layout 
with "header", "side", "content" and "footer":</p>

<div class="row">
<div class="col-md-6">

<p></p>

<pre><code class="twig">{# layout.html.twig #}
{% block html %}
&lt;html&gt;
    &lt;title&gt;Welcome to my website&lt;/title&gt;
    {% block body %}
        &lt;body&gt;
            {% block header %}
                &lt;div class="header"&gt;
                    My homepage
                &lt;/div&gt;
            {% endblock %}
            {% block content_wrapper %}
                &lt;div class="content-wrapper"&gt;
                    {% block sidebar %}
                        &lt;div class="sidebar"&gt;
                            Menu items here
                        &lt;/div&gt;
                    {% endblock %}

                    {% block content %}
                    {% endblock %}
                &lt;/div&gt;
            {% endblock %}
            {% block footer %}
                &lt;div class="footer"&gt;
                    My copyright
                &lt;/div&gt;
            {% endblock %}
        &lt;/body&gt;
     {% endblock %)
&lt;/html&gt;
{% endblock %)
</code></pre>

<p></p>

</div>

<div class="col-md-6">

<p></p>

<pre><code class="html">&lt;!-- html equivalent --&gt;
&lt;html&gt;
    &lt;title&gt;Welcome to my website&lt;/title&gt;
        &lt;body&gt;
            &lt;div class="header"&gt;
                My homepage
            &lt;/div&gt;
            &lt;div class="content-wrapper"&gt;
                &lt;div class="sidebar"&gt;
                   Menu items here
                &lt;/div&gt;

                &lt;!-- 
                content section here 
                &lt;div class="content"&gt;
                    Lorem ipsum sploram
                &lt;/div&gt;
                --&gt;
            &lt;/div&gt;

            &lt;div class="footer"&gt;
                My copyright
            &lt;/div&gt;
        &lt;/body&gt;
&lt;/html&gt;
</code></pre>

</div>
</div>

<p>This can be the general layout for our website, a more detailed page (the <code>homepage.html.twig</code>) can as example extend the 
template and "implement" the homepage.</p>

<p></p>

<pre><code class="twig">{# homepage.html.twig #}
{% extends layoutName %}   
{% block content %}
    &lt;h1&gt;Welcome to my homepage&lt;/h1&gt;
{% endblock %}
</code></pre>

<p></p>

<p>Here there are two important things to notice:</p>

<ul>
<li>First, the <code>{% block content %}</code> allows us to overwrite the default (empty) 
implementation offered by the <code>layout.html.twig</code> template. 
The resulting template is simple and focused on the homepage (as it should be). <code>homepage.html.twig</code> can override any 
block defined in the parent template, changing virtually any part.</li>
<li>Second and more important, the "parent" template name is not hardcoded in the template. 
Is dynamic and defined at runtime ( by the <code>layoutName</code> variable) and can be changed (by a registered plugin as example). </li>
</ul>

<p>A code necessary to run our page can be:</p>

<pre><code class="php">&lt;?php
// this variable holds the layout template name
$defaultLayout = 'layout.html.twig';

$twig = new Twig_Environment(new Twig_Loader_Filesystem('/path/to/templates'));
$twig-&gt;display('homepage.html.twig', array('layoutName' =&gt; $defaultLayout));
</code></pre>

<p>Let's suppose now we want to allow a plugin to change the layout but keeping the homepage content 
(as example removing the sidebar). 
We need only to allow the plugin to change the <code>$defaultLayout</code> value and provide its own layout implementation.</p>

<p>Our new template can be just:</p>

<p></p>

<pre><code class="twig">{# plugins/pluginName/layout.html.twig #}
{% block content_wrapper %}
    &lt;div class="content-wrapper"&gt;
        &lt;h1&gt;my plugin layout&lt;/h1&gt;        
        {{ block('content') }}
    &lt;/div&gt;
{% endblock %}
</code></pre>

<p></p>

<p><code>plugins/pluginName/layout.html.twig</code> is overriding the <code>content_wrapper</code> block and just calling the <code>content</code>
block (not calling also the <code>sidebar</code> block as in the original layout).</p>

<p>Obviously the plugin needs to be registered (via discovery or configuration) and being able
to change the  <code>$defaultLayout</code> value. As example:</p>

<pre><code class="php">&lt;?php
// should be 'plugins/pluginName/layout.html.twig' as registered by our plugin
$defaultLayout = PluginRegistry::getLayout(); 

// this is identical as before
$twig = new Twig_Environment(new Twig_Loader_Filesystem('/path/to/templates'));
$twig-&gt;display('homepage.html.twig', array('layoutName' =&gt; $defaultLayout));
</code></pre>

<p>This strategy allows to use inheritance to change completely the layout without the supervision of the "application core".</p>

<h2 id="other-strategies">Other strategies</h2>

<p>In my experience I saw other implementations that were also allowing similar things in pure PHP code, by generating at
runtime (or at deploy) "custom" PHP classes, renaming the original one and creating more complex inheritance schemes
that are not available in PHP. Some examples were allowing to extend from two (or more) classes at the same time and
other "esoteric" tricks.</p>

<p>Nowadays PHP <a href="http://php.net/manual/en/language.oop5.traits.php">traits</a> can be used to achieve similar functionalities 
in a much more clean and efficient way.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Personally I'm not a big fan of inheritance. When it comes to PHP applications I try to use as much as possible 
<a href="https://en.wikipedia.org/wiki/Composition_over_inheritance">composition over inheritance</a> 
(of course inheritance has its use cases too). On the other side, the "inheritance" model offered by twig is tailored 
for layouts and works really well.</p>

<p>Advantages:</p>

<ul>
<li>Allows plugins to alter almost any part of the application</li>
<li>Does not require explicit "entrypoints"</li>
<li>For some use cases works really well</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>Allowing to change "anything" increases the risk that a plugin breaks the application</li>
<li>Difficult to document what a plugin can do  </li>
<li>Difficult to control what a plugin can do</li>
<li>Requires deeper knowledge of the "application core" compared to other plugin strategies.</li>
<li>The plugin is tightly coupled with the application structure</li>
<li>Changes to the application may require bigger changes to the plugin compared to other plugin strategies.</li>
</ul>

<p>Inheritance can go out of control very quickly compared to that other strategies, but when dealing with some
particular requirements (as the necessity to allow changes to any part of the application or not limiting 
the extensibility to specific extension points pre-defined by the application core), the inheritance-based 
strategies are a good solution.</p>

<p>Hope you enjoyed this article and if you have some feedback, do not hesitate to leave a comment.  In the next article 
will discuss strategies that are based on conventions and have similar characteristics to what covered in this article.</p>]]></content>
        </entry>        <entry>
            <title>Modular Application Architecture - Pipelines</title>
            <link href="https://www.goetas.com/blog/modular-application-architecture-pipelines"/>
            <updated>2017-11-30T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/modular-application-architecture-pipelines</id>                <summary>When developing software, sometimes we need to allow our application to have plug-ins or modules  developed by third parties. 
In this post we will see in which contexts &quot;pipelines&quot; can be used as plugin mechanism.
</summary>                <content type="html"><![CDATA[<p><strong>This is the third post from a series of posts that will describe strategies to build 
modular and extensible applications. In this post we will start looking on how to implement a plugin-system by using 
"pipelines". Some implementations calls them "middleware".</strong></p>

<p>The approach of pipelines if compared to events is used in a different context. Events are used more often in contexts
where is necessary to  <strong>add</strong> functionalities 
(changing existing functionalities is possible only if the event-type supports it).
Pipelines on the other hand are used when in addition to adding new <strong>data</strong> 
is useful also to allow <strong>altering</strong> existing data.</p>

<p>Generally speaking, with "pipelines" we have simply some data going through a series of transformations.</p>

<div class="text-center">

<p><img src="/img/posts/plugin-based-architecture/pipeline.png" alt="application and components" /></p>

</div>

<p>Some implementations allows you to decide the order of execution, some do not, some allows you also to change the order
at runtime.</p>

<p>As the execution order might be important, the "plugin registration" step is very important 
(see the <a href="/blog/modular-application-architecture-intro">first post</a> for more info about it).</p>

<h3 id="use-case">Use case</h3>

<p>A great example of pipelines are the PSR-7 middlewares 
(<a href="https://philsturgeon.uk/php/2016/05/31/why-care-about-php-middleware/">here</a> a good post about it), 
where each "plugin" can alter an HTTP message by adding as example
HTTP cookies, Credentials, Compression, Caching headers and so on. 
As example, adding a cookie can be considered as <em>adding</em><sup>1</sup> a functionality, 
but as example compressing the HTTP message means most probably replacing the old message with a new and compressed one.</p>

<p>Note <sup>1</sup>: currently PSR-7 is immutable so we are always replacing the old object with a new one.</p>

<p>Text processing can be also a pretty common use case, as example, 
if we want to extract only "important" words from a text. We could have 
steps that performs tokenization, removal of stopwords, stemming and verbs/nouns extraction.</p>

<p>Another use of the pipelines can be image processing. Let's suppose we want to allow a user upload a 
picture that needs to be stored. The image probably should be resized, have applied a watermark and compressed for storage.
This are 3 independent steps and probably are useful in different cases. 
As example: 
- "watermarking" step can be used also for the profile picture, 
- "resizing" can be used probably in many other cases where is necessary to resize an image.</p>

<p>"compression" can be more delicate as it might compress the image as ".zip", ant the other transformations should be 
able to deal with that type of data.</p>

<p>This step puts us in front of one of drawbacks of this plugin system. The "datatype" 
that is going through the pipeline should be known to the transformer in order to allow it to apply some transformation,
but the concept of pipelines is the "altering" of the datatype on each step.</p>

<h2 id="implementations">Implementations</h2>

<p>Different needs have different implementations, this mostly because of the different data-types that we might be 
interested in processing.</p>

<p>In the following examples, each implementation works on a different data-type so it might look different from the other.</p>

<h3 id="the-php-league-pipeline">The PHP League Pipeline</h3>

<p><a href="https://github.com/thephpleague/pipeline">The PHP League Pipeline</a> aims to be a independent library to implement pipelines.</p>

<p>The implementation is more "pure" compared to other, the "data" are always replaced with the new one on each step (immutability). 
Steps can't block the processing or later the execution order.</p>

<p>A basic example can be:</p>

<pre><code class="php">&lt;?php


class MultiplyTwoStage
{
    public function __invoke($payload)
    {
        return $payload * 2;
    }
}

class AddOneStage
{
    public function __invoke($payload)
    {
        return $payload + 1;
    }
}
</code></pre>

<p><strong>Registration</strong></p>

<p>The <code>Pipeline</code> implementation is bare minimum, and the registration is fully manual.
Order execution needs to be explicitly configured.</p>

<pre><code class="php">&lt;?php
use League\Pipeline\Pipeline;

$pipeline = (new Pipeline)
    -&gt;pipe(new MultiplyTwoStage)
    -&gt;pipe(new AddOneStage);


// run the pipeline

$pipeline-&gt;process(10); // Returns 21

</code></pre>

<p>As you can see, the <code>Pipeline</code> makes no assumption on the data and is bare minimum.</p>

<h3 id="laravel">Laravel</h3>

<p>The <a href="https://laravel.com/docs/5.5/middleware">laravel middleware</a> strategy is heavily used as plugin 
system for it.</p>

<p>Most of the request-response cycle in laravel is handled by it. Different "plugins" can add them self to the pipeline 
and perform some operations to the final HTTP message that will be delivered to the user.</p>

<p>As plugins in this case we have anything that might be interested to interact with the HTTP request.</p>

<p>Laravel middlewares are able to: modify the HTTP message, replace it, change the pipeline order or even to stop 
the pipeline processing.</p>

<p>Let's see how it works:</p>

<pre><code class="php">&lt;?php

use Closure;

class CheckIp
{
    public function handle($request, Closure $next)
    {
        if ($request-&gt;getClientIp() === '1.2.3.4') {
            return redirect('/not-allowed');
        }

        return $next($request);
    }
}
</code></pre>

<p>This is one example transformer. 
If the user's IP is in a blacklist, redirects it to a specific page (<code>redirect('/not-allowed')</code>), otherwise
allows the pipeline processing to continue <code>$next($request)</code>.
In this case the <code>CheckIp</code> transformers acts "before" the "next" step. In case of a not-authorized user, 
it stops the pipeline processing.</p>

<p>Another example can be</p>

<pre><code class="php">&lt;?php

use Closure;

class CompressResponse
{
    public function handle($request, Closure $next)
    {
        $response = $next($request);

        $response-&gt;setContent(gzcompress($response-&gt;getContent()));
        $response-&gt;headers-&gt;set("Content-Encoding",  "gzip");

        return $next($request);
    }
}
</code></pre>

<p>In this example we are letting the pipeline continue the flow, and at the end we compress the response.</p>

<p>In this case the <code>CompressResponse</code> transformers acts "after" the "next" step.</p>

<p><strong>Registration</strong></p>

<p>As plugin registration mechanism, laravel uses explicit configuration, by using an array of middlewares. 
It is a simple but powerful enough mechanism to deal with it. The developer can choose which middleware use 
and in which order execute them.</p>

<p>Laravel middleware registration.</p>

<pre><code class="php">&lt;?php

protected $routeMiddleware = [
    'auth' =&gt; \Illuminate\Auth\Middleware\Authenticate::class,
    'auth.basic' =&gt; \Illuminate\Auth\Middleware\AuthenticateWithBasicAuth::class,
    'bindings' =&gt; \Illuminate\Routing\Middleware\SubstituteBindings::class,
    'can' =&gt; \Illuminate\Auth\Middleware\Authorize::class,
    'guest' =&gt; \App\Http\Middleware\RedirectIfAuthenticated::class,
    'throttle' =&gt; \Illuminate\Routing\Middleware\ThrottleRequests::class,
];
</code></pre>

<p>Laravel offers also a way to register middlewares acting ony on a specific url/route, 
but this is already an implementation detail.</p>

<h2 id="gulp">Gulp</h2>

<p><a href="https://gulpjs.com/">Gulp</a> is a javascript build tool.</p>

<p>Is based on the concept that a series of files go through a series of transformations and produce some result 
(most of the times some <code>.js</code> or <code>.css</code> files).
As plugins in this case we have the possible transformers for those files.</p>

<pre><code class="js">var gulp = require('gulp');

var less = require('gulp-less');
var minifyCSS = require('gulp-csso');

gulp.task('css', function () {
  return gulp.src('client/templates/*.less') // get all the *.less files inside the client/templates folder
    .pipe(less()) // process *.less files
    .pipe(minifyCSS()) // compress them
    .pipe(gulp.dest('build/css')) // write the result to the build/css folder
});
</code></pre>

<p>To run this "task" you will have to run in your console something as <code>gulp css</code>.</p>

<p>This example is relatively simple. A task named <code>css</code> has been created.
As "data" we are using the <code>*.less</code> files inside the <code>client/templates</code> folder; the data are going through the <code>less</code> 
transformer (that converts them to CSS), then the data are going to <code>minifyCSS</code> that reduces the size of the CSS; 
in the end data are written to the <code>build/css</code> folder.</p>

<h3 id="more-implementations%3F">More implementations?</h3>

<p>As the data that we might be working with can be really different, 
the list of possible implementations is too long to be discussed here.
Just as example, <a href="https://github.com/pditommaso/awesome-pipeline">this</a> is just a list of pipeline-based tools 
in the sys-admin area. Pretty big, right?</p>

<h2 id="conclusion">Conclusion</h2>

<p>The pipeline approach is useful to <strong>filter/edit data</strong> by using a series of transformers/filters. 
Adding/Editing data can be relatively easy if the data structure we are working on supports it explicitly 
(as example HTTP headers), but when it comes to e adding/editing something in a data structure that does not support it
out of the box (HTTP body) things can become more challenging. 
A key factor for the pipeline to work well is to have a convenient data structure.</p>

<p>Advantages:</p>

<ul>
<li>Transformers can follow strictly the "single responsibility principle"</li>
<li>Simple architecture (it is just data going through transformations)</li>
<li>Transformers can become really re-usable  </li>
<li>Very convenient for small changes</li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>Transformers can break the data</li>
<li>Transformers have to understand well the data</li>
<li>Linear flow (can't re process easily, things happen in a pre-defined sequence... that might not work for some cases)</li>
<li>(If the implementation allows to change the execution order or to stop processing, things can go out of control and 
debugging can be hard)  </li>
</ul>

<p>The "pipeline" way of making plugins is simple and powerful, but has very specific use cases.</p>

<p>Hope you enjoyed this article and if you have some feedback, do not hesitate to leave a comment.</p>]]></content>
        </entry>        <entry>
            <title>Modular Application Architecture - Events</title>
            <link href="https://www.goetas.com/blog/modular-application-architecture-events"/>
            <updated>2017-11-14T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/modular-application-architecture-events</id>                <summary>When developing software, sometimes we need to allow our application to have plug-ins or modules  developed by third parties. 
In this post we will start looking on how to implement a plugin-system by using &quot;events&quot;.
</summary>                <content type="html"><![CDATA[<p><strong>This is the second post from a series of posts that will describe strategies to build 
modular and extensible applications. In this post we will start looking on how to implement a plugin-system by using 
"events".</strong></p>

<p>One of the most common approaches to implement a plugin-based architecture is to use "events".
The idea is that the application core "throws" events, modules listen to those events and react accordingly.</p>

<p>Modules (often called as plugins) can also "throw" events, 
making the whole mechanisms even more powerful since is possible to build 
a sort of communication between modules not coordinated directly by the core.</p>

<p>The event listeners can react to the events and eventually return some data to the core. 
The core application can use the received data to achieve some goal 
(as example displaying them to the user or using them to "throw" other events).</p>

<p>This strategy is used by popular frameworks as <a href="https://symfony.com">Symfony</a> or by <a href="https://wordpress.com/">WordPress</a>.
WordPress calls this approach "hooks", while Symfony and Laravel calls them "events" and they are managed by 
an "event dispatcher".</p>

<div class="text-center">

<p><img src="/img/posts/plugin-based-architecture/events.png" alt="application and components" /></p>

</div>

<h2 id="event-dispatcher">Event Dispatcher</h2>

<p>Here we will analyze in detail how the core application can throw events, how a plugin can listen to them 
and how is possible to use this mechanism to build a plugin system.</p>

<ol>
<li>The application core throws <code>events</code>. To make the system more flexible and performant events have a <code>name</code>, 
so event listeners (plugins) can register on specific events without having to listen to each single event thrown by the core.</li>
<li>The events might have a <code>payload</code>. A payload contains information associated to the event, making it more
meaningful. In this way plugins have more info about the context where the event was thrown as just the name could be 
not enough.</li>
<li>Event listeners can <code>return</code> some data back to who has thrown the event. The returned data should be in a format that 
can be understood by the who has thrown the event. </li>
</ol>

<h3 id="event-features">Event features</h3>

<p>Is possible to add or remove some characteristics from the event system,
this will give us in exchange some features and/or limitations. 
Even the "name", ""payload" and the "return" characteristics can be removed from the event system making it extremely minimal.</p>

<ul>
<li><strong>removing "name"</strong>: by removing the event name we will make our system more complex and less performant as listeners will 
have to listen to all the events and decide by themself to react to the event or not. Removing the event name is an 
uncommon choice and therefore not recommended.</li>
<li><strong>removing "payload"</strong>: by removing the payload we make our event less useful as the only info available  information 
for listeners to understand the context of the event is the name. The event listeners will have to find alternative 
ways to get the context information.</li>
<li><strong>removing "return"</strong>: by removing the return we make our event listeners less able to interact with the core application.
Effects to the system have to be implemented directly by the plugin 
using alternative ways. The core application is not able to "use" directly the effects of a plugin.
Having return values is common, not having them makes easier to introduce asynchronous events.</li>
<li><strong>adding "stop event propagation"</strong>: by giving to event listeners the ability to stop the event propagation
(by returning a special value as example),  it will be possible to influence the "event" effects thrown by the core 
on other plugins.</li>
<li><strong>adding "listener priorities"</strong>: by adding listener priorities we allow plugins to co-operate better, 
especially when the "payload" and "stop event propagation" feature is available. It is possible to decide the order 
of execution of event listeners or to execute only one event listeners even if many are registered on the same event.</li>
<li><strong>adding "wildcard event listeners"</strong>: when necessary to listen on multiple events, on all events or on events where 
the name is only partially known, can be useful to be able to listen on all the events. A common case for this are 
profiling plugins or debug features.</li>
<li><strong>adding "ability to throw event"</strong>: having plugins able to throw events is a useful feature that allows to crete 
"plugins for plugins" making the whole system more flexible. A drawback of too much flexibility is that can go out 
of control really easily.</li>
</ul>

<p><strong>Asynchronous events</strong></p>

<p>If we decide to have only "name" and a "payload" into our event system, it can be easily converted into an asynchronous 
event system. This because each event is independent from the others 
(this can be valid for web applications that have a synchronous flow, 
while with desktop applications or multithreaded/concurrent languages the situation is quite different).</p>

<h3 id="event-listener-registration">Event listener registration</h3>

<p>As said in the first article, a fundamental step is the plugin registration. 
Both, discovery and configuration strategies can be used.</p>

<h2 id="example-implementations">Example implementations</h2>

<p>Implementing and event system is a pretty common task, this means that out there there are already many implementations
we can use instead re-inventing the wheel.</p>

<p>All of them have specific features and is just about choosing the best fit for us.
As is possible to see here below, all of them offers you slightly different features and have slightly different behaviours, 
but in general they are very similar.</p>

<p>We are going to see how is possible to implement a side-bar having the content created dynamically by plugins. 
We will see how to do it using Symfony, Laravel and Wordpress.</p>

<p><strong>Symfony</strong></p>

<pre><code class="php">&lt;?php
use Symfony\Component\EventDispatcher\EventDispatcher;
use Symfony\Component\EventDispatcher\Event;

// the event and payload
class SidebarEvent extends Event
{
    private $placement;
    private $items = [];
    public function __construct($placement)
    {
        $this-&gt;placement = $placement;
    }

    public function getPlacement()
    {
        return $this-&gt;placement;
    }

    public function addItem(Item $item)
    {
         $this-&gt;items[] = $item;
    }

    /**
     * @return Item[] 
     */
    public function getItems() : array
    {
        return $this-&gt;items;
    }
}


// listeners/plugins
$addContactListener = function (SidebarEvent $ev) {
    $ev-&gt;addItem(new Item('Contact on the ' . $ev-&gt;getPlacement()));
};
$addHomeListener = function (SidebarEvent $ev, $eventName, EventDispatcher $dispatcher) {
    $ev-&gt;addItem(new Item('Homepage' . $ev-&gt;getPlacement()));
};

// here is the application
$dispatcher = new EventDispatcher();

// listener registration
$dispatcher-&gt;addListener('site_bar', $addContactListener, -100); // priority
$dispatcher-&gt;addListener('site_bar', $addHomeListener);

// application throwing events
$dispatcher-&gt;dispatch('site_bar', $event = new SidebarEvent('left'));
$items = $event-&gt;getItems();

// example use the return values
echo "&lt;li&gt;";
foreach ($items as $item) {
    echo "&lt;ul&gt;" . $item-&gt;getTitle() . "&lt;/li&gt;";    
}
echo "&lt;/li&gt;";
</code></pre>

<p>What do we have here?</p>

<ul>
<li>the <code>$dispatcher</code> variable is the standard symfony event dispatcher;
the symfony event dispatcher is synchronous and it allows to
listeners to stop the event propagation or to dispatch/throw new events.</li>
<li>we are using a custom <code>SidebarEvent</code> event that has <code>$placement</code> (that says us where is the sidebar) as payload and
allows <code>data</code> as return value; setting the return value will stop the event propagation. </li>
<li>our event listeners are <code>$addContactListener</code> and <code>$addHomeListener</code> as simple callbacks.</li>
<li>the event listeners are registered on the <code>site_bar</code> and <code>admin_site_bar</code> events using the <code>addListener</code> method.</li>
<li>our application throws some events (using the <code>dispatch</code> method);
in this case we are using a specific event class (<code>SidebarEvent</code>)
and as payload we have the page url.</li>
<li><code>$items</code> will contain the values eventually set by an event listener when calling the <code>setItems</code> method; 
<code>$items</code> will be an an array of <code>Item</code> as enforced by the type hinting.</li>
</ul>

<p><strong>Laravel</strong></p>

<pre><code class="php">&lt;?php

use App\Events\SidebarEvent;  

// the event and payload
class SidebarEvent
{
    private $placement;
    public function __construct($placement)
    {
        $this-&gt;placement = $placement;
    }

    public function getPlacement()
    {
        return $this-&gt;placement;
    }
}

// listeners/plugins
$addContactListener = function (array $payload) {
    list($ev) = $payload;

    return 'Contact on the ' . $ev-&gt;getPlacement();
};
$addHomeListener = function ($payload) {
    list($ev) = $payload;

    return 'Homepage on the ' . $ev-&gt;getPlacement();
};

// listener registration
Event::listen('site_bar', $addContactListener);
Event::listen('site_bar', $addHomeListener);

// application throwing events
$items = Event::fire('site_bar', [new SidebarEvent('left')]);

// example use the return values
echo "&lt;li&gt;";
foreach ($items as $item) {
    echo "&lt;ul&gt;" . $item . "&lt;/li&gt;";    
}
echo "&lt;/li&gt;";
</code></pre>

<p>What do we have here?</p>

<ul>
<li>the <code>Event</code> is the default laravel facade;
the laravel event system is also synchronous, has a payload (that by default has to be an array), 
supports broadcasting, wildcard events and return values.</li>
<li>we are using a custom <code>SidebarEvent</code> event that has <code>$placement</code> as payload.   </li>
<li>our event listeners are <code>$addContactListener</code> and <code>$addHomeListener</code> as simple callbacks. returning <code>false</code>
will stop the event propagation.</li>
<li>the event listeners are registered on the <code>site_bar</code> and <code>admin_site_bar</code> events using the <code>Event::listen</code> facade method.</li>
<li>our application throws some events (using the <code>Event::fire</code> facade method); 
in this case we are using a specific event class (<code>SidebarEvent</code>) and as payload we have the page url. </li>
<li><code>$returnPageView</code> will contain an array with all the not false return values from all the events. 
since the values inside <code>$returnPageView</code> can't be checked by type hinting, a sanity check at application level should be done</li>
</ul>

<p><strong>WordPress</strong></p>

<pre><code class="php">&lt;?php

// listeners/plugins
$addContactListener = function ($placement) {
    echo "&lt;ul&gt;Contact on the $placement&lt;/li&gt;";
};
$addHomeListener = function ($placement) {
    echo "&lt;ul&gt;Homepage on the $placement&lt;/li&gt;";
};

// listener registration
add_action('site_bar', $addContactListener); 
add_action('site_bar', $addHomeListener);

// application throwing events

echo "&lt;li&gt;";
$returnPageView = do_action('site_bar', 'left');
echo "&lt;/li&gt;";
</code></pre>

<p>What do we have here?</p>

<ul>
<li>we are using the basic action event system from wordpress; supports priorities, payload and return values</li>
<li>our event listeners are <code>$addContactListener</code> and <code>$addHomeListener</code> as simple callbacks.</li>
<li>the event listeners are registered on the <code>site_bar</code> and <code>admin_site_bar</code> events using the <code>add_action</code> function.</li>
<li>our application throws some events (using the <code>do_action</code> facade method); 
in this case we are using a simple string as payload we have the page url. </li>
<li><code>$returnPageView</code> will contain the return value from the last event listener; 
since the value inside <code>$returnPageView</code> can't be checked, a sanity check at application level should be done</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>The "event" plugin mechanism is commonly used when the <strong>application want to be able to control what plugins are able to do</strong>.
Plugins can interact with the application only in the "extension points" decided by the application.</p>

<p>Advantages:</p>

<ul>
<li>Very flexible.</li>
<li>High degree of control offered to the application: the application decides extension points and data structures use.</li>
<li>Can be asynchronous.</li>
<li>Transparent: the application does not need to be aware of how many plugins are registered and how they are structured.</li>
<li>Documentable: return and payload can be explicitly defined. </li>
</ul>

<p>Disadvantages:</p>

<ul>
<li>Limited to pre-defined extension points: there is not an easy way to interact with parts of the application that does
not offer appropriate extension points.  </li>
<li>Transparent: since the application is not aware of what plugins do and how many they are,
can be difficult to keep application quality; "bad" plugins can't be excluded.</li>
<li>In some dispatcher implementations is difficult to understand if listeners were triggered</li>
<li>Hard to debug and almost useless stack traces in case of errors.</li>
</ul>

<p><strong>Notes</strong>: Depending on the implementation you decide to use, some of the said advantages/disadvantages may change.</p>

<p>Events are one of the most popular extension mechanism, in the next article we will see how and when the "middleware"
extension pattern can be used.</p>

<p>Looking forward to your feedback.</p>]]></content>
        </entry>        <entry>
            <title>Modular Application Architecture - Intro</title>
            <link href="https://www.goetas.com/blog/modular-application-architecture-intro"/>
            <updated>2017-11-01T00:00:00+01:00</updated>
            <id>https://www.goetas.com/blog/modular-application-architecture-intro</id>                <summary>When developing software, sometimes we need to allow our application to have plug-ins or modules  developed by third parties.  Creating a robust architecture that allows a powerful mechanism can be challenging. In this series of posts we will see some strategies to do it. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the first post from a series of posts that will describe strategies to build 
modular and extensible applications.</strong> (inspired by a talk from <a href="https://qafoo.com/">qafoo</a> in 2011 in Berlin)</p>

<p>When developing a software, one of the most common steps is taking care that the resulting application is extensible and 
modular.</p>

<p>Let's suppose we have our application or library. If we see it from outside, often it looks as a single thing. 
Taking a closer look we can identify different and more-or-less independent set of components. 
Components might still be really interconnected and have dependencies with the rest of the application 
and their development might require a deep knowledge of the rest of the application.</p>

<div class="text-center">

<p><img src="/img/posts/plugin-based-architecture/application.png" alt="application and components" title="" class="not-too-big text-center" /></p>

</div>

<p>As the application grows we can continue adding components... but this comes with a price. 
Components often knows too much of our application and  there is a delicate equilibrium of dependencies between them 
and our application. When not handled carefully, a small change in one component might require changes in many other.</p>

<p><em>As a rule of thumb, I personally try to follow as much as possible the 
<a href="https://en.wikipedia.org/wiki/Acyclic_dependencies_principle">Acyclic dependencies principle</a></em></p>

<p>Another way to allow extensibility but keeping the application "clean" is to introduce modules.</p>

<h2 id="modules">Modules</h2>

<p>Modules are "components" developed outside of the application. Modules communicate the the application only via
defined entry-points.</p>

<p>Different people use different names for modules as plugins, bundles, add-ons and so on... 
but the main idea is always the same, independent functionalities developed outside of the application.</p>

<div class="text-center">

<p><img src="/img/posts/plugin-based-architecture/modules.png" alt="application and components" title="" class="not-too-big" /></p>

</div>

<h2 id="why%3F">Why?</h2>

<p>By using modules that are not part of the application, we gain several advantages as:</p>

<ul>
<li><strong>Customizations</strong>: the application can behave really differently by just enabling/disabling some modules.</li>
<li><strong>Less dependencies</strong>: the modules are more independent from the application itself, till the "entrypoints" are 
compatible, both modules and the application can evolve independently.</li>
<li><strong>Third-party extensions</strong>: since the modules are not part of the application and the "entrypoints" are well defined, 
developing modules can be done by third parties.</li>
<li><strong>Independent development</strong>: Since the application and the modules are independent, they can be: 

<ul>
<li>developed by external developers</li>
<li>released with independent release cycles</li>
<li>developed potentially with different technologies</li>
</ul></li>
<li><strong>Smaller application</strong> The applications is smaller (many functionalities can be implemented via modules) 
and smaller application is translated in better maintainability.</li>
</ul>

<p>Just to make few example to understand why is important to have an application that allows modules:</p>

<ul>
<li><strong>Wordpress</strong> it the most popular blogging platform nowadays. Part of its success is an incredibly powerful and
at the same time incredibly simple plug-in and theme system. </li>
<li><strong>Symfony</strong> is a popular PHP framework. Thanks to its "bundles" system, is possible to get a large variety of ready 
functionalities with almost no effort.</li>
<li><strong>PHP</strong>, itself offers a great set of extensions and those extensions allowed to integrate the language with a large
variety of systems.</li>
</ul>

<h2 id="challenges-to-solve">Challenges to solve</h2>

<p>To make our application modular we have to carefully decide what modules should be able to do, 
how will communicate with the application and how will expose their functionalities.
When implementing a plug-in/modular system we have to consider which part of application behavior
 a module will be allowed to change and in which measure.</p>

<p>As example, if we are developing a word processor,
a module might be able to add just the support for a new file format or just a new icon 
on the toolbar or on the opposite side, to change completely the whole application.</p>

<p>Depending on this decisions we can architect our module very differently, but almost always we will have to 
deal with module structure, registration/configuration, resource management and communication with the core.</p>

<h3 id="1.-module-structure">1. Module structure</h3>

<p>A starting point is the module structure as file naming, folder structure, class naming, etc.
When the module structure is decided, the remaining aspects are highly influenced.</p>

<p>Often the module structure is the first entrypoint with the application.</p>

<p><em>My personal suggestion is to not impose any specific folder structure but to have a single "entrypoint" where 
the module is loaded and using that entrypoint as source of truth and configuration. 
The meaning of "entrypoint" will be defined in the next paragraph</em></p>

<h3 id="2.-registration">2. Registration</h3>

<p>When the module structure is decided, the next step is to let our application know about it.</p>

<p>There are mainly two ways to do it:</p>

<ul>
<li><strong>discovery</strong>: our application can try to use predefined set of "rules" to look for new modules and load/register them.
<br>As example is possible to try to look in <em>specific folder (s)</em> for some <em>particular file (s)</em> 
or check if a <em>particular class (es)</em> is defined and try to instantiate it. </li>
<li><strong>configuration</strong>: we can <em>explicitly configure</em> the application to load a  <em>particular file (s)</em> 
or a <em>particular class (es)</em>.</li>
</ul>

<p>There is no preferred ways to do it, it really depends on your application. 
"configuration" based registration might be better if your audience is more technical while the "discovery" approach 
can offer a more smooth user experience (but might be more difficult to implement).</p>

<p><em>My personal suggestion is to have again a single "entrypoint" that will register 
the module into the application core, can be a single file, class, database row, service-name etc...</em></p>

<p>The application offers one or more "interfaces" to interact with its "core" and they are used via "entrypoints". 
By "interface" here is intended any possible protocol or convention defined by the application
that might be used used by the module to achieve a goal.</p>

<div class="text-center">

<p><img src="/img/posts/plugin-based-architecture/interface.png" alt="application and components" /></p>

</div>

<p>Examples:</p>

<ul>
<li><strong>composer</strong>: the <a href="https://getcomposer.org/">composer</a> way of adding functionalities is by <em>discovery</em>.<br>
Composer will search for installed packages having <code>type=composer-plugin</code>, will search for a property named <code>class</code> 
defined in the package's <code>composer.json</code> file and will register the event listeners declared by that class. 
More info about it are available <a href="https://getcomposer.org/doc/articles/plugins.md">here</a></li>
<li><strong>symfony</strong>: the <a href="https://www.symfony.com/">symfony</a> way of adding functionalities is by <em>configuration</em>.<br>
In the <code>Appkernel</code> class is necessary to define all the "bundles" that will be used by symfony to enrich 
the framework functionalities. The "bundle" class will be instantiated and is responsible for registering the bundle 
into the extension points that symfony allows. 
The bundle class is able to alter almost any part of symfony itself as it has access to 
the symfony core dependency injection mechanism.
More info about it are available <a href="https://symfony.com/doc/3.4/bundles.html">here</a>.
Recently with <a href="https://github.com/symfony/flex">flex</a> an automatic discovery and configuration become available. </li>
<li><strong>laravel</strong>: the <a href="https://getcomposer.org/">laravel</a> way of adding functionalities is by <em>discovery</em>.<br>
Laravel will search for installed packages and will search for a property named <code>extra</code>.<code>laravel</code> 
defined in the package's <code>composer.json</code> file and will register the service providers declared in there. 
More info about it are available <a href="https://laravel.com/docs/5.5/packages">here</a></li>
</ul>

<h3 id="3.-configuration">3. Configuration</h3>

<p>When the module is registered into the core it might be useful to allow some additional configurations.<br>
Most probably the application is configured somehow to match user-expectations, the same thing should happen 
for the module.</p>

<p><em>Is possible to offer different configuration strategies, my personal suggestion is to offer a configuration 
mechanism very close to the way how the application is configured.
If the application is configured via configuration files, environment variables, database entries or a 
nice control panel, the module should be configured in the same way</em></p>

<p>A nice feature that modules should have (and application should allow) are "defaults", so virtually a module might 
need no configuration.</p>

<p>Examples:</p>

<ul>
<li><strong>composer</strong>: the <a href="https://getcomposer.org/">composer</a>'s <code>composer.json</code> file allows you to decide which plugins to load, 
which commands to run and so on.</li>
<li><strong>symfony</strong>: the <a href="https://www.symfony.com/">symfony</a> allows YAML, XML and PHP as configuration languages, 
configuration values can to be declared in <code>config.(yml|xml|php)</code> (and in few other places) and bundles can provide default values for it.</li>
<li><strong>laravel</strong>: the <a href="https://getcomposer.org/">laravel</a> uses mainly PHP as configuration language and thank to it
is possible to use the power of the PHP syntax when defining configurations.</li>
</ul>

<h3 id="4.-resources">4. Resources</h3>

<p>Some modules might need to expose some content to the user as images, CSS, templates, translations, files or similar. 
Applications can take different way to make available the content to the user.</p>

<p>When the resource is "application managed", 
is just about using the "application interface" to provide/override the resource
(as example using the application plugin interface).</p>

<p>When resources are not managed by the application but are module-specific, things can get more 
complicated. Some way to handle it are:</p>

<ul>
<li>linking/copying the content to the web directory

<ul>
<li>requires some step to publish the resources</li>
</ul></li>
<li>serve the resources using the applications, as example pipe-it via PHP 

<ul>
<li>can be slow or resource intensive</li>
</ul></li>
<li>configure the webserver to serve explicitly the plugin resources 

<ul>
<li>makes in vain the "plug &amp; play" idea behind modules by adding additional configurations</li>
</ul></li>
<li><p>let the user "copy and paste" resources manually</p>

<p>Each option has drawbacks and advantages. Lately the "linking/copying" strategy is the most popular.</p></li>
</ul>

<h3 id="5.-interaction-with-the-application">5. Interaction with the application</h3>

<p>Now that the module is loaded and configured, everything is ready.<br />
The application can decide all the "interaction" points and go through modules checking if some module is "interested"
in interacting with the application core (probably)</p>

<p>There are many strategies to "interact" with the application some of them limit the ability of a module to interact 
on a specific (user defined) part if the application while other might allow to change potentially the whole 
application flow.</p>

<p><strong>In the next articles we will go deeper into possible ways to interact with the application.</strong></p>

<h2 id="conclusion">Conclusion</h2>

<p>In this first article I've described some motivations and challenges when building extensible applications. 
In the next articles we will see some strategies of interaction with the application as 
<em>hooks</em>, <em>event manager</em>, <em>observer pattern</em>, <em>inheritance</em> and others.</p>

<p>Looking forward for your feedback.</p>]]></content>
        </entry>        <entry>
            <title>Immobinet at TTG 2017 Rimini</title>
            <link href="https://www.goetas.com/blog/immobinet-at-ttg-2017-rimini"/>
            <updated>2017-10-23T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/immobinet-at-ttg-2017-rimini</id>                <summary>From the 12th to the 14th of October 2017 I was presenting with Mercurio Sistemi S.r.l. &quot;Immobinet&quot;, a software  to manage touristic rentals as apartments, villas any similar types of accommodation. 
</summary>
        </entry>        <entry>
            <title>How do I deploy my Symfony API - Part 5 - Conclusion</title>
            <link href="https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-5-conclusion"/>
            <updated>2017-10-18T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-5-conclusion</id>                <summary>This is the fifth and last article from a series of blog posts on how to deploy a symfony PHP based application to a docker swarm cluster hosted on Amazon EC2 instances.
 This post is a conclusive post. Contains a summary and some tips. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the fifth post from a series of posts that described the whole deploy process from development to production
of a <a href="http://www.symfony.com">Symfony</a> API.</strong></p>

<p>This series of blog posts had the aim to show a possible approach to build a continuous integration and 
continuous delivery pipeline.</p>

<p>The continuous integration and continuous delivery process reduces bugs and makes the development simper.</p>

<p>The blog post story was divided into:</p>

<ul>
<li><p><a href="/blog/how-do-i-deploy-my-symfony-api-part-1-development/">Part 1 - Development</a>
In this step was shown how to build the local development environment using Docker and Docker Compose.
Considering some important details (as example: stateless application or share-nothing containers...), 
the work in the next steps will be much easier.</p></li>
<li><p><a href="/blog/how-do-i-deploy-my-symfony-api-part-2-build/">Part 2 - Build</a>
This step was about "building" and "pushing" the artifacts (the docker images to the docker registry). 
Docker images are ready-to-run applications containing almost everything necessary to be executed on a docker engine.
This is one fundamental steps for the development flow. Here is also a great spot where introduce automated tests.</p></li>
<li><p><a href="/blog/how-do-i-deploy-my-symfony-api-part-3-infrastructure/">Part 3 - Infrastructure</a>
In order to run in a reliable way the application, is necessary to have a properly configured infrastructure
(servers, services running on them, docker...).</p></li>
<li><p><a href="/blog/how-do-i-deploy-my-symfony-api-part-4-deploy/">Part 4 - Deploy</a>
When everything is ready we can finally deploy the application. The application has to stay up and running! Always!</p></li>
</ul>

<h2 id="improvements">Improvements</h2>

<p>As always happen in software development, solutions are not perfect. There is always room for improvement.
Here are few examples of what could have be done better. Obviously can be done better and this are not all the possible 
improvements that can be done on the system.</p>

<h3 id="migrations">Migrations</h3>

<p>Running migrations (database changes as example) in between of deployments is a common use case.</p>

<p>In an environment with multiple copies of the same application running in parallel
(and probably having different versions), the application must be able to run without errors 
with different database versions.
To do that we need a to have backward-compatible migrations.</p>

<p>Let's suppose we have an  <strong>application version:1</strong> running on <strong>database version:1</strong>
and we have to run a migration that renames a database column from "old" to "new". We can do:</p>

<ol>
<li>Run <strong>migration 1</strong>

<ol>
<li>Add a "new" column (with the new name)</li>
<li>Copy values from "old" to "new" column</li>
<li>(now the database version is v2)</li>
</ol></li>
<li>Deploy <strong>application v2</strong> (this app must be able to work with both "new" and "old" column)

<ol start="4">
<li>Wait till all the copies of <strong>application v1</strong> are not in service anymore</li>
</ol></li>
<li>Run <strong>migration 2</strong>

<ol>
<li>Copy values from "old" to "new" column where the "new" values are still NULL (or something similar).<br>
This is necessary because while the <strong>application v2</strong> was going up, <strong>application v1</strong> was still running and
was using only the "old" column. </li>
<li>(at the end of this migration the database version is still v2)</li>
</ol></li>
<li>Deploy <strong>application v3</strong> (knows only the "new" column)<br />

<ol start="4">
<li>Wait till all the copies of <strong>application v2</strong> are not in service anymore</li>
</ol></li>
<li>Run <strong>migration 3</strong>

<ol start="2">
<li>Drop "old" column</li>
<li>(now the database version is v3)</li>
</ol></li>
</ol>

<p>The "new" column can't be marked as "not null" in the first migration 
(this because the first application version does not know about it), 
only in the last migration can be set to eventually to "not null".
If the "not null" constraint is a must, is necessary to specify a default.</p>

<p>In a real world example, supposing we are using <a href="https://github.com/doctrine/migrations">Doctrine Migrations</a> and its 
<a href="https://github.com/doctrine/DoctrineMigrationsBundle">DoctrineMigrationsBundle</a>, to run the migrations we can execute:</p>

<pre><code class="bash">docker-compose run --rm -u www-data --no-deps php bin/console doctrine:migrations:migrate
</code></pre>

<h3 id="health-checks">Health checks</h3>

<p>As of docker-compose 2.1 file format, is possible to specify container 
<a href="https://docs.docker.com/compose/compose-file/#healthcheck">health checks</a> to ensure that our application 
is ready to accept traffic. The docker swarm cluster routing mechanism will send traffic only to healthy containers.</p>

<pre><code class="yaml"># docker-compose.live.yml
version: '3.3'
services:
    php:
        # php service definition...             
    www:
        # www service definition...             
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost/check.php"]
          interval: 30s
          timeout: 10s
          retries: 2            
</code></pre>

<p>This will try to download <code>http://localhost/check.php</code> from the container, in case of two or more not <code>200 OK</code> responses 
will consider the container as not-healthy. Will also not send traffic to that container in case the container is part
of a swarm cluster. Is is also possible to configure a <code>restart_policy</code> to decide what to do with an not-healthy container.</p>

<p>Is possible to configure health checks not only at runtime via docker-compose but also via 
<a href="https://docs.docker.com/engine/reference/builder/#healthcheck">HEALTHCHECK</a> in the Dockerfile.</p>

<h3 id="container-placement">Container placement</h3>

<p>An interesting feature of docker orchestrators is the ability to influence the container placement on the nodes part
of a cluster. Using docker-swarm and docker-compose, 
this is possible using the <a href="https://docs.docker.com/compose/compose-file/#placement"><code>placement</code></a> property.</p>

<p>Each node part of a cluster has some "labels" (system or user defined); docker-compose services can 
set as a requirement to place services having only specific labels.</p>

<pre><code class="yaml">version: '3'
services:
  www:
    # www service definition...             
    deploy:
      placement:
        constraints:
          - node.role == worker
          - engine.labels.operatingsystem != ubuntu 14.04
</code></pre>

<p>In the snippet before, we ask to place the "www" service only on worker nodes having 
operating systems different from "ubuntu 14.04".</p>

<p>This can be very useful if nodes have a specific configuration 
(shared directories, log drivers or public IPs as example) and we want containers to be distributed on specific nodes.</p>

<h3 id="php-and-nginx-together">PHP and Nginx together</h3>

<p>The application shown in this series of post was using  two separate images, one for php and one for the web-server (nginx).
It was also possible to create a single image having both php and nginx together and communicating over a socket instead
of a TCP port.</p>

<p>This is more a philosophical preference that has benefits and drawbacks, will let decide to the reader the one to prefer.
Some advantages are: a single image for both components, performance (?); 
on the other hand as disadvantages: we lose the single responsibility 
principle and we have sometimes more complex and interdependent configurations.</p>

<p>Currently there are no official images to do it using nginx as webserver 
(for example apache + php is <a href="https://hub.docker.com/_/php/">available</a>), 
so is necessary to use some images user-provided as <a href="https://hub.docker.com/r/richarvey/nginx-php-fpm/">richarvey/nginx-php-fpm</a>.</p>

<h3 id="node-draining">Node draining</h3>

<p>When we need to remove a node from a cluster, is necessary to stop sending 
traffic to that node and to remove (or move) containers from that node to avoid service interruptions.
Only when this operations are completed is safe to remove the node.</p>

<p>Highly suggested to read <a href="https://github.com/miglen/aws/blob/master/ec2/run-script-on-ec2-instance-termination.md">this</a>
article on possible ways to do it.</p>

<h2 id="kubernetes">Kubernetes</h2>

<p>Many of you probably already heard of <a href="https://kubernetes.io/">Kubernetes</a>.
Kubernetes is an production-grade container orchestration. It is something similar
to docker-swarm, just way more powerful.</p>

<p>But from great power comes complexity and for me was an overkill for the application I was building.
I also already had experience with docker swarm, so for me was a natural choice.
From many point of views, Kubernetes looks way superior to Docker-Swarm and could have been a valid alternative.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I wanted to share my experience. While writing this series of post I learned many details and also had a chance to 
improve my application.</p>

<p>Hope you (reader) enjoyed the articles and as already said many times, feedback is welcome.</p>]]></content>
        </entry>        <entry>
            <title>How do I deploy my Symfony API - Part 4 - Deploy</title>
            <link href="https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-4-deploy"/>
            <updated>2017-10-10T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-4-deploy</id>                <summary>This is the forth article from a series of blog posts on how to deploy a symfony PHP based application to a docker swarm cluster hosted on Amazon EC2 instances.
 This post focuses on the final and last step, the deploy. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the forth post from a series of posts that will describe the whole deploy process from development to production.</strong>
The first article is available <a href="/blog/how-do-i-deploy-my-symfony-api-part-1-development/">here</a>,<br />
the second <a href="/blog/how-do-i-deploy-my-symfony-api-part-2-build/">here</a> and 
the third <a href="/blog/how-do-i-deploy-my-symfony-api-part-3-infrastructure/">here</a>.</p>

<p>After covering the steps 1-3 and having prepared our infrastructure, 
we can see how to <strong>deploy</strong> our application to production. 
Almost the same approach can be used to deploy not only to production but also to test environments.</p>

<h2 id="workflow">Workflow</h2>

<p>Different "git push" operation should trigger different actions.
Just as example a push to <code>master</code> should trigger a deploy to production, while other branches may trigger a deploy to 
a test environment, or not trigger deploys at all.</p>

<p>I've used <a href="https://circleci.com/docs/2.0/workflows/">Circe CI Workflows</a> to manage this set of decisions.</p>

<p>Workflows are just another section from the same <code>.circleci/config.yml</code> file and here they are:</p>

<pre><code class="yaml">workflows:
  version: 2
  build_and_deploy-workflow:
    jobs:
      - build
      - deploy_to_live:
          requires:
            - deploy_to_live_approval
          filters:
            branches:
              only:
                - master

      - deploy_to_live_approval:
          type: approval
          requires:
            - build
</code></pre>

<p>In this workflow configuration we have 3 jobs.</p>

<ul>
<li><code>build</code>: is the main job; the one explained in the <a href="/blog/how-do-i-deploy-my-symfony-api-part-2-build/">second article</a>
and responsible for pushing the images to the docker registry.</li>
<li><code>deploy_to_live</code>: is the job for the deploy to live (will talk about it in a moment); this job will be executed 
only for the branch named <code>master</code> and<br />
before running requires a successful completion of a job called <code>deploy_to_live_approval</code>.</li>
<li><code>deploy_to_live_approval</code>: is an "type = approval"  job, and its completion is just a button on the Circle CI web interface; 
this allow us to effectively decide if deploying to live or not.</li>
</ul>

<p>The node <code>build_and_deploy-workflow</code> is just a workflow "name", 
CircleCi allows multiple workflows for the same project, but will not handle this topic now.</p>

<p><img src="/img/posts/circleci-approve.png" alt="The circle CI approve button" /></p>

<h2 id="the-deploy-job">The deploy job</h2>

<p>As said, there is a job named <code>deploy_to_live</code> that is responsible for the live deploy.
The job is just another portion of the same <code>.circleci/config.yml</code> file we saw in this and previous articles.</p>

<pre><code class="yaml">version: 2
executorType: machine
jobs:
  build: # this is the job that pushes the images to the registry 
    # ...

  deploy_to_live: # this is the job that effectively deploys to live 
    working_directory: ~/my_ap
    environment:
      - DOCKER_HOST: "tcp://myapp-manager.yyy.local:2375"
    steps:
      - *helpers_system_basic
      - *helpers_docker
      - run: sudo apt-get -qq -y install openvpn
      - checkout
      - add_ssh_keys:
          fingerprints:
            - "af:83:39:00:ad:af:83:39:00:ad:af:83:39:00:ad:99"  # import VPN private key
      - run:
          name: Connect to VPN
          command: |
            sudo openvpn --daemon --cd .circleci/vpn-live --config my-vpn-config.ovpn
            while ! (echo "$DOCKER_HOST" | sed 's/tcp:\/\///'|sed 's/:/ /' |xargs nc -w 2) ;  do sleep 1; done

      - deploy:
          name: Deploy
          command: |
            docker login -u $DOCKER_HUB_USERNAME -p $DOCKER_HUB_PASS
            docker stack deploy live --compose-file=docker-compose.live.yml --with-registry-auth
</code></pre>

<h3 id="step-by-step">Step by step</h3>

<p>Let's analyze step-by-step the build process by looking in detail at the <code>.circleci/config.yml</code> file.</p>

<h4 id="preparation">Preparation</h4>

<pre><code class="yaml">environment:
       - DOCKER_HOST: "tcp://myapp-manager.yyy.local:2375"`
</code></pre>

<pre><code class="yaml">- *helpers_system_basic # use basic system configurations  helper
- *helpers_docker # use basic docker installation  helper
- run: sudo apt-get -qq -y install openvpn # install openvpn client
- checkout # checkout the source code
</code></pre>

<p>Again, as it was in the <code>build</code>, this part of the configuration file is just about setting up some basics for the 
deploy environment. The only difference with the build job is the <code>openvpn</code> package installation because it will be
necessary to connect to the docker swarm manager that will run the deploy.</p>

<p>We also export an environment variable (<code>DOCKER_HOST</code>) for the docker daemon targeting our docker swarm cluster manager.</p>

<h4 id="credentials">Credentials</h4>

<pre><code class="yaml">- add_ssh_keys:
  fingerprints:
    - "af:83:39:00:ad:af:83:39:00:ad:af:83:39:00:ad:99" 
</code></pre>

<p>This snipped is about importing the "af:83:39:00:ad:af:83:39:00:ad:af:83:39:00:ad:99" private key into the environment.</p>

<p>The key needs to be placed into the CircleCI web interface before. The key will be available at 
<code>/home/circleci/.ssh/id_rsa_af833900adaf833900adaf833900ad99</code>.</p>

<h4 id="vpn">VPN</h4>

<pre><code class="yaml">- run:
  name: Connect to VPN
  command: |
    sudo openvpn --daemon --cd .circleci/vpn-live --config my-vpn-config.ovpn
    while ! (echo "$DOCKER_HOST" | sed 's/tcp:\/\///'|sed 's/:/ /' |xargs nc -w 2) ;  do sleep 1; done
</code></pre>

<p>This will connect to the VPN and will wait till the connection to 
<code>myapp-manager.yyy.local</code> on the port 2377 does not become available.</p>

<p>The folder <code>.circleci/vpn-live</code> contains the OpenVPN configuration files necessary for the connection to the VPN.
I will not tackle this topic as it is a completely different subject.</p>

<h4 id="the-deploy">The deploy</h4>

<pre><code class="yaml">- deploy:
  name: Deploy
  command: |
    docker login -u $DOCKER_HUB_USERNAME -p $DOCKER_HUB_PASS
    docker stack deploy live --compose-file=docker-compose.live.yml --with-registry-auth
</code></pre>

<p>This is obviously <strong>the most important part</strong>, the deploy to the cluster.
We login to the docker registry and later running <code>docker stack deploy</code>  effectively deploys the application.</p>

<p><strong>The omitted part:</strong> most probably you application needs some credentials for the database connection, 
api keys and many other configuration services.
A good way to handle credentials can be using done using the 
<a href="https://docs.docker.com/engine/swarm/secrets/">docker secrets management</a>, but in this application<br />
I've used <a href="https://symfony.com/blog/new-in-symfony-3-2-runtime-environment-variables">symfony environment variables</a> 
to configure the application, placed them in the CircleCI web interface, and exported them right before running 
<code>docker stack deploy</code>.</p>

<pre><code class="bash">export DB_USER="$LIVE_DB_USER"
export DB_PWD="$LIVE_DB_PWD"
docker stack deploy live --compose-file=docker-compose.live.yml --with-registry-auth
</code></pre>

<p>Alternatively is possible to place the exports in a dedicated file (<code>exports_live_vars.sh</code> as example).</p>

<pre><code class="bash">source exports_live_vars.sh
docker stack deploy live --compose-file=docker-compose.live.yml --with-registry-auth
</code></pre>

<p>You (reader) should have noticed that I've used a different docker-compose file, named<code>docker-compose.live.yml</code> for 
the deploy.</p>

<h2 id="the-%60docker-compose.live.yml%60">The <code>docker-compose.live.yml</code></h2>

<pre><code class="yaml"># docker-compose.live.yml
version: '3.3'
services:
    php:
        image: goetas/api-php:master 
        deploy:
          replicas: 6
          update_config:
            parallelism: 2
            delay: 30s
          restart_policy:
            condition: on-failure                 
    www:
        image: goetas/api-nginx:master
        deploy:
          replicas: 6
          update_config:
            parallelism: 2
            delay: 30s
          restart_policy:
            condition: on-failure            
        ports:
            - "80:80"
</code></pre>

<p>This <code>docker-compose.live.yml</code> is much simpler than the <code>docker-compose.yml</code> file used for development.</p>

<p>Whe <code>www</code> container binds the port <code>80</code> so the web server is exposed.
The rest of the file defines only the image names to download from the registry and the section <code>deploy</code>
used to configure the rolling updates policy 
(<a href="http://searchitoperations.techtarget.com/definition/rolling-deployment">this</a> article gives a good overview 
of what a rolling update is).</p>

<p>The policies defined by <code>deploy</code> are:</p>

<ul>
<li>Deploy 4 containers for each service. Hopefully they will be uniformly distributed across the cluster, 
but this is not guaranteed. Docker offers a <code>placement</code> configuration option to instruct the scheduler 
on how to distribute containers across the cluster. </li>
<li>When updating the containers (as example a second deploy) update two containers and wait for 30 seconds before 
updating other two containers. <br>
Updating the container here means: download latest image, stop and remove old container, 
create and start the container using the new image. </li>
<li>Restart the containers if they fail. In case of "weird" errors that should not happen anyway but they will.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>This article combines the results achieved in the previous three articles and show how is possible to setup
a relatively sophisticated and expandable continuous delivery pipeline.</p>

<p>The application has been developed, (tested!), built, (re-tested!) and deployed.</p>

<p>Obviously there are as usual many topics that require attention and improvements, as:</p>

<ul>
<li>How to run migrations for database schema changes?</li>
<li>How to maximize the server resource usage?</li>
<li>How to secure the deploy better than a VPN?</li>
<li>What about <code>healthcheck</code>s?</li>
</ul>

<p>In the next article will make a summary of this 4 blog posts and will try to answer to some of the open questions 
by showing some of the improvements implemented in the project that were not easy to pace in the blog post stories.</p>]]></content>
        </entry>        <entry>
            <title>How do I deploy my Symfony API - Part 3 - Infrastructure</title>
            <link href="https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-3-infrastructure"/>
            <updated>2017-10-04T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-3-infrastructure</id>                <summary>This is the third article from a series of blog posts on how to deploy a symfony PHP based application to a docker swarm cluster hosted on Amazon EC2 instances.
 This post focuses on the AWS and instances configuration. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the third post from a series of posts that will describe the whole deploy process from development to production.</strong>
The first article is available <a href="/blog/how-do-i-deploy-my-symfony-api-part-1-development/">here</a> and 
the second <a href="/blog/how-do-i-deploy-my-symfony-api-part-2-build/">here</a>.</p>

<p>In the previous two articles I've covered the steps 1-3 from the schema here below.
Before being able to discuss the steps 4-5  we will need to see how to <strong>configure the instances</strong>  properly 
to be able to deploy our application to a docker-swarm cluster with rolling updates.</p>

<p><img src="/img/posts/deploy-php-1-schema.png" alt="Deploy schema summary" /></p>

<h2 id="database">Database</h2>

<p>The database (PostgreSQL) used for the application was a simple <a href="https://aws.amazon.com/rds/">AWS RDS</a> instance.</p>

<p>The configuration is pretty simple and requires few steps to be put in place.
I've not used an home-made postgres installation with or without docker just because the AWS solution works well 
and for few $$ more offers you security and scalability.</p>

<h2 id="docker-swarm">Docker swarm</h2>

<p><a href="https://docs.docker.com/engine/swarm/">Docker swarm</a> is a specific docker configuration that allows you to see a group 
of physical or virtual machines as a single one. 
It allows you to deploy docker images on it as on an ordinary docker engine, but the load will be distributed
across multiple nodes.</p>

<p>This is a very simplified and probably imprecise definition, but personally, to me it gives the idea of what docker
swarm does.</p>

<p>Docker swarm has one or many manager nodes (the one that decides who does what) and zero or many worker nodes (the one
who can only do the job). To not waste resources, manager nodes act also as workers too (so they take jobs too).</p>

<div class="text-center">

<p><img src="/img/posts/docker-swarm.jpg" alt="Deploy swarm architecture" /></p>

</div>

<p>From outside, requesting <code>http://node1/service1</code> or <code>http://node2/service1</code> or <code>http://node3/service1</code> 
makes no difference since the request will be internally re-routed randomly to one of the nodes 
part of the docker swarm cluster that hosts the <code>service1</code>. 
Note that <code>service1</code> can be running on multiple nodes at the same time, is up to the developer make sure to have always
a consistent application state (as example by implementing stateless services or having a shared state storage).</p>

<h2 id="aws-configuration">AWS configuration</h2>

<p>Here are some important details about the AWS and <a href="https://aws.amazon.com/vpc/">VPC</a> configuration that are necessary
 do know in order to understand better the article, but are out of the scope for this series of posts.</p>

<ul>
<li>The AWS VPC had the internal DNS resolution enabled.</li>
<li>There was a VPN configured to access the VPC from outside (<code>openvpn</code> was installed on an instance on a public subnet)
and the commands executed here imply being connected to the VPN.</li>
<li>The VPC internal DNS was pushed down to VPN clients by allowing them to resolve AWS internal IPs.</li>
<li>AWS security groups were properly configured (this was the biggest headache!).</li>
</ul>

<p>All the nodes were exposing the port 80 and using a <a href="https://aws.amazon.com/elasticloadbalancing/">ELB</a> 
(Elastic load balancer) was possible to distribute the load across the cluster.</p>

<p>If somebody is interested to go deeper into one of the topics, just let me know, 
can be a good topic for one the next posts.</p>

<h2 id="instance-spin-up-and-type">Instance spin-up and type</h2>

<p>As said, the application was hosed on AWS, more precisely a region with 3 adaptability zones
(more info on this <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">here</a>).
The 3 availability zones are an important fact for a fault-tolerance of a swarm cluster as explained 
<a href="https://docs.docker.com/engine/swarm/admin_guide/#add-manager-nodes-for-fault-tolerance">here</a>.</p>

<p>I've used two separate <a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroup.html">Auto Scaling Groups</a>, 
one for the manager nodes and one for the worker nodes.</p>

<ul>
<li>Manager nodes Auto Scaling Group: configured to have always one node per availability zone.</li>
<li>Worker nodes Auto Scaling Group: configured to have 1 or more nodes depending on the CPU usage.</li>
</ul>

<p>Each node, when spinning up was executing a simple bash script 
(<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html">AWS User data scripts</a>), 
that was allowing to run some specific node configurations and decide if a node is manager or a worker.</p>

<h3 id="manager-nodes">Manager nodes</h3>

<p>As said, manager nodes in a swarm cluster responsible in managing the cluster state. 
For more detail on administrating manager nodes, 
<a href="https://docs.docker.com/engine/swarm/admin_guide/">here</a> is a great overview of what might be necessary to know.</p>

<p>Manager nodes had a <a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html">Private Hosted Zone</a>.
On creation (in the user data script), manager nodes were registering them self in a private hosted zone, 
more specifically, manager nodes were setting their IP into a record <code>myapp-manager.yyy.local</code> 
(instead of "myapp-manager.yyy.local" I've used obviously something more meaningful).<br>
<code>myapp-manager.yyy.local</code> was a record with a 
<a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html">Failover</a> routing policy.
AWS health checks were making sure to not return records for not healthy nodes. 
This made possible to have always a docker swarm manager available at that address/name.</p>

<p>Here the script that was executed for on each manager node creation.</p>

<pre><code class="bash">#!/bin/bash -v
set -ex

HOSTED_ZONE_ID='xxx'
HOSTED_ZONE_RECORD='myapp-manager.yyy.local'

# install basics
apt-get update -y -m &amp;&amp; apt-get upgrade -y -m
apt-get -y install awscli curl &gt; /tmp/userdata.log

# docker installation
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
apt-get update -q -y &amp;&amp; apt-get install -y docker-ce

# configure docker swarm
if nc "$HOSTED_ZONE_RECORD" 2377 &lt; /dev/null -w 10; then #
    # join the cluster as manager
    docker swarm join --token $(docker -H "$HOSTED_ZONE_RECORD" swarm join-token -q manager);
else 
    # create the swarm
    docker swarm init; 
fi

# update DNS
IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)

echo "
{
    \"Comment\": \"Update record for manager node\",
    \"Changes\": [
        {
            \"Action\": \"UPSERT\",
            \"ResourceRecordSet\": {
                \"Name\": \"$HOSTED_ZONE_RECORD.\",
                \"Type\": \"A\",
                \"TTL\": 300,
                \"ResourceRecords\": [
                    {
                        \"Value\": \"$IP\"
                    }
                ]
            }
        }
    ]
}
" &gt; /tmp/change-resource-record-sets.json
aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --change-batch file:///tmp/change-resource-record-sets.json

exit 0;
</code></pre>

<p>Let's analyze what this script does:</p>

<ul>
<li>We have some basic system setup as the <code>curl</code> installation and package updates.</li>
<li>Then Docker is installed via its official packages (the underlying distribution was a recent Debian)</li>
<li>The docker swarm cluster configured:

<ul>
<li>If there is a host listening on <code>myapp-manager.yyy.local:2377</code> means that the cluster is running 
and the node has to <a href="https://docs.docker.com/engine/reference/commandline/swarm_join/">join a swarm</a>
and does is using the <code>docker swarm join</code> command.<br> 
The command <code>docker -H "$HOSTED_ZONE_RECORD" swarm join-token -q manager</code> connects to a currently running manager 
and retrieves the token necessary to join the cluster as a manager.</li>
<li>If there are no nodes listening on <code>myapp-manager.yyy.local:2377</code> means that the cluster has not yet been created
and the current node is the responsible for creating the cluster, so it does.</li>
</ul></li>
<li>Using the AWS client, the node register it self to the DNS record <code>myapp-manager.yyy.local</code>, so newly created managers
can connect to it when crated by the auto scaling group.</li>
</ul>

<p>With this script our manager was ready to host new docker containers and was reachable via <code>myapp-manager.yyy.local</code> DNS
name.</p>

<p>We can check how it went the cluster creation operation by running the following command.</p>

<pre><code class="bash">docker -H myapp-manager.yyy.local node ls
</code></pre>

<p>The output should be something as:</p>

<pre><code>ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
cer8w4urw 8yxrwzr987wyrzw    node3     Ready   Active        Reachable
ecru45c8m5uw358xmw0x68cw8    node2     Ready   Active        Reachable
ecruw490rx4w8w4ynw4vy9wv4 *  node1     Ready   Active        Leader
</code></pre>

<p>In this example the command was executed from an instance outside of a cluster, 
for that reason, the option <code>-H myapp-manager.yyy.local</code> was used.
If the command was executed from a shell inside one of the managers, the <code>-H option</code> will be not necessary</p>

<h3 id="worker-nodes">Worker nodes</h3>

<p>In a swarm cluster worker nodes run containers assigned to them by the manager nodes.</p>

<p>Here is the script executed on node creation.</p>

<pre><code class="bash">#!/bin/bash -v
set -ex

HOSTED_ZONE_RECORD='myapp-manager.yyy.local'

# install basics
apt-get update -y -m &amp;&amp; apt-get upgrade -y -m
apt-get -y install curl &gt; /tmp/userdata.log

# docker installation
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
apt-get update -q -y &amp;&amp; apt-get install -y docker-ce

# join the cluster as worker
docker swarm join --token $(docker -H "$HOSTED_ZONE_RECORD" swarm join-token -q worker);

exit 0;
</code></pre>

<p>Let's analyze what the worker script does:</p>

<ul>
<li>We have some basic system setup as the <code>curl</code> installation and package updates.</li>
<li>Then Docker is installed via its official packages (the underlying distribution was a recent Debian)</li>
<li>The docker swarm cluster configured:

<ul>
<li>The worker joins the cluster using the <code>docker swarm join</code> command.<br> 
The command <code>docker -H "$HOSTED_ZONE_RECORD" swarm join-token -q worker</code> connects to a currently running manager 
and retrieves the token necessary to join the cluster as a worker.</li>
</ul></li>
</ul>

<p>When this script completes the worker is part of the cluster.</p>

<p>We can check how it went the worker creation operation by running the following command.</p>

<pre><code class="bash">docker -H myapp-manager.yyy.local node ls
</code></pre>

<p>The output should be something as:</p>

<pre><code>ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
8j34r9j3498rj34f9n34f9448    node4     Ready   Active
cer8w4urw 8yxrwzr987wyrzw    node3     Ready   Active        Reachable
ecru45c8m5uw358xmw0x68cw8    node2     Ready   Active        Reachable
ecruw490rx4w8w4ynw4vy9wv4 *  node1     Ready   Active        Leader
</code></pre>

<p>We can see here a node <code>node4</code> as worker.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article I've explained how AWS and the VPC was configured. 
Now the instances are ready to receive the docker images to run.</p>

<p>Even in this case there are many gray areas that require attention and improvement, 
but the project was in a very early stage and some aspects had to be postponed.</p>

<p>Some of the improvements I can easily spot are:</p>

<ul>
<li>Use custom <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">AMI</a> with pre-installed and up to date
docker to have faster the spin-up time. </li>
<li>Handle the container re-balancing when the auto scaling group creates a worker node.</li>
<li>The use of an ELB and the way how docker-swarm works (the traffic is internally re-balanced) changes 
partially the aim of a load balancer (suggestions are welcome!).</li>
<li>When the cluster is scaling down, <a href="https://docs.docker.com/engine/swarm/swarm-tutorial/drain-node/">drain</a> 
the nodes to make sure to not "kill" active connections 
and gracefully <a href="https://docs.docker.com/engine/reference/commandline/swarm_leave/">leave</a> the cluster.
<br>
Possible ways to do it in my opinion are available
<a href="https://circleci.com/blog/graceful-shutdown-using-aws/">here</a> 
or <a href="https://github.com/miglen/aws/blob/master/ec2/run-script-on-ec2-instance-termination.md">here</a></li>
<li>When a node just "dies" without executing any maintenance script, will be seen by the cluster as "Unavailable" and 
needs to be manually removed. Not sure how this can be automated.</li>
<li>An alternative to AWS DNS to be used for manager discover can be <a href="https://www.consul.io/">Consul</a></li>
</ul>

<p>Most probably many other things can be improved but it depends a lot on specific use cases.</p>

<p>In the <strong>next article</strong> we will se how will be possible to <strong>deploy the application</strong> prepared in the 
first two articles.</p>

<p>As usual, I'm always happy to hear constructive feedback and to learn what else can be improved in the above explained
setup.</p>]]></content>
        </entry>        <entry>
            <title>How do I deploy my Symfony API - Part 2 - Build</title>
            <link href="https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-2-build"/>
            <updated>2017-09-27T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-2-build</id>                <summary>This is the second article from a series of blog posts on how to deploy a symfony PHP based application to a docker swarm cluster hosted on Amazon EC2 instances.
 This post focuses on the build process, executed on a continuous integration server. 
</summary>                <content type="html"><![CDATA[<p><strong>This is the second post from a series of posts that will describe the whole deploy process from development to production.</strong>
The first article is available <a href="/blog/how-do-i-deploy-my-symfony-api-part-1/">here</a>.</p>

<p>Here a short summary of the blog article series, quoted directly from the first post.</p>

<blockquote>
  <p>The application was a more-or-less basic API implemented using <a href="https://www.symfony.com">Symfony</a> 3.3 and few other things
  as <a href="http://www.doctrine-project.org/projects/orm.html">Doctrine ORM</a> 
  and <a href="https://github.com/FriendsOfSymfony/FOSRestBundle">FOS Rest Bundle</a>.
  Obviously the source code was stored in a GIT repository.</p>
  
  <p>When the project/application was completed, I've used<br />
  <a href="https://circleci.com">CircleCI</a> (that offers 1 container for private builds for free) to coordinate the "build and deploy"
  process.</p>
  
  <p>The repository code was hosted on Github and CircleCI was configured to trigger a build on each commit.</p>
  
  <p>As development flow I've used <a href="http://nvie.com/posts/a-successful-git-branching-model/">GitFlow</a> and each commit to 
  <code>master</code> was triggering a deploy to live, each commit to <code>develop</code> was triggering a deploy to staging
   and each commit to feature branches was triggering a deploy to a test environment.
  Commit to release branches were triggering a deploy to a pre-live environment. 
  The deploy to live had a manual confirmation step.</p>
</blockquote>

<p>In this part we will see how the <strong>build</strong> process was organized.</p>

<h2 id="build-process">Build process</h2>

<p>The build process was triggered on each push to the GitHub repository. The process was identical for all the 
branch "types" (master/develop/feature branches).
This was fundamental to reduce environment-differences, it is a common problem a situation when something 
works on staging but does not work on production. The goal here was to have the development, build, staging and 
production environments as close as possible.</p>

<p>The build was performed using CircleCI, and the whole flow was managed by a single <code>.circleci/config.yml</code> file
 to decide what to do in each step. Might be useful to have a quick look to the 
 <a href="https://circleci.com/docs/2.0/configuration-reference/">CircleCI config file reference</a>.</p>

<h3 id="the-configuration-file">The configuration file</h3>

<p></p>

<pre><code class="yaml"># helper nodes
helpers: &amp;helpers
  - &amp;helpers_system_basic # basic system configurations  helper
    run:
      name: System basic
      command: |
        sudo apt-get -qq update
        sudo apt-get -qq -y install \
        apt-transport-https \
        ca-certificates \
        curl wget \
        software-properties-common openvpn
        pip install -q awscli

  - &amp;helpers_docker # basic docker installation  helper
    run:
      name: Docker installation
      command: |
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
        sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
        sudo apt-get -qq update
        sudo apt-get install -y -qq docker-ce

  - &amp;helpers_docker_compose # docker compose installation helper
    run:
      name: Install Docker Compose
      command: |
        sudo curl -L https://github.com/docker/compose/releases/download/1.15.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
        sudo chmod a+x /usr/local/bin/docker-compose


# build configurations
version: 2
executorType: machine
jobs:
  build:
    working_directory: ~/my_ap
    steps:
      - *helpers_system_basic # use basic system configurations  helper
      - *helpers_docker # use basic docker installation  helper
      - *helpers_docker_compose # docker compose installation helper
      - checkout # checkout the source code

      - restore_cache: # restore docker images cache
          keys:
            - myproject-{{ epoch }}
            - myproject

      - restore_cache: # restore composer cache
          keys:
            - myproject-composer-{{ checksum "composer.lock" }}

      - run: # load docker images cache into docker engine
          name: Restore docker images cache
          command: |
            mkdir -p ~/cache
            if [ -e ~/cache/docker-images-cache.tar.gz ]; then docker load -i ~/cache/docker-images-cache.tar.gz; fi

      - run:
          name: Build
          command: |
            docker-compose build

      - run:
          name: Install dependencies
          command: |
            mkdir -p ~/.composer &amp;&amp; chmod a+rw -R vendor var ~/.composer
            chmod a+rw "$SSH_AUTH_SOCK"
            docker-compose run --rm -u www-data php composer install --no-dev --no-autoloader --no-progress --no-interaction --no-scripts --prefer-dist
            docker-compose build php

      - deploy:
          name: Push images to registry
          command: |
            docker login -u $DOCKER_HUB_USERNAME -p $DOCKER_HUB_PASS
            docker-compose push

      - save_cache: # save composer cache
          key: myproject-composer-{{ checksum "composer.lock" }}
          paths:
            - ~/.composer

      - run:  # save docker images into a TAR file
          name: Save images cache
          command: |
            docker save -o ~/cache/docker-images-cache.tar.gz $(docker images -q -a)

      - save_cache:  # cache the docker TAR file  
          key: myproject-{{ epoch }}
          paths:
            - ~/cache
</code></pre>

<p></p>

<p>The configuration file might look complicated, but lets analyze it step by step.</p>

<p>The first part of the file with the key <code>helpers</code> is just a set of helper nodes obtained using the 
<a href="http://blog.daemonl.com/2016/02/yaml.html">YAML anchors and references</a>, that allows the reuse of portions of YAML code
on multiple places.</p>

<h2 id="step-by-step">Step by step</h2>

<p>Let's analyze step-by-step the build process by looking in detail at the <code>.circleci/config.yml</code> file.</p>

<h4 id="preparation">Preparation</h4>

<pre><code class="yaml">- *helpers_system_basic # use basic system configurations  helper
- *helpers_docker # use basic docker installation  helper
- *helpers_docker_compose # docker compose installation helper
- checkout # checkout the source code
</code></pre>

<p>The first part of the file consists in simply installing the required libraries for our build, this includes
 installing some basic libraries as <code>curl</code>, <code>docker</code>, 
<code>docker-compose</code> and updating the general system configuration 
(in this case Ubuntu 14.04 was the underlying Linux distribution).</p>

<h4 id="restoring-caches">Restoring caches</h4>

<p></p>

<pre><code class="yaml">- restore_cache: # restore docker images cache
  keys:
    - myproject-{{ epoch }}
    - myproject

- restore_cache: # restore composer cache
  keys:
    - myproject-composer-{{ checksum "composer.lock" }}

- run: # load docker images cache into docker engine
  name: Restore docker images cache
  command: |
    mkdir -p ~/cache
    if [ -e ~/cache/docker-images-cache.tar.gz ]; then docker load -i ~/cache/docker-images-cache.tar.gz; fi
</code></pre>

<p></p>

<p>To speed up the build process I've tried to cache as many things as possible, this includes docker images and
composer file. The two caches are separate, this allows to refresh independently the files as most likely they will not change 
together.</p>

<p>The composer cache will change only if the <code>composer.lock</code> changes, while the docker image cache will change probably on 
each build.
Saving the docker images cache might be not trivial as it requires to save the images in a TAR file running the
<code>docker save</code> command and to restore the cache is necessary to use the <code>docker load</code> command. Docker does not offer a 
folder that can be simply copied as composer does.</p>

<h4 id="build">Build</h4>

<p></p>

<pre><code class="yaml">- run:
  name: Build
  command: |
    docker-compose build

- run:
  name: Install dependencies
  command: |
    mkdir -p ~/.composer &amp;&amp; chmod a+rw -R vendor var ~/.composer
    chmod a+rw "$SSH_AUTH_SOCK"
    docker-compose run --rm -u www-data php composer install --no-dev --no-autoloader --no-progress --no-interaction --no-scripts --prefer-dist
    docker-compose build php
</code></pre>

<p></p>

<p>To create the docker images we simply run <code>docker-compose build</code> that will build all the images following the 
<code>docker-compose.yml</code> file shown in the <a href="/blog/how-do-i-deploy-my-symfony-api-part-1/">previous article</a>.</p>

<p>After the images are ready we can install the composer dependencies. Note that the <code>composer install</code> command 
is executed disabling the post install and the autoloader creation.
When the dependencies are installed, the "php" image is re-build to include the just created vendor folder.
In the rebuild process, he autoloader is created 
and the post install command is executed (will warm-up the symfony cache).</p>

<p><em>Note: this solution can be not optimal in some circumstances.<br>
In the described project we had not yet tests (early 
stage start-up).
<strong>I highly recommend to have a good test suite and to run it on each build</strong>. 
The tests can be executed right after the build process or in a separate CircleCI build 
(this step will be explained in the next articles by 
introducing the <a href="https://circleci.com/docs/2.0/workflows/">CircleCI Workflows</a>).</em></p>

<p><strong>TIP: Private repositories</strong></p>

<p>If you have some private repositories in your project
and you rely in the SSH keys to download them in the <code>composer install</code> step, 
you will have to enable the SSH key forwarding from the build environment to the docker container and to add a valid 
SSH key on to the build machine (CircleCI calls them "Checkout SSH keys" under the project setting menu).</p>

<p>To do it is necessary to edit the <code>docker-compose.yml</code> file by adding the volume reference to the SSH socket.</p>

<pre><code class="yaml">php: 
    volumes:
      - $SSH_AUTH_SOCK:$SSH_AUTH_SOCK
    environment:
      - SSH_AUTH_SOCK
</code></pre>

<p>(this is the reason for <code>chmod a+rw "$SSH_AUTH_SOCK"</code> in the "Install dependencies" step)</p>

<h4 id="push">Push</h4>

<p></p>

<pre><code class="yaml">- deploy:
  name: Push images to registry
  command: |
    docker login -u $DOCKER_HUB_USERNAME -p $DOCKER_HUB_PASS
    docker-compose push
</code></pre>

<p></p>

<p>When the docker images are ready we just push them to the DockerHub.</p>

<p>If you are on AWS, most probably is more convenient (economically) to push tem to the Amazon ECR. In that case
the "login" part will be slightly different as the ECR has a bit different authentication mechanism.</p>

<p>Will be something as:</p>

<pre><code class="bash">aws configure set aws_access_key_id $AWSKEY
aws configure set aws_secret_access_key $AWSSECRET
aws configure set region $AWS_REGION

eval $(aws ecr get-login --no-include-email)

docker-compose push
</code></pre>

<p>In this case is necessary to install the <code>aws-cli</code> (the Amazon console client). The environment variables contains
the amazon region and credentials for the registry-push operation.</p>

<p></p>

<h4 id="save-caches">Save caches</h4>

<pre><code class="yaml">- save_cache: # save composer cache
  key: myproject-composer-{{ checksum "composer.lock" }}
  paths:
    - ~/.composer

- run:  # save docker images into a TAR file
  name: Save images cache
  command: |
    docker save -o ~/cache/docker-images-cache.tar.gz $(docker images -q -a)

- save_cache:  # cache the docker TAR file  
  key: myproject-{{ epoch }}
  paths:
    - ~/cache
</code></pre>

<p></p>

<p>If everything went fine, we can save the caches that will allow us to speed up the next builds.
To note, if the <code>composer.lock</code> file did not change between builds, the "save cache step" will be just skipped, 
saving some time necessary to upload the (not-modified) composer cache directory.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article I've explained how the build process was done (including the push of the docker images to the 
registry). In the next articles the focus will move on how to deploy to some environment (live/staging...) the just 
pushed images.</p>

<p>As you can see I've decided to not use any of the pre-installed tools/services/libraries offered by CircleCI.
The reason behind it is that I want to use exactly what developers are using and what will be deployed on production.
By doing so, in my opinion the environment differences are less and the possibility of bugs caused by those differences
decreases.</p>

<p>As usual I think that many things can be improved and I'm always happy to hear constructive feedback.
As <a href="/blog/how-do-i-deploy-my-symfony-api-part-1/#db-postgres">already happen</a> will be happy to publish updates.</p>]]></content>
        </entry>        <entry>
            <title>How do I deploy my Symfony API - Part 1 - Development</title>
            <link href="https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-1-development"/>
            <updated>2017-09-20T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-do-i-deploy-my-symfony-api-part-1-development</id>                <summary>This is the first from a series of blog posts on how to deploy a symfony PHP based application to a docker swarm cluster hosted on Amazon EC2 instances.
 This post focuses on the development environment. 
</summary>                <content type="html"><![CDATA[<p>In this blog post I'm going to share the deploy strategy I've used to deploy an API-centric application to AWS for a customer.</p>

<p><strong>This is the first post from a series of posts that will describe the whole deploy process from development to production.</strong></p>

<p>The application was a more-or-less basic API implemented using <a href="https://www.symfony.com">Symfony</a> 3.3 and few other things
as <a href="http://www.doctrine-project.org/projects/orm.html">Doctrine ORM</a> 
and <a href="https://github.com/FriendsOfSymfony/FOSRestBundle">FOS Rest Bundle</a>.
Obviously the source code was stored in a GIT repository.</p>

<p>When the project/application was completed, I've used<br />
<a href="https://circleci.com">CircleCI</a> (that offers 1 container for private builds for free) to coordinate the "build and deploy"
process.</p>

<p>The process can be summarized in the following schema:</p>

<p><img src="/img/posts/deploy-php-1-schema.png" alt="Deploy schema summary" /></p>

<p>The repository code was hosted on Github and CircleCI was configured to trigger a build on each commit.</p>

<p>As development flow I've used <a href="http://nvie.com/posts/a-successful-git-branching-model/">GitFlow</a> and each commit to 
<code>master</code> was triggering a deploy to live, each commit to <code>develop</code> was triggering a deploy to staging
 and each commit to feature branches was triggering a deploy to a test environment.
Commit to release branches were triggering a deploy to a pre-live environment.</p>

<p>The deploy to live had a manual confirmation step.</p>

<h2 id="development-environment">Development environment</h2>

<p>Obviously everything starts from the development environment.</p>

<p>As development environment I've used <a href="https://docs.docker.com/">Docker</a> and <a href="https://docs.docker.com/compose/">Docker Compose</a>.</p>

<p>A lot of emphasis was used to ensure a <strong>stateless</strong> approach in each component.
From the docker point of view this meant that was not possible to use shared folders between container and the host 
to store the state of the application (sessions, logs, ...).</p>

<p>Not storing data on the host is fundamental to be able to deploy seamlessly the application to live.
Logs, sessions, uploads, userdata have to be stored in specialized services as log management tools, key value storages, 
object storages (as AWS S3 or similar).</p>

<h3 id="docker-compose-file">Docker compose file</h3>

<p>Docker compose is a tool that allows you to configure and coordinate containers.</p>

<pre><code class="yaml"># docker-compose.yml
version: '3.3'
services:
    db:
      image: goetas/api-pgsql:dev
      build: docker/pgsql
      ports:
          - "5432:5432"
      volumes:
          - db_data:/var/lib/postgresql/data/pgdata
      environment:
          PGDATA: /var/lib/postgresql/data/pgdata
          POSTGRES_PASSWORD: api
    php:
        image: goetas/api-php:dev
        build:
          context: .
          dockerfile: docker/php-fpm/Dockerfile
        volumes:
          - .:/var/www/app

    www:
        image: goetas/api-nginx:dev
        build:
          context: .
          dockerfile: docker/nginx/Dockerfile
        ports:
            - "7080:80"
        volumes:
            - .:/var/www/app

volumes:
  db_data: {}
</code></pre>

<p>From this <code>docker-compose.yml</code> we can see that we have PHP, Nginx and a PostgreSQL database.</p>

<p>Please pay attention to the compose-version <code>3.3</code>, means that we are can run this compose file as a 
<a href="https://docs.docker.com/engine/reference/commandline/stack/">docker stack</a> and deploy it on simple docker hosts 
of on <a href="https://docs.docker.com/engine/swarm/">docker swarm</a> clusters.</p>

<p>To note also the <code>build</code> section of the <code>php</code> and <code>www</code> services. The images are built from the specified <code>dockerfile</code>, 
but the <code>context</code> of build (the working directory) is <code>.</code>.
This allows to build the docker image containing already the source code of our application 
but keeping a nice folder structure.</p>

<p>The <code>db</code> container is used only for development and test environments
 (where performance and high availability are not necessary), for live was used an Amazon RDS instance.</p>

<h2 id="docker-files">Docker files</h2>

<p>From the previous <code>docker-compose.yml</code> file we can see that we are using "custom" images for each service.</p>

<p>Let's have a look at them.</p>

<h3 id="db-postgres">DB (postgres)</h3>

<p><code>docker/pgsql/Dockerfile</code></p>

<pre><code class="dockerfile">FROM postgres:9.6

COPY bin/init-user-db.sh /docker-entrypoint-initdb.d/init-user-db.sh
</code></pre>

<p><code>docker/pgsql/bin/init-user-db.sh</code></p>

<pre><code class="bash">#!/bin/bash
set -e

psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" &lt;&lt;-EOSQL
    CREATE USER mydbuser PASSWORD 'XXXXX';
    CREATE DATABASE mydb;
    GRANT ALL PRIVILEGES ON DATABASE mydb TO mydbuser;
EOSQL
</code></pre>

<p>This docker file is trivial, just references the official <code>postgres:9.6</code> image and creates a database when
spinning up the container.</p>

<p>Data are persisted to the host machine in a docker volume as specified in the <code>volumes</code> section 
on the <code>docker-compose.yml</code> file.</p>

<p><strong class="less">EDIT (2017-09-24)</strong>: Thanks to <a href="https://github.com/mleczakm">Michał Mleczko</a>, I've found out that the custom build script
is not really necessary since the default postgres images allows to configure the default database creation via 
environment variables. To have postgres running is sufficient to do something as:</p>

<pre><code class="yaml">db:
  image: postgres:9.6
  ports:
      - "5432:5432"
  volumes:
      - db_data:/var/lib/postgresql/data/pgdata
  environment:
      PGDATA: /var/lib/postgresql/data/pgdata  
      POSTGRES_USER: mydb # postgres user
      POSTGRES_DB: mydb # postgres db name
      POSTGRES_PASSWORD: XXX # postgres password  
</code></pre>

<h3 id="php">PHP</h3>

<p>This is the <code>docker/php-fpm/Dockerfile</code> file to build the PHP image.</p>

<pre><code class="dockerfile"># docker/php-fpm/Dockerfile 
FROM php:7.1-fpm
RUN usermod -u 1000 www-data
RUN groupmod -g 1000 www-data

# install system basics
RUN apt-get update -qq &amp;&amp; apt-get install -y -qq \
        libfreetype6-dev \
        libjpeg62-turbo-dev \
        libmcrypt-dev \
        libpng12-dev git \
        zlib1g-dev libicu-dev g++ \
        wget libpq-dev

# install some php extensions
RUN docker-php-ext-install -j$(nproc) iconv mcrypt bcmath intl zip opcache pdo_pgsql\
    &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \
    &amp;&amp; docker-php-ext-install -j$(nproc) gd \
    &amp;&amp; docker-php-ext-enable opcache

# install composer
RUN curl -s https://getcomposer.org/composer.phar &gt; /usr/local/bin/composer \
    &amp;&amp; chmod a+x /usr/local/bin/composer

# copy some custom configurations for PHP and FPM
COPY docker/php-fpm/ini/app.ini /usr/local/etc/php/conf.d/app.ini
COPY docker/php-fpm/conf/www.pool.conf /usr/local/etc/php-fpm.dwww.pool.conf

WORKDIR /var/www/app

# copy the source code
COPY composer.* ./
COPY app app
COPY bin bin
COPY src src
COPY web/index.php web/index.php
COPY var var
COPY vendor vendor

# set up correct permissions to run the next composer commands 
RUN if [ -e vendor/composer ]; then chown -R www-data.www-data vendor/composer vendor/*.php app/config web var; fi

# generate the autoloaders and run composer scripts
USER www-data
RUN if [ -e vendor/composer ]; then composer dump-autoload --optimize --no-dev --no-interaction \
    &amp;&amp; APP_ENV=prod composer run-script post-install-cmd --no-interaction; fi
</code></pre>

<p>This is the PHP image, not much to say except the permission issue that was necessary since the <code>COPY</code> operation is
always executed as <code>root</code>. Executing the <code>chown</code> command was necessary for the folders that will be touched by the 
<code>composer dump-autoload</code> and <code>composer run-script post-install-cmd</code>.</p>

<h3 id="www-nginx">WWW (nginx)</h3>

<pre><code class="dockerfile"># docker/nginx/Dockerfile
FROM nginx:stable

# copy some custom configurations
COPY docker/nginx/conf /conf

# copy the source code
COPY web /var/www/app/web
</code></pre>

<p>The web server image is also trivial, it just copies some configurations (the virtual host setup mainly) and 
copies the code available in the <code>web</code> directory (the only one exposed to the server by symfony recommendations).</p>

<h2 id="running">Running</h2>

<p>By typing in your shell...</p>

<pre><code class="bash">docker-compose up -d
</code></pre>

<p>the development environment is up and ready!</p>

<h2 id="tips">Tips</h2>

<p>Here are two tips on how manage some aspects of your application</p>

<h3 id="environment-variables">Environment variables</h3>

<p>The exposed setup is sufficient but to create a more flexible image, for me was fundamental to allow to use
<strong>environment variables in configuration files</strong>.</p>

<p>To do so I've used a helper script to replace environment variables from a file with their values.</p>

<p>The following file was copied on each image built via Dockerfile to <code>/conf/substitute-env-vars.sh</code>.</p>

<pre><code>#!/usr/bin/perl -p
s/\$\{([^:}]+)(:([^}]+))?\}/defined $ENV{$1} ? $ENV{$1} :  (defined $3 ? "$3": "\${$1}")/eg
</code></pre>

<p>All configuration files were copied always to <code>/conf</code>.</p>

<p>Later the docker-<code>ENTRYPOINT</code> was responsible to replace the environment variables from a file with their runtime values
 and to place the files the right location.</p>

<p>This is a portion of the entrypoint file:</p>

<pre><code class="bash">#!/bin/bash
set -e
/conf/substitute-env-vars.sh &lt; /conf/app.ini  &gt; /usr/local/etc/php/conf.d/app.ini
/conf/substitute-env-vars.sh &lt; /conf/nginx.conf &gt; /etc/nginx/conf.d/www.conf

## here the rest of your entrypoint ...

</code></pre>

<p>With this script we are able to use environment variables in each configuration file. 
Was possible to specify different host names for the nginx containers without rebuilding the images
but by just setting up appropriate environment variables.</p>

<p>Let's consider the following nginx configuration file:</p>

<pre><code>server {
    server_name ${APP_SERVER_NAME:project.dev};
    root ${APP_SERVER_ROOT:/var/www/app/web};
}
</code></pre>

<p><code>$APP_SERVER_NAME</code> and <code>$APP_SERVER_ROOT</code> are environment variables and when set to <code>www.example.com</code> and <code>/home/me/www</code>
the resulting file will be:</p>

<pre><code>server {
    server_name www.example.com;
    root /home/me/www;
}
</code></pre>

<p>If some variables are not available, they will be replaced by its default value 
(<code>project.dev</code> and <code>/var/www/app/web</code> in this case).
If the variable is not defined and no default value is specified, the result will be an empty string.</p>

<h3 id="logs">Logs</h3>

<p>By default, Symfony stores logs to file, using docker, in some situations can be not the best approach.</p>

<p>I've configured monolog to output logs directly to the "standard error" stream. 
This later will allow to configure docker and 
<a href="https://docs.docker.com/engine/admin/logging/overview/">docker log drivers</a> to store logs via
 one of the supported drivers (json-file, syslog, gelf, awslogs, splunk... and many other).</p>

<p>Here the <code>config_prod.yml</code> snippet.</p>

<pre><code class="yaml">monolog:
    handlers:
        main:
            type:         fingers_crossed
            action_level: error
            handler:      nested
            buffer_size:  5000 # log up to 5000 items
            excluded_404s: # do not consider 404 as errors
                - ^.*
        nested:
            type:  stream
            path:  "php://stderr"
            level: debug
</code></pre>

<p>Note the <code>buffer_size</code> option, if you have long running scripts (cli daemons), 
that option is fundamental to avoid memory leaks.</p>

<p>Symfony by default will store logs forever waiting for an error critical enough to flush the log buffer, 
but a "good" should not have errors, so the buffer will never be flushed.
With that option symfony will keep only the last 5000 log items.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post I've shared my local environment. There are many things that can be improved, many of them were fundamental
for my project but for brevity I could not include them in this post.</p>

<p>Some improvements that can be done:</p>

<ul>
<li>Reducing image size by deleting temporary files after the installation of libraries is done</li>
<li>Removing composer</li>
<li>?</li>
</ul>

<p>In the next posts will see how to move the local development environments thought the build pipeline 
to the live environment.</p>

<p>Did I miss something? Feedback are welcome.</p>]]></content>
        </entry>        <entry>
            <title>How to add custom error codes to your Symfony API responses</title>
            <link href="https://www.goetas.com/blog/how-to-add-custom-error-codes-to-your-symfony-api-responses"/>
            <updated>2017-09-12T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/how-to-add-custom-error-codes-to-your-symfony-api-responses</id>                <content type="html"><![CDATA[<p>When writing APIs, a proper error handling is fundamental.</p>

<p>HTTP status codes are a great start, but often when we deal with user inputs is not enough. 
If out model has complex validation rules, understanding the reason behind an <code>400 Bad Request</code> error can be not trivial.</p>

<p>Fortunately when for symfony developers there are many libraries to deal with it. 
<a href="https://github.com/symfony/validator">Symfony Validator</a>,
<a href="https://github.com/symfony/form">Symfony Form</a>,
<a href="https://github.com/FriendsOfSymfony/FOSRestBundle">FOS REST Bundle</a> and 
<a href="https://github.com/schmittjoh/serializer">JMS Serializer</a> combined allows you to have nice error messages 
to be shown to your users.</p>

<h4 id="the-entity">The entity</h4>

<p>This is out model and in this case is petty trivial with only one property called "name".
By using the symfony validator annotations we define that the name can not be blank.</p>

<pre><code class="php">namespace AppBundle\Entity;

use Symfony\Component\Validator\Constraints as Assert;

class Author
{
    /**
     * @Assert\NotBlank()
     */
    public $name;
}

</code></pre>

<h4 id="the-form">The form</h4>

<p>This is the symfony form type to handle the author createing.</p>

<pre><code class="php">namespace AppBundle\Form;

use Symfony\Component\Form\AbstractType;
use Symfony\Component\Form\FormBuilderInterface;
use Symfony\Component\OptionsResolver\OptionsResolver;

class AuthorType extends AbstractType
{
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        $builder-&gt;add('name');
    }

    public function configureOptions(OptionsResolver $resolver)
    {
        $resolver-&gt;setDefault('data_class', Author::class);
    }
}

</code></pre>

<h4 id="the-controller">The controller</h4>

<p>This is the controller that acts as a glue of our components.</p>

<pre><code class="php">namespace AppBundle\Controller;

use AppBundle\Entity\Author;
use AppBundle\From\AuthorType;
use FOS\RestBundle\View\View;
use Symfony\Component\HttpFoundation\Request;
use Symfony\Component\HttpFoundation\Response;

class AuthorController
{
    public function create(Request $request)
    {
        $author = new Author(); 
        $form = $this-&gt;createForm(AuthorType::class, $author);
        $form-&gt;handleRequest($request);

        if ($form-&gt;isValid()) {

            // do something here

            return new View($author, Response::HTTP_CREATED);
        } else {
            return new View($form, Response::HTTP_BAD_REQUEST);
        }
    }
}

</code></pre>

<h4 id="the-result">The result</h4>

<p>The result of a PUT call to the <code>create</code> action, if the data are not correct will be:</p>

<pre><code>HTTP/1.1 400 Bad Request
Content-Type: application/json

{
  "code": 400,
  "message": "Validation Failed",
  "errors": {
    "children": {
      "name": {
          "errors": [
            "This value shoul not be blank."
          ],
      }
    }
  }
}
</code></pre>

<p>Here the error message can be displayed to users, but if the API has to be consumed by some other software 
(as example a mobile app), the error message can be not sufficient anymore. 
Having some "error code" that can be hardcoded in some reliable way in the app code, will allow us 
to provide a better user experience (offering some nice popup as example).</p>

<p>To do so is pretty easy, we will have to add just a custom error handler into the JMS serializer serialization process.
The custom error handler will use the the symfony validator <code>payload</code> feature to add machine readable error codes.</p>

<h3 id="custom-error-codes">Custom error codes</h3>

<h4 id="the-entity">The entity</h4>

<p>In the original entity we just specify the <code>payload</code> property with our error code.</p>

<p>(The provided error handler accepts a string or an array of strings as error code)</p>

<pre><code class="php">namespace AppBundle\Entity;

use Symfony\Component\Validator\Constraints as Assert;

class Author
{
    /**
     * @Assert\NotBlank(payload={"error_code"="INVALID_NAME"})
     */
    public $name;
}

</code></pre>

<h4 id="the-error-handler">The error handler</h4>

<p>This is the longest class in this tutorial but its logic is pretty simple and is mainly
 a copy/paste from the original error handler.</p>

<pre><code class="php">namespace AppBundle\Serializer;

use JMS\Serializer\Context;
use JMS\Serializer\GraphNavigator;
use JMS\Serializer\Handler\SubscribingHandlerInterface;
use JMS\Serializer\JsonSerializationVisitor;
use JMS\Serializer\VisitorInterface;
use Symfony\Component\Form\Form;
use Symfony\Component\Form\FormError;
use Symfony\Component\Translation\TranslatorInterface;
use Symfony\Component\Validator\ConstraintViolation;

class FormErrorHandler implements SubscribingHandlerInterface
{
    private $translator;

    public static function getSubscribingMethods()
    {
        $methods = array();
        $methods[] = array(
            'direction' =&gt; GraphNavigator::DIRECTION_SERIALIZATION,
            'type' =&gt; Form::class,
            'format' =&gt; 'json',
        );

        return $methods;
    }

    public function __construct(TranslatorInterface $translator)
    {
        $this-&gt;translator = $translator;
    }

    public function serializeFormToJson(JsonSerializationVisitor $visitor, Form $form, array $type, Context $context = null)
    {
        $isRoot = null === $visitor-&gt;getRoot();
        $result = $this-&gt;adaptFormArray($this-&gt;convertFormToArray($visitor, $form), $context);

        if ($isRoot) {
            $visitor-&gt;setRoot($result);
        }

        return $result;

    }

    private function getErrorMessage(FormError $error)
    {
        if (null !== $error-&gt;getMessagePluralization()) {
            return $this-&gt;translator-&gt;transChoice($error-&gt;getMessageTemplate(), $error-&gt;getMessagePluralization(), $error-&gt;getMessageParameters(), 'validators');
        }

        return $this-&gt;translator-&gt;trans($error-&gt;getMessageTemplate(), $error-&gt;getMessageParameters(), 'validators');
    }

    private function getErrorPayloads(FormError $error)
    {
        /**
         * @var $cause ConstraintViolation
         */
        $cause = $error-&gt;getCause();

        if (!($cause instanceof ConstraintViolation) || !$cause-&gt;getConstraint() || empty($cause-&gt;getConstraint()-&gt;payload['error_code'])) {
            return null;
        }

        return $cause-&gt;getConstraint()-&gt;payload['error_code'];
    }

    private function convertFormToArray(VisitorInterface $visitor, Form $data)
    {
        $isRoot = null === $visitor-&gt;getRoot();

        $form = new \ArrayObject();
        $errorCodes = array();
        $errors = array();

        foreach ($data-&gt;getErrors() as $error) {
            $errors[] = $this-&gt;getErrorMessage($error);
        }

        foreach ($data-&gt;getErrors() as $error) {
            $errorCode = $this-&gt;getErrorPayloads($error);
            if (is_array($errorCode)) {
                $errorCodes = array_merge($errorCodes, array_values($errorCode));
            } elseif ($errorCode !== null) {
                $errorCodes[] = $errorCode;
            }
        }

        if ($errors) {
            $form['errors'] = $errors;
            if ($errorCodes) {
                $form['error_codes'] = array_unique($errorCodes);
            }
        }

        $children = array();
        foreach ($data-&gt;all() as $child) {
            if ($child instanceof Form) {
                $children[$child-&gt;getName()] = $this-&gt;convertFormToArray($visitor, $child);
            }
        }

        if ($children) {
            $form['children'] = $children;
        }

        if ($isRoot) {
            $visitor-&gt;setRoot($form);
        }

        return $form;
    }


    private function adaptFormArray(\ArrayObject $serializedForm, Context $context = null)
    {
        $statusCode = $this-&gt;getStatusCode($context);
        if (null !== $statusCode) {
            return [
                'code' =&gt; $statusCode,
                'message' =&gt; 'Validation Failed',
                'errors' =&gt; $serializedForm,
            ];
        }

        return $serializedForm;
    }

    private function getStatusCode(Context $context = null)
    {
        if (null === $context) {
            return;
        }

        $statusCode = $context-&gt;attributes-&gt;get('status_code');
        if ($statusCode-&gt;isDefined()) {
            return $statusCode-&gt;get();
        }
    }
}
</code></pre>

<h4 id="registering-the-error-handler">Registering the error handler</h4>

<p>Add this snippet to your <code>services.xml</code> to register the custom error handler to your application.</p>

<pre><code class="xml">&lt;service class="AppBundle\Serializer\FormErrorHandler" id="AppBundle\Serializer\FormErrorHandler"&gt;
    &lt;argument id="translator" type="service"/&gt;
    &lt;tag name="jms_serializer.subscribing_handler"/&gt;
&lt;/service&gt;

</code></pre>

<h4 id="the-result">The result</h4>

<p>That's all, ehe result of a PUT call to the <code>create</code> action now will contain our custom error codes.</p>

<pre><code>HTTP/1.1 400 Bad Request
Content-Type: application/json

{
  "code": 400,
  "message": "Validation Failed",
  "errors": {
    "children": {
      "name": {
          "errors": [
            "This value should not be blank."
          ],
          "error_codes": [
            "INVALID_NAME"
          ],
      }
    }
  }
}
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>In this short tutorial we saw how is easy to add custom error codes to your API.</p>

<p>Of course, using the same strategy is possible to add many more information as "error_severity" or whatever you prefer.</p>]]></content>
        </entry>        <entry>
            <title>Deserialization, normalization, validation and the JMS Serializer</title>
            <link href="https://www.goetas.com/blog/deserialization-normalization-validation-and-the-jms-serializer"/>
            <updated>2017-08-21T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/deserialization-normalization-validation-and-the-jms-serializer</id>                <summary>Serializing can be a not trivial process, but when talking about de-serialization, the problem is even more difficult. The source data (XML or JSON) can be not well formatted, well formatted but not valid or valid syntactically but not valid in our domain. Dividing deserialization, normalization, validation is fundamental.</summary>                <content type="html"><![CDATA[<p>Recently, while speaking at the <a href="/project/berlin-symfony-user-group-boring-api-and-symfony-2017/">Symfony Berlin User Group</a>, 
during the Q&amp;A session, I was asked to comment on how to solve an issue when using 
<a href="https://github.com/FriendsOfSymfony/FOSRestBundle">FOS REST Bundle</a> and <a href="https://github.com/schmittjoh/serializer">JMS Serializer</a>.</p>

<p>The JMS Serializer is a library that allows to serialize and deserialize
and object graph into a JSON or XML representation and vice versa and in the last two years I'm doing my best as maintainer of it.</p>

<p>Serializing can be a not trivial process, but when talking about de-serialization, the problem is even more difficult.
The source data (XML or JSON) can be not well formatted, 
well formatted but not valid or valid syntactically but not valid in our domain.</p>

<p>An example of this situation can be:</p>

<pre><code>// blog post article
{
    "author_id": 60, 
    "price": "123.540,50",
    "date": "2010-02-05 10:20:60+02:00",
    "text": "hello world"
}
</code></pre>

<p>Here we can ask our self:</p>

<ul>
<li>is <code>author_id = 60</code> a valid author id?

<ul>
<li>maybe is <code>"AUTHOR60"</code> instead of <code>60</code></li>
<li>maybe is <code>59</code> instead of <code>60</code></li>
</ul></li>
<li>what is the value of <code>price</code>?</li>
<li>is the <code>date</code> "5th of February" or "2nd of May"?</li>
<li>is <code>text = "hello world"</code> a valid text?

<ul>
<li>maybe is too short?</li>
<li>maybe is too long?</li>
<li>maybe should be a valid HTML document?</li>
</ul></li>
</ul>

<p>This and many other question may arise from few JSON lines.</p>

<p>What we are talking about here is not de-serialization but we are talking about <strong>normalization</strong> and <strong>validation</strong>.</p>

<h2 id="normalization">Normalization</h2>

<p>Normalization is the process of translating the data from one representation to a different one 
(eventually more convenient for some reason).</p>

<p>As example:</p>

<p><code>12.4872,50</code>,  <code>12 4872.50</code>, <code>124872,50</code> and <code>124872.50</code> may represent the same number, just expressed using different 
localization standards.</p>

<p>Normalizing the previously said numbers means converting all of them into a single unified format that allows us to work on them.
Can be a <code>float</code> number or a <code>Money</code> value object or an <code>integer</code> (multiplied by 100 in this case) 
or any other representation fits well our domain model.</p>

<p>Note: normalization <strong>is about converting/translating data</strong>.</p>

<h2 id="validation">Validation</h2>

<p>Validation is the process of asserting if a specified information is valid for a given context.</p>

<p>We can validate data either before or after the normalization phase.</p>

<p><strong>Post Normalization validation</strong></p>

<p>As example, given <code>124872.50</code> normalized as <code>(float)</code>.</p>

<p>If interpreted as meters, this value might be valid if we are talking about distances, 
but most probably is not valid if we are talking about human height.</p>

<p>Another example can be the text <code>hello world</code> normalized as <code>(string)</code>, can be valid as "english language string" 
but most probably is not valid as "blog post" where we might require at least 300 words.</p>

<p>In the post-normalization phase, the validity of the data depends on the application domain/context.</p>

<p><strong>Pre Normalization validation</strong></p>

<p>As example, given the <code>12.4872.50</code> information.</p>

<p>If we want to normalize it to a <code>float</code> it is really difficult to decide which number it should represent.</p>

<p>Is it <code>124872.50(float)</code> or <code>12.487250(float)</code> or is just invalid? 
If we had <code>12 4872.50</code>, representing it as <code>float</code> is a bit easier, 
but again, is it <code>12.0(float)</code> or<code>124872.50(float)</code> or is just invalid?</p>

<p>In the case of pre-normalization, the validation process overlaps with what we are able to normalize in a reliable way
(by reliability I mean the level of error I want to allow in my application).</p>

<ul>
<li>In applications where the data should be produced by other software is a good practice to have really strict validation 
and reduce the normalization to the minimum possible 
by being explicit to the maximum extend by which data representations are allowed.</li>
<li>In applications where data are produced by humans the situation is more complex and it depends 
by how much frustration we want to inflict to the user when providing the data. 
As example in an e-commerce shop can be a good idea to "help" a bit the user by normalizing more 
compared to an application to fill a tax declaration form.</li>
</ul>

<p>Note: validation <strong>does not perform any data conversion</strong>.</p>

<h2 id="de-serialization">De-Serialization</h2>

<p>De-serialization is the process of translating a representation that can be stored or transmitted 
into an <a href="https://en.wikipedia.org/wiki/Object_(computer_science)">object</a> state.</p>

<p>Deserialization does not do explicitly validation or normalization, 
but in order to create the object state the "deserialization engine"
 can do implicitly some "pre-normalization validation" and normalization.</p>

<p>The deserializer will not do "post-normalization validation", 
this mean that your object graph might be in an invalid state after the deserialization process.</p>

<h3 id="jms-serializer">JMS Serializer</h3>

<p>This post is mainly about PHP and more in detail about the JMS Serializer and its deserialization capabilities.</p>

<p>In the past different users have tried to enrich the JMS serializer ability to validate objects, 
but mixing the deserialization, validation and normalization is a risky idea. 
Is risky because can end up with a library not able to do well none of the three.
Currently there are great alternatives when talking about validation or normalization.</p>

<h4 id="post-normalization-validation">Post-normalization validation</h4>

<p>If you want to ensure a valid object state at the end of the deserialization process, a great solution is the 
 <a href="https://github.com/symfony/validator">Symfony Validator</a></p>

<pre><code>$myObject = $serializer-&gt;deserialize('some json data here', 'MyObject', 'json');


$errors = $validator-&gt;validate($myObject);

if (count($errors)&gt;0) {
   // do something
} else {
   // do something else :)
}

</code></pre>

<p>The valid object state can be described using XML, YAML and annotations when working with the Symfony Validator.</p>

<h4 id="pre-normalization-validation-and-normalization">Pre-normalization validation and normalization</h4>

<p>At the moment there is not out there a solution to do proper normalization and pre-normalization validation of the data
using the JMS Serializer, but luckily there is another great solution, 
the <a href="https://github.com/symfony/form">Symfony Form</a> component.</p>

<p>The Symfony form component allows you a much more granular process of "deserializing" some data into an object graph.</p>

<pre><code>// just setup a fresh MyObject object (remove the dummy data)
$form = $this-&gt;createFormBuilder(new MyObject())
        -&gt;add('task', TextType::class)
        -&gt;add('dueDate', DateType::class)
        -&gt;getForm();

$form-&gt;submit(json_decode('some json data here', true));


if ($form-&gt;isValid()) {

    $myObject = $form-&gt;getData();

   // do something
} else {
   // do something else :)
}

</code></pre>

<p>The data types <code>DateType</code> and <code>TextType</code> (and many others built in into symfony) have plenty of useful options 
and configurations that allows you to customize the de-serialization process. The form component can also set to NULL 
all the fields before starting the "conversion" process or can do just simple update to your object state 
(something that is not possible with the JMS serializer).</p>

<p>Compared to the JMS serializer, the symfony form deserialization process is much more complex
and is not trivial to configure but it will do pre-normalization validation and normalization for you.</p>

<h2 id="conclusion">Conclusion</h2>

<p>With this post I'm not saying "stop using the JMS Serializer" for deserialization, what I'm saying is 
"<strong>use the jms de-serializer in specific situations</strong>" where the validation and normalization are not fundamental.</p>

<p>My personal <strong>use-case</strong> for the JMS de-serializer is to exchange data between "Queue-workers". 
 Data are produced by a php application (using the JMS serializer) and consumed by various PHP workers 
(data deserialized using the JMS de-serializer).
 In my case the data are really simple and produced by reliable sources. 
 Because of that validation and normalization and validation are almost not necessary.
 (Some post-normalization validation is performed at application level to ensure the data received are valid, but not much)</p>

<p>My personal <strong>NOT-use-case</strong> for the JMS de-serializer is the handling of POST/PUT/PATCH requests in a REST webservice.
 The data that a public REST webservice should handle are often complex, especially if it is part of an API consumed 
 by some user-facing frontend application.
 The API has to validate and normalize the user input as emails, date of birth, bank transaction amounts and so on...</p>

<p>What do you think? Is out there a library that handles properly all the three parts?</p>

<p>Looking forward to hear your feedback!</p>]]></content>
        </entry>        <entry>
            <title>The way to the goetas-webservices/client release</title>
            <link href="https://www.goetas.com/blog/the-way-to-the-goetas-webservicesclient-release"/>
            <updated>2016-10-04T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/the-way-to-the-goetas-webservicesclient-release</id>                <summary>After many years of work, finally I have released a fully PHP based SOAP client up to date with the latest best practice.</summary>                <content type="html"><![CDATA[<p>I started to work on this project (let's say technology) almost 10 years ago 
while was working at <a href="http://www.mercuriosistemi.com/">Mercurio Sistemi</a>...</p>

<p>Everything started with a customer that had an already existing <a href="https://en.wikipedia.org/wiki/SOAP">SOAP webservice</a>, 
this webservice was necessary to integrate their existing Visual Basic (.NET ? ) application stack with a new PHP website.
Fortunately PHP had already "ext-soap" that allows to communicate with it and the task was pretty easy.</p>

<p>This is how I was introduced to SOAP...</p>

<p>After some time and many completed integrations, client and server side, I was looking enviously to some .NET and Java 
developers, and how easy, "safe", was working for them with SOAP webservices.</p>

<p>This was mainly because of some commodities their programming language and IDE was providing them.
Auto completion, static checks, code generation were just great, and their job essentially was just about choosing the 
right variable name or method call from a list provided by their IDE.
Of course if something was wrong, their IDE or some software tool will warn them, telling: "hei! here something is wrong
fix it, otherwise yor webservice will not work well".</p>

<p>That was just great, errors were discovered even before saving the file containing the code, and often errors were 
auto fix-able by this set of tools.
PHP on the other hand was a jungle... tons of arrays, stdClass instances, caches to be refreshed, and even more 
variables, properties that needs to be guessed by the developer and so on.</p>

<p>I tried many libraries but not a single one was close to the .NET webservices implementation or Java Axis and similar.</p>

<p>Things got even worse when I had to integrate in our system an <a href="https://en.wikipedia.org/wiki/OpenTravel_Alliance">OTA</a> 
company. OTA is a set of specifications built on top of XML on how to communicate between travel-focused companies as flights companies, 
hotel chains, car rentals and so on.
OTA is a great set of standards but for the PHP soap ecosystem, is just crazy complex.</p>

<p>I started blinding some scripts to generate PHP classes from the <a href="https://en.wikipedia.org/wiki/XML_schema">XML Schema</a>
definitions provided by the Open Travel Alliance, to have some hints by my IDE, in order to increase productivity, and
reduce bugs.</p>

<p>To me was looking the right way... in the mean while I have released <a href="https://github.com/goetas/xsd2php">goetas/xsd2php</a>
(used to generate PHP classes) and a set of other libraries as <a href="https://github.com/goetas/webservices">goetas/webservices</a>
that essentially was my final goal, a fully PHP based SOAP client and server.</p>

<p>But as every developer should do, for me the result was not good enough and I continued working on it.
Was not good because there were still so many things not working well, complex xml parsing done on each webservice call, 
no tests, some edge cases were just not working, and the whole result was looking really unstable.</p>

<p>So I decided to restart from scratch (almost from scratch).</p>

<p><a href="https://github.com/schmittjoh/serializer">jms/serializer</a> was mature enough to do the XML serialization/deserialization
 even for some really complex cases, PSR-7 was out and great HTTP clients were available.</p>

<p>This put on me even more motivation and the current result is good enough, of course there is plenty of work that still 
has to be done! Multiple things needs to be added, improved, fixed, and so on, but I decided that was the time 
to release <a href="https://github.com/goetas-webservices/soap-client">goetas-webservices/soap-client</a>!</p>

<p>The project is released under a <a href="https://en.wikipedia.org/wiki/MIT_License">MIT</a> license 
and is freely available on <a href="https://github.com/goetas-webservices/soap-client">GitHub</a>.</p>

<p>Some of the strengths of the project are:</p>

<ul>
<li>Pure PHP, no dependencies on ext-soap</li>
<li>Extensible (JMS event listeners support)</li>
<li>PSR-7 HTTP messaging compatible</li>
<li>Multi HTTP client (guzzle, buzz, curl, react)</li>
<li>No WSDL/XSD parsing on production</li>
<li>IDE type hinting support</li>
</ul>

<p>So, if you need a SOAP client for your project, give it a try!</p>

<p>In the mean while I have in plan to release soon also a fully PHP SOAP server. 
You can <a href="https://twitter.com/goetas_asmir">follow me</a>, if you are interested to know more about it.</p>

<p>A note that has to be said about this is that, in the last years 
<a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a>
arrived and changed completely the way on how webservices are built, somehow deprecating SOAP. 
I see also on the way also <a href="https://en.wikipedia.org/wiki/GraphQL">GraphQL</a> trying to deprecate REST too.</p>

<p>So, was it worth working on this project? Definitely yes! I have learned a lot, and I think there are still many 
developers struggling with legacy applications every day as I did.</p>

<p>If somebody has some feedback, please just drop me a line at <a href="&#109;&#x61;&#105;&#x6c;&#116;&#x6f;&#58;&#x67;&#111;&#x65;&#116;&#x61;&#115;&#x40;&#103;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#109;">goetas@gmail.com</a> 
or tweet me at <a href="https://twitter.com/goetas_asmir">@goetas</a></p>]]></content>
        </entry>        <entry>
            <title>Some notes about JMS Serializer (plus v1.3.0 release)</title>
            <link href="https://www.goetas.com/blog/some-notes-about-jms-serializer-plus-v130-release"/>
            <updated>2016-08-21T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/some-notes-about-jms-serializer-plus-v130-release</id>                <summary>It is almost a month that I started as maintainer of JMS Serializer and today I&#039;m proud to announce the release v1.3.0 of JMS Serializer that contains many bug fixes and the new method &quot;hasData&quot; for generic serialization visitors (JSON/YAML)</summary>                <content type="html"><![CDATA[<p>It is almost a month that I started as maintainer of <a href="https://github.com/schmittjoh/serializer">JMS Serializer</a> 
(<a href="http://jmsyst.com/libs/serializer">here</a> is available the official documentation).
I started to contribute to this project almost two years ago, with some small pull requests
necessary for <a href="https://github.com/goetas/xsd2php">xsd2php</a> ( a tool to convert XSD XMLSchema definitions into PHP classes and instances).</p>

<p>JMS Serializer allows to serialize/deserialize into/from JSON/XML/YAML. 
The main advantage I see in this project, compared to other similar tools, is that uses "declarative" approach.
 A declarative serialization requires often no special code to handle the serialization/deserialization process, 
and this reduces the number of possible bugs.
Configurations can be defined as XML files, YAML files or Annotations.</p>

<p>In the last year (or two) it appeared that JMS Serializer was not really well maintained,
the feedback loop was really long (some PR took 3 years to be merged or closed),
the PR count was about 200 and the issue count was more than 300.
The first thing I did was going trough the long list of issues and pull requests, 
closing what was not valid anymore (some were 3 years old), finding or closing duplicates, 
asking for feedback and merging what was good to merge.</p>

<p>This was not an easy process, often PR were really old, but some authors 
were really kind, rebasing or explaining really "ancient" issues they were solving.
Of course in some cases, authors express their disappointment or confusion seeing their PRs 
being closed because now outdated.
But this was to me, a know consequence of my work, so I continued to do my job.</p>

<p>Now after almost 30 days, we had two releases, with more than merged pull requests.
The new PR count is around 60, now the number of open issues decreased to ~200
 and I have started using ghthub milestones to track the release process 
 and what to expect for the next releases.
Was also really cool to see some appreciation from authors (contributors via PR to the project)
for the new "life" activity of the project.</p>

<p>The JMS Serializer activity is pretty high, 
 nowdays it has more than 13000 downloads via composer every day.
To me, this is a clear signal of the strategic importance of this project for many people and companies, 
and this makes me proud and motivated to continue improving it.</p>

<p>If somebody has some feedback, or drop me a line at <a href="&#109;&#x61;&#105;&#x6c;&#116;&#x6f;&#58;&#x67;&#111;&#x65;&#116;&#x61;&#115;&#x40;&#103;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#109;">goetas@gmail.com</a> 
or tweet me at <a href="https://twitter.com/goetas_asmir">@goetas</a></p>]]></content>
        </entry>        <entry>
            <title>Available as Software Architect and Consultant</title>
            <link href="https://www.goetas.com/blog/available-as-software-architect-and-consultant"/>
            <updated>2016-08-10T00:00:00+02:00</updated>
            <id>https://www.goetas.com/blog/available-as-software-architect-and-consultant</id>                <summary>Starting from September 2016 I&#039;m available as Software Architect and Consultant. Mainly PHP applications, infrastructure setup, deployment process and workflow management.</summary>                <content type="html"><![CDATA[<p>After spending almost 15 years in the IT industry, mainly as PHP developer, 
I have decided to start a new and exciting activity as Independent Consultant and Software Architect.</p>

<p>If you'd like to have an idea of what kind of projects I'm running, 
please visit my <a href="/services">services</a> page.</p>

<p>To know more about my background, skills and experience, 
I invite you to have a look at my 
<a href="https://linkedin.com/in/goetas" rel="me" title="Asmir Mustafic LinkedIn profile" target="_blank">LinkedIn</a> profile.</p>]]></content>
        </entry>        <entry>
            <title>Redovisningsdatum</title>
            <link href="https://www.goetas.com/projects/redovisningsdatum"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/redovisningsdatum</id>                <summary>A web platform that helps to keep track of the most important reports to file for your company
</summary>                <content type="html"><![CDATA[<p>Redovisningsdatum is a digital service that helps business owners and accounting firms in Sweden
 to be reminded of when financial reports are to be reported to Swedish authorities and other public actors</p>]]></content>
        </entry>        <entry>
            <title>PINT</title>
            <link href="https://www.goetas.com/projects/pint"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/pint</id>                <summary>PINT, San Diego Web Development and Design</summary>                <content type="html"><![CDATA[<p>Software engineer Consultant for <a href="https://www.pint.com/" title="" rel="noreferrer">PINT</a>.</p>

<p><a href="https://www.pint.com/" title="" rel="noreferrer">PINT Inc.</a> (headquarter in San Diego, CA) 
is a nationally recognized interactive Web development agency providing web strategy, 
UX design, user experience, analytics, search marketing, and optimization to global companies and institutions.</p>

<p>PINT founder Thomas Powell is the author of eleven best-selling industry textbooks on HTML and Web design.</p>]]></content>
        </entry>        <entry>
            <title>YoCutie</title>
            <link href="https://www.goetas.com/projects/yocutie"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/yocutie</id>                <summary>Free Dating platform</summary>                <content type="html"><![CDATA[<p>YoCutie is a 100% free dating app.
As a backend lead I'm responsible for the whole backend infrastructure that is necessary to support the company business.</p>]]></content>
        </entry>        <entry>
            <title>LaCure</title>
            <link href="https://www.goetas.com/projects/lacure"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/lacure</id>                <summary>LaCure offers booking services for luxury villa accommodations.</summary>                <content type="html"><![CDATA[<p>In collaboration with the rest of the company I have planed a transition from a legacy PHP3/4 codebase
(including a complex CRM for the booking management and invoicing) 
to a modern PHP 7 application based on Symfony framework and its components.</p>

<h2 id="the-existing-product">The existing product</h2>

<p>LaCure has a great portfolio of properties and customers. Managing in the best way that relation is fundamental 
 for the success.</p>

<p>All the data are stored in a custom-legacy CRM and data are manually duplicated in a separate system to be 
displayed on the website.</p>

<p>The LaCure team was struggling to keep the two systems in sync. The CRM was also really user-unfriendly and 
was really difficult to add new feature necessary for the company growth.</p>

<h2 id="the-goal">The goal</h2>

<p>In order to speed up the business development LaCure decided a big refactoring of the whole system.</p>

<p>The goal was to estimate and to plan te cost of a brand new system that should address most of the 
issues the previous system had.
The new CRM should also implement many of the features were not present in the previous version of it and
were continuously asked by the different departments (invoicing, integration with payments, emailing...).</p>

<h2 id="conclusions">Conclusions</h2>

<p>The result was a detailed work plan for almost a year of development, with a detailed workload, 
necessary workforce and skills, cost estimation, technical solutions and implementation details.</p>]]></content>
        </entry>        <entry>
            <title>SolidOpinion</title>
            <link href="https://www.goetas.com/projects/solidopinion"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/solidopinion</id>                <summary>SolidOpinion is a startup, that revolutionize the way comments can be used to engage users and produce value for publishers</summary>                <content type="html"><![CDATA[<p>SolidOpinion by allowing users to comment on websites collects hundreds of thousands of comments a day.</p>

<p>I've helped SolidOpinion to perform some ETL (extract, transform, load) and helped to ease the analysis the collected data.</p>]]></content>
        </entry>        <entry>
            <title>Collabout.com</title>
            <link href="https://www.goetas.com/projects/collabout"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/collabout</id>                <summary>Collabout is a platform created to connect event organizers with partners  that might be interested in sponsoring their events.
 
</summary>                <content type="html"><![CDATA[<p>Collabout is a platform created to connect event organizers with partners that might be interested in sponsoring 
their events in order to get visibility, contacts, ideas and many other advantages.</p>]]></content>
        </entry>        <entry>
            <title>Synergist</title>
            <link href="https://www.goetas.com/projects/synergistio"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/synergistio</id>                <summary>synergist.io is a secure cloud platform to negotiate contracts in real-time.</summary>                <content type="html"><![CDATA[<p>synergist.io is a secure cloud platform to negotiate contracts in real-time.</p>

<h2 id="the-product">The product</h2>

<p>The product developed by synergist is composed mainly by an API and a fronted application.</p>

<p>The API is a PHP + Laravel application.</p>

<h2 id="the-goal">The goal</h2>

<p>I joined the project with the aim of setting-up a continuous delivery pipeline that will allow a faster development 
of the API with a lower number of bugs as the bugs should be discovered in earlier stages.</p>

<p>The goal was reached by implementing a CI pipeline that on each commit was running tests and building docker images 
ready to be deployed on the live system.</p>

<h2 id="conclusions">Conclusions</h2>

<p>The project was really exciting and the team was great. 
The CI pipeline implemented was clean and included secrets and log management too.</p>]]></content>
        </entry>        <entry>
            <title>Nestpick</title>
            <link href="https://www.goetas.com/projects/nestpick"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/nestpick</id>                <summary>An online medium-long term apartment and room rental web platform.</summary>                <content type="html"><![CDATA[<p>Nestpick is a company that allows to rent homes or apartments to people for a mid-long term (from 1 month to 1 year).
The company was Founded by Fabian Dudek when he was just 20. Rocket Internet (and others) invested more than 10M euro
in the idea.</p>

<p>I joined Nestpick as Senior Software Engineer right after the first round of substantial investments and in the 
maximum growth period. I quickly moved to a <strong>Tech-Lead</strong> role and acquired more and more responsibilities.</p>

<p>The main product was quickly built from an existing code base in really short time with a team of 7 developers, 
1 quality assurance and a product manager (excluding the marketing and operations teams that quickly reached more than 
100 heads).</p>

<h2 id="the-old-application">The old application</h2>

<p>The first application was built with PHP, using Zend Framework 1 application for the backend and Yii Framework for 
the frontend. Mysql for the main database, Solr as search engine and Memcache for boost performance.</p>

<p>A really big monolith, a single repo for everything and all the code was sharing dependencies everywhere.</p>

<p><img src="/img/projects/nestpick/nestpick-ab1.png" alt="The old application schema" /></p>

<p>The application worked fine but soon we stared to see rooms for improvement 
and the team <a href="https://en.wikipedia.org/wiki/Velocity_(software_development)">velocity</a> going down.
We quickly realized that we have to start improving it. We decided to start a refactoring process.</p>

<h2 id="the-goal">The goal</h2>

<p>The main constraint was a <strong>continuous improvement</strong> that meant improving the application from the technical point 
of view while continuously delivering improvements from the business point of view (more features and less bugs).</p>

<h2 id="the-new-application">The new application</h2>

<p>We started adding a <strong>Symfony 3.0 API</strong> as main access point to the data stored in the database, a <strong>webpack</strong> and 
<strong>React.js</strong> frontend, and removed thousands lines of codes that allowed us to improve performance and deliver
features quicker and quicker.</p>

<p>We focused at the beginning on the user-facing part of the application. Later we started also to improve 
the operations-facing part of it by trying to increase the velocity on the back-office.
We decided to start dropping the "custom" back-office and use <a href="https://sonata-project.org/">SonataAdmin</a>, 
and "admin-generator"  that allows to create basic back-office panels in hours for some simple areas.</p>

<p><img src="/img/projects/nestpick/nestpick-ab2.png" alt="The new application schema" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>The new architecture had a much more clear dependency graph, that allowed to have less bugs and to increase the speed 
of delivering new feature.</p>]]></content>
        </entry>        <entry>
            <title>ImmobiNet</title>
            <link href="https://www.goetas.com/projects/immobinet"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/immobinet</id>                <summary>Immobinet is a CRM for touristic apartments and rooms rental.</summary>                <content type="html"><![CDATA[<p>Immobinet is a CRM for touristic apartments and rooms rental.</p>

<p>Suppose a company that has to manage 10, 20, 100 or 1000 properties; 
keeping tack of them, renting all of them, maximizing the earning per property is not an easy task.</p>

<p>Here comes <strong>Immobinet</strong> a specialized CRM to help landlords managing their properties. By managing, 
Immobinet allows you to keep track of guests, their reservations, make them offers, manage mailing lists, 
handle all the communications with the customers, allows intermediary landlords with commissions, 
allows complex pricing models, allows complex allocation and pre sale models common for big-scale tour operators, 
synchronizing data between personal website and the most popular online travel operators as Booking.com or Airbnb 
and many many other features.</p>

<p>The software has been released and battle tested with a really large variety of companies and from small 
landlords (1-10 properties) to big tour operators (1000-5000 properties).</p>

<h2 id="the-technology">The technology</h2>

<p>Immobinet is mainly a Symfony + PHP application, using PostgreSQL as database.
It is a fully modular application with multiple independent components that can be installed on demand by each customer.</p>

<p>The codebase nowadays has more than 50 independent modules and approximately 500K lines of code.</p>

<h2 id="my-role">My role</h2>

<p>The business followed by Immobinet was a know market to Mercurio Sistemi (the company behind Immobinet), 
we just decided to tackle it in the most professional way 
and to do our best to deliver a product that each landlord or travel agency will love to use.</p>

<p>When the project started, I was responsible for the whole architecture, 
maintainability, speed of development, features and user experience of the product. 
I was also actively building (coding) the product it self writing most of the necessary features. 
Of course I was surrounded by a great team of founders,
 business experts that allowed to create the great product what Immobinet is now.</p>

<p>Few years later I've decided to continue my professional path in a different company, 
and Mercurio Sistemi continued to deliver the product in the excellent was as it always was.</p>

<p>I kept collaborating with Mercurio Sistemi in implementing features and promoting Immobinet.</p>]]></content>
        </entry>        <entry>
            <title>Lignano.it</title>
            <link href="https://www.goetas.com/projects/lignanoit"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/projects/lignanoit</id>                <summary>A booking.com competitor for hotel and apartment reservations.</summary>                <content type="html"><![CDATA[<p>Lignano.it is one of the biggest projects I've ever worked.</p>

<p>It is a website were is possible to 
book your holiday in a popular touristic location in the north Italy 
(<a href="https://en.wikipedia.org/wiki/Lignano_Sabbiadoro">Lignano Sabbiadoro</a>)</p>

<p>It involves thousands of accommodation providers, million of users, dozens of data providers, 
dozens of integrations with third party applications, 10 years of improvements and continuous learning.</p>

<p>The initial version of the product stated with almost a static website, quicker the website growth became exponential 
and lignano.it become a market leader.</p>

<p>The website offers a booking engine, listing pages, trips many other information that a tourist might need.</p>

<h2 id="history">History</h2>

<p>In 12 years the website had 4 restyling and a continuous improvement process that kept the website in a leader position
for all that time.</p>

<p>From a static website, when dynamic pages arrived on internet, the choice was PHP and PostgreSQL.</p>

<p>Many versions of the website arrived, each of them was bringing new features and an every-day more sophisticated
booking engine, more detailed pages and better user experience.</p>

<p>In the past there were no frameworks and really few opensource libraries. Every part had to be implemented.</p>

<h2 id="status">Status</h2>

<p>Nowadays Lignano.it evolved in a ecosystem of applications that provide a great user experience and a very reach 
set of tools for accommodation providers and advertisers.</p>

<p>It includes complex pricing models, complex availability and payment systems. 
Offers channel managers, custom websites, mobile apps and everything might be necessary to run a budiness based on 
short term tourism.</p>

<p>Some of the technologies used are:</p>

<ul>
<li>Linux: Linux is a fundamental building block of almost every component, from development to production.</li>
<li>Symfony Framework: most of the recent applications are built using symfony. 
Some of the legacy applications are still running, some are wrapped into symfony applications and some are there to stay
with the old PHP4 as it was in the past.</li>
<li>Rest webservices: some applications expose REST webservices that allow to consume data (internally and externally)</li>
<li>SOAP: Many integrations with third party providers use the SOAP technology (Open Travel Alliance as example) </li>
<li>Redis: Lignano.it has to be fast, caching data and its invalidation is fundamental</li>
<li>PostgreSQL: RostgreSQL has been a valid allied from the beginning, 
delivering great performance and exceptional consistency</li>
</ul>]]></content>
        </entry>        <entry>
            <title>jms/serializer</title>
            <link href="https://www.goetas.com/opensource/jms-serializer"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/jms-serializer</id>                <summary>This library allows you to serialize/deserialize data of any complexity. It supports XML, JSON, and YAML.</summary>                <content type="html"><![CDATA[<p>This library allows you to (de-)serialize data of any complexity. Currently, it supports XML, JSON, and YAML.</p>

<p>It also provides you with a rich tool-set to adapt the output to your specific needs.</p>

<p>Built-in features include:</p>

<p>(De-)serialize data of any complexity; circular references are handled gracefully.</p>

<ul>
<li>Supports many built-in PHP types (such as dates)</li>
<li>Integrates with Doctrine ORM, et. al.</li>
<li>Supports versioning, e.g. for APIs</li>
<li>Configurable via PHP, XML, YAML, or Doctrine Annotations</li>
</ul>]]></content>
        </entry>        <entry>
            <title>goetas-webservices/xsd2php</title>
            <link href="https://www.goetas.com/opensource/goetas-webservicesxsd2php"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/goetas-webservicesxsd2php</id>                <summary>Convert XSD into PHP classes, and serialize/unserialze instances from/into XML</summary>                <content type="html"><![CDATA[<p>With <code>goetas/xsd2php</code> you can convert any XSD/WSDL definition into PHP classes.</p>

<p>XSD2PHP can also generate <a href="https://github.com/schmittjoh/serializer">JMS Serializer</a> 
compatible metadata that can be used to serialize/unserialize the object instances.</p>]]></content>
        </entry>        <entry>
            <title>masterminds/html5-php</title>
            <link href="https://www.goetas.com/opensource/mastermindshtml5-php"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/mastermindshtml5-php</id>                <summary>An HTML5 parser and serializer for PHP</summary>                <content type="html"><![CDATA[<p>An <a href="https://en.wikipedia.org/wiki/HTML5">HTML5</a> parser and serializer for PHP.</p>

<p>Allows to read and write HTML5 documents. Internally it uses DOM, 
that enables interoperability with many XML related tools.</p>]]></content>
        </entry>        <entry>
            <title>doctrine/migrations</title>
            <link href="https://www.goetas.com/opensource/doctrine-migrations"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/doctrine-migrations</id>                <summary>Doctrine Database Migrations Library</summary>                <content type="html"><![CDATA[<p>The Doctrine Migrations project offers additional functionality on top of the DBAL and ORM 
for versioning your database schema. 
It makes it easy and safe to deploy changes to it in a way that can be 
reviewed and tested before being deployed to production.</p>]]></content>
        </entry>        <entry>
            <title>goetas-webservices/soap-client</title>
            <link href="https://www.goetas.com/opensource/goetas-webservicessoap-client"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/goetas-webservicessoap-client</id>                <summary>PHP implementation of SOAP 1.1 client specifications</summary>                <content type="html"><![CDATA[<p>PHP implementation of SOAP 1.1 client specifications.</p>

<p>Strengths:</p>

<ul>
<li>Pure PHP, no dependencies on ext-soap</li>
<li>Extensible (JMS event listeners support)</li>
<li>PSR-7 HTTP messaging compatible</li>
<li>Multi HTTP client (guzzle, buzz, curl, react)</li>
<li>No WSDL/XSD parsing on production</li>
<li>IDE type hinting support</li>
</ul>]]></content>
        </entry>        <entry>
            <title>goetas-webservices/soap-reader</title>
            <link href="https://www.goetas.com/opensource/goetas-webservicessoap-reader"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/goetas-webservicessoap-reader</id>                <summary>Read any SOAP definitions programmatically with PHP.</summary>                <content type="html"><![CDATA[<p>Read <a href="https://en.wikipedia.org/wiki/SOAP">SOAP</a> definitions contained into 
 <a href="https://en.wikipedia.org/wiki/Web_Services_Description_Language">WSDL</a> files programmatically with PHP.</p>]]></content>
        </entry>        <entry>
            <title>goetas-webservices/wsdl-reader</title>
            <link href="https://www.goetas.com/opensource/goetas-webserviceswsdl-reader"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/goetas-webserviceswsdl-reader</id>                <summary>Read any WSDL files programmatically with PHP.</summary>                <content type="html"><![CDATA[<p>Read <a href="https://en.wikipedia.org/wiki/Web_Services_Description_Language">WSDL</a> files programmatically with PHP.</p>]]></content>
        </entry>        <entry>
            <title>goetas-webservices/xsd-reader</title>
            <link href="https://www.goetas.com/opensource/goetas-webservicesxsd-reader"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/goetas-webservicesxsd-reader</id>                <summary>Read any XML Schema (XSD) programmatically with PHP.</summary>                <content type="html"><![CDATA[<p>Read any <a href="https://en.wikipedia.org/wiki/XML_schema">XML Schema</a> (XSD) programmatically with PHP.</p>]]></content>
        </entry>        <entry>
            <title>willdurand/hateoas-bundle</title>
            <link href="https://www.goetas.com/opensource/willdurand-hateoas-bundle"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/willdurand-hateoas-bundle</id>                <summary>Integration of the willdurand/hateoas library into Symfony.</summary>                <content type="html"><![CDATA[<p>Integration of the willdurand/hateoas library into Symfony.</p>

<p>Hypermedia As The Engine Of Application State (HATEOAS) is a constraint of the REST application architecture 
that distinguishes it from other network application architectures.</p>

<p>With HATEOAS, a client interacts with a network application whose application servers provide information
 dynamically through hypermedia. A REST client needs little to no prior knowledge about how to interact
  with an application or server beyond a generic understanding of hypermedia.</p>]]></content>
        </entry>        <entry>
            <title>willdurand/hateoas</title>
            <link href="https://www.goetas.com/opensource/willdurand-hateoas"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/willdurand-hateoas</id>                <summary>A PHP library to support implementing representations for HATEOAS REST web services</summary>                <content type="html"><![CDATA[<p>A PHP library to support implementing representations for HATEOAS REST web services.</p>

<p>Hypermedia As The Engine Of Application State (HATEOAS) is a constraint of the REST application architecture 
that distinguishes it from other network application architectures.</p>

<p>With HATEOAS, a client interacts with a network application whose application servers provide information
 dynamically through hypermedia. A REST client needs little to no prior knowledge about how to interact
  with an application or server beyond a generic understanding of hypermedia.</p>]]></content>
        </entry>        <entry>
            <title>goetas/twital</title>
            <link href="https://www.goetas.com/opensource/twital"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/twital</id>                <summary>Twital is a plugin for Twig (a template engine for PHP) that adds some shortcuts and makes Twig&#039;s syntax more suitable for HTML based (XML, HTML5, XHTML, SGML) templates.</summary>                <content type="html"><![CDATA[<p>Twital is a plugin for <a href="http://twig.sensiolabs.org/">Twig</a> (a template engine for PHP) 
that adds some shortcuts and makes Twig's syntax more suitable for HTML based (XML, HTML5, XHTML, SGML) templates.
Twital takes inspiration from <a href="http://phptal.org/">PHPTal</a>, <a href="http://en.wikipedia.org/wiki/Template_Attribute_Language">TAL</a> 
and <a href="http://angularjs.org/">AngularJS</a> (just for some aspects), 
mixing their language syntaxes with the powerful Twig templating engine system.</p>]]></content>
        </entry>        <entry>
            <title>hautelook/TemplatedUriBundle</title>
            <link href="https://www.goetas.com/opensource/hautelook-templated-uri-bundle"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/hautelook-templated-uri-bundle</id>                <summary>Symfony Bundle for the hautelook/TemplatedUriRouter. Provides a RFC-6570 compatible Symfony router and URL Generator.</summary>                <content type="html"><![CDATA[<p>Symfony Bundle for the <a href="https://github.com/hautelook/TemplatedUriRouter">https://github.com/hautelook/TemplatedUriRouter</a>
library. 
<code>hautelook/TemplatedUriRouter</code> provides a <a href="https://tools.ietf.org/html/rfc6570">RFC-6570</a> compatible 
Symfony router and URL Generator.</p>]]></content>
        </entry>        <entry>
            <title>hautelook/TemplatedUriRouter</title>
            <link href="https://www.goetas.com/opensource/hautelook-templated-uri-router"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/opensource/hautelook-templated-uri-router</id>                <summary>Symfony UrlGenerator that provides a RFC-6570 compatible router and URL Generator.</summary>                <content type="html"><![CDATA[<p>Symfony UrlGenerator that provides a <a href="https://tools.ietf.org/html/rfc6570">RFC-6570</a> compatible router and URL Generator.</p>]]></content>
        </entry>        <entry>
            <title>Roberto Fantuzzo</title>
            <link href="https://www.goetas.com/testimonials/roberto-fantuzzo"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/testimonials/roberto-fantuzzo</id>                <summary>For more than 10 years Asmir contributed to the success of my company</summary>
        </entry>        <entry>
            <title>Mladen Stific</title>
            <link href="https://www.goetas.com/testimonials/mladen-stific"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/testimonials/mladen-stific</id>                <summary>Asmir helped us to setup our continuous delivery pipeline and to deliver features in less time with less bugs</summary>
        </entry>        <entry>
            <title>Mats Kyllenius</title>
            <link href="https://www.goetas.com/testimonials/mats-kyllenius"/>
            <updated>2023-07-06T11:25:34+02:00</updated>
            <id>https://www.goetas.com/testimonials/mats-kyllenius</id>                <summary>Asmir is a really talented full-stack developer. Apart from that, he is also a very good general discussion partner in a development project, with great business insight.</summary>
        </entry></feed>